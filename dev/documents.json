[{"doctype":"documentation","id":"references/FastAI.withcallbacks","title":"withcallbacks","text":" withcallbacks(f, learner, callbacks...)\n Run  f  with  callbacks  on  learner . Existing callbacks on  learner  of the same type as in  callbacks  are swapped during the execution of  f ."},{"doctype":"documentation","id":"references/FastAI.lrfindtextplot!","title":"lrfindtextplot!","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.NamedTupleData","title":"NamedTupleData","text":""},{"doctype":"documentation","id":"references/Flux.GlobalMaxPool","title":"GlobalMaxPool","text":" GlobalMaxPool()\n Global max pooling layer. Transforms (w,h,c,b)-shaped input into (1,1,c,b)-shaped output, by performing max pooling on the complete (w,h)-shaped feature maps. See also  MaxPool ,  GlobalMeanPool . julia> xs = rand(Float32, 100, 100, 3, 50);\n\njulia> m = Chain(Conv((3,3), 3 => 7), GlobalMaxPool());\n\njulia> m(xs) |> size\n(1, 1, 7, 50)\n\njulia> GlobalMaxPool()(rand(3,5,7)) |> size  # preserves 2 dimensions\n(1, 5, 7)\n"},{"doctype":"documentation","id":"references/FastAI.Vision.IMAGENET_MEANS","title":"IMAGENET_MEANS","text":""},{"doctype":"document","id":"documents/docs/status.md","title":"Status","text":" Status To - Dos [x] release DLPipelines.jl update its documentation register [x] release DataAugmentation.jl add onehot-encoding of  MaskMulti s add tests for  PinOrigin fix bounds assignment for  PinOrigin  on  Keypoints [x] move basic datasets and dataset utilities from DLDatasets.jl to FastAI.jl [x] image classification datasets [y] other datasets [x] add  ImageSegmentation  method training utilities [x]  fitonecycle! [x]  finetune! [x]  lrfind! wait for Flux .12.0 to be release plot recipes for learning rate finder, visualizing losses, hyperparameters"},{"doctype":"documentation","id":"references/DataAugmentation.tensortoimage","title":"tensortoimage","text":""},{"doctype":"documentation","id":"references/Flux._show_leaflike","title":"_show_leaflike","text":""},{"doctype":"documentation","id":"references/FastAI.ParamGroups","title":"ParamGroups","text":" ParamGroups(grouper, m)\n A logical grouping of parameters in  m  created by  ParamGrouper grouper . Parameters in  m  are assigned a group that can be queried using  getgroup(paramgroups, param) . If a parameter is not assigned a group,  getgroup  returns  nothing . Examples using   Flux :   Chain ,   Dense ,   params \n using   FastAI :   ParamGroups ,   IndexGrouper ,   getgroup \n \n model   =   Chain ( Dense ( 3 ,   5 ) ,   Dense ( 5 ,   3 ) ) \n paramgroups   =   ParamGroups ( IndexGrouper ( [ 1 ,   2 ] ) ,   model ) \n \n getgroup ( paramgroups ,   model [ 1 ] . weight )   ==   1 \n getgroup ( paramgroups ,   model [ 2 ] . weight )   ==   2 \n getgroup ( paramgroups ,   rand ( 10 ) )   ===   nothing"},{"doctype":"documentation","id":"references/FastAI.Vision.Models.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.tests","title":"tests","text":""},{"doctype":"documentation","id":"references/FluxTraining.@pack_History!","title":"@pack_History!","text":""},{"doctype":"documentation","id":"references/FluxTraining.AbstractCallback","title":"AbstractCallback","text":" abstract type AbstractCallback\n Supertype of  SafeCallback / Callback . When implementing callbacks, you should subtype  SafeCallback  instead."},{"doctype":"documentation","id":"references/FluxTraining.Phases.ValidationPhase","title":"ValidationPhase","text":" ValidationPhase()\n A regular validation phase. It iterates over batches in  learner.data.validation  and performs a forward pass. Throws the following events:  EpochBegin ,  StepBegin , LossBegin ,  StepEnd ,  EpochEnd . Throws the following events in this order: EpochBegin  when an epoch starts, StepBegin  when a step starts, LossBegin  after the forward pass but before loss calculation, StepEnd  when a step ends; and EpochEnd  when an epoch ends It writes the following step state to  learner.state , grouped by the event from which on it is available. StepBegin : xs  and  ys : encoded input and target (batch) LossBegin : ŷs : model output StepEnd : loss : loss"},{"doctype":"documentation","id":"references/DataAugmentation.showpolygon!","title":"showpolygon!","text":""},{"doctype":"documentation","id":"references/FluxTraining.iterpairs","title":"iterpairs","text":" iterpairs(a)\n Iterators over the Cartesian product of  a  with itself, skipping any pairs  (a, b)  where  a == b ."},{"doctype":"documentation","id":"references/FluxTraining.StopOnNaNLoss","title":"StopOnNaNLoss","text":" StopOnNaNLoss()\n Stops the training when a NaN loss is encountered. This callback is added by default to every  Learner  unless you pass in usedefaultcallbacks = false ."},{"doctype":"documentation","id":"references/Flux._big_show","title":"_big_show","text":""},{"doctype":"documentation","id":"references/FastAI.defaulttaskregistry","title":"defaulttaskregistry","text":""},{"doctype":"documentation","id":"references/FastAI.WrapperBlock","title":"WrapperBlock","text":""},{"doctype":"documentation","id":"references/Flux.nfan","title":"nfan","text":" nfan(n_out, n_in=1) -> Tuple\nnfan(dims...)\nnfan(dims::Tuple)\n For a layer characterized by dimensions  dims , return a tuple  (fan_in, fan_out) , where  fan_in is the number of input neurons connected to an output one, and  fan_out  is the number of output neurons connected to an input one. This function is mainly used by weight initializers, e.g., [ kaiming_normal ](  Flux.kaiming_normal). Examples julia> layer = Dense(10, 20);\n\njulia> Flux.nfan(size(layer.weight))\n(10, 20)\n\njulia> layer = Conv((3, 3), 2=>10);\n\njulia> Flux.nfan(size(layer.weight))\n(18, 90)\n"},{"doctype":"documentation","id":"references/Flux.isleaflike","title":"isleaflike","text":""},{"doctype":"documentation","id":"references/FastAI.predictbatch","title":"predictbatch","text":" predictbatch(task, model, inputs[; device, context])\n Predict  targets  from a vector of  inputs  using  model  by batching them. Optionally apply function  device  to batch before passing to  model  and use  context  instead of the default  Inference ."},{"doctype":"documentation","id":"references/FluxTraining.LinearRunner","title":"LinearRunner","text":""},{"doctype":"documentation","id":"references/FastAI._blockcell","title":"_blockcell","text":""},{"doctype":"documentation","id":"references/FastAI.mockinput","title":"mockinput","text":" mockinput(task)\n Generate a random  input  compatible with  task ."},{"doctype":"documentation","id":"references/FastAI.Vision.Models.AdaptiveConcatPool","title":"AdaptiveConcatPool","text":""},{"doctype":"documentation","id":"references/Flux.ConvTranspose","title":"ConvTranspose","text":" ConvTranspose(weight::AbstractArray, [bias, activation; stride, pad, dilation, groups])\n Constructs a layer with the given weight and bias arrays. Accepts the same keywords as the  ConvTranspose((4,4), 3 => 7, relu)  method. ConvTranspose(filter, in => out, σ=identity; stride=1, pad=0, dilation=1, [bias, init])\n Standard convolutional transpose layer.  filter  is a tuple of integers specifying the size of the convolutional kernel, while in  and  out  specify the number of input and output channels. Note that  pad=SamePad()  here tries to ensure  size(output,d) == size(x,d) * stride . Parameters are controlled by additional keywords, with defaults init=glorot_uniform  and  bias=true . See also  Conv  for more detailed description of keywords. Examples julia> xs = rand(Float32, 100, 100, 3, 50);  # a batch of 50 RGB images\n\njulia> lay = ConvTranspose((5,5), 3 => 7, relu)\nConvTranspose((5, 5), 3 => 7, relu)  # 532 parameters\n\njulia> lay(xs) |> size\n(104, 104, 7, 50)\n\njulia> ConvTranspose((5,5), 3 => 7, stride=2)(xs) |> size\n(203, 203, 7, 50)\n\njulia> ConvTranspose((5,5), 3 => 7, stride=3, pad=SamePad())(xs) |> size\n(300, 300, 7, 50)\n"},{"doctype":"documentation","id":"references/Flux.rand32","title":"rand32","text":" rand32([rng], size...)\nrandn32([rng], size...)\n Return an  Array{Float32}  of the given  size , filled like  rand  or  randn . When the size is not provided,  rand32(rng::AbstractRNG)  returns a function."},{"doctype":"documentation","id":"references/Flux.Data","title":"Data","text":""},{"doctype":"documentation","id":"references/FluxTraining.setlearningrate!","title":"setlearningrate!","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.groupobs","title":"groupobs","text":" groupobs(f, data)\n Split data container data  data  into different data containers, grouping observations by  f(obs) . data   =   -10 : 10 \n datas   =   groupobs ( > ( 0 ) ,   data ) \n length ( datas )   ==   2"},{"doctype":"documentation","id":"references/FastAI.Vision.ImageKeypointRegression","title":"ImageKeypointRegression","text":" ImageKeypointRegression(size, nkeypoints; kwargs...)\n Learning task for regressing a set of  nkeypoints  keypoints from images. Images are resized to  size  and a class is predicted for every pixel."},{"doctype":"documentation","id":"references/Flux.CUDAint","title":"CUDAint","text":""},{"doctype":"documentation","id":"references/DataAugmentation.normalize!","title":"normalize!","text":""},{"doctype":"documentation","id":"references/FluxTraining.RunFirst","title":"RunFirst","text":" abstract type RunFirst <: ConflictResolution\n Return  RunFirst(cb1/cb2)  from  resolveconflict (cb1, cb2)  to indicate that one of the callbacks should always run before the other."},{"doctype":"documentation","id":"references/FastAI.showblocksinterpretable","title":"showblocksinterpretable","text":" showblocksinterpretable(backend, encodings, block, obss)\n Multi-sample version  showblockinterpretable ."},{"doctype":"documentation","id":"references/Flux.testmode!","title":"testmode!","text":" testmode!(m, mode = true)\n Set a layer or model’s test mode (see below). Using  :auto  mode will treat any gradient computation as training. Note : if you manually set a model into test mode, you need to manually place it back into train mode during training phase. Possible values include: false  for training true  for testing :auto  or  nothing  for Flux to detect the mode automatically"},{"doctype":"documentation","id":"references/FastAI.setwrapped","title":"setwrapped","text":""},{"doctype":"documentation","id":"references/FluxTraining.ToDevice","title":"ToDevice","text":" ToDevice(movefn[, movemodelfn]) <: Callback\n Moves model and step data to a device using  movedatafn  for step data and  movemodelfn  for the model. For example  ToDevice(Flux.gpu, Flux.gpu) , moves them to a GPU if available. See  ToGPU . By default, only moves  step.xs  and  step.ys , but this can be extended to other state by implementing  on(::StepBegin, ::MyCustomPhase, ::ToDevice, learner) ."},{"doctype":"documentation","id":"references/FluxTraining._on","title":"_on","text":""},{"doctype":"documentation","id":"references/FastAI.PropagateWrapper","title":"PropagateWrapper","text":" abstract type PropagateWrapper\n Defines the default propagation behavior of a  WrapperBlock  when an encoding is applied to it. Propagation refers to what happens when an encoding is applied to a  WrapperBlock . If no  encode  method is defined for a wrapper block wrapper ,  encode  is instead called on the wrapped block. Propagating the wrapper block means that the block resulting from encoding the wrapped block is rewrapped in  wrapper. . wrapper = Wrapper(block)\n# propagate\nencodedblock(enc, wrapper) = Wrapper(encodedblock(enc, wrapped(wrapper)))\n\n# don't propagate\nencodedblock(enc, wrapper) = encodedblock(enc, wrapped(wrapper))\n The following wrapping behaviors exist: PropagateAlways : Always propagate. This is the default behavior. PropagateNever : Never propagate PropagateSameBlock : Only propagate if the wrapped block is unchanged by the encoding"},{"doctype":"documentation","id":"references/FastAI.CONTEXTS","title":"CONTEXTS","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Image","title":"Image","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.apply!","title":"apply!","text":""},{"doctype":"document","id":"documents/docs/howto/augmentvision.md","title":"How to augment vision data","text":" How to augment vision data Data augmentation is important to train models with good generalization ability, especially when the size of your dataset is limited. FastAI.jl gives you high-level helpers to use data augmentation in vision learning tasks, but also allows directly using  DataAugmentation . jl , the underlying data augmentation library. By default, the only augmentation that will be used in computer vision tasks is a random crop, meaning that after images, keypoints and masks are resized to a similar size a random portion will be cropped during training. We can demonstrate this on the image classification task. ENV [ \" DATADEPS_ALWAYS_ACCEPT \" ]   =   \" true \" using   FastAI \n import   FastAI :   Image \n import   CairoMakie ;   CairoMakie . activate! ( type = \" png \" ) \n \n data ,   blocks   =   loaddataset ( \" imagenette2-160 \" ,   ( Image ,   Label ) ) \n task   =   BlockTask ( \n     blocks , \n     ( \n         ProjectiveTransforms ( ( 128 ,   128 ) ) , \n         ImagePreprocessing ( ) , \n         OneHot ( ) \n     ) \n ) \n xs ,   ys   =   FastAI . makebatch ( task ,   data ,   fill ( 4 ,   3 ) ) \n showbatch ( task ,   ( xs ,   ys ) ) Most learning tasks let you pass additional augmentations as keyword arguments. For example,  ImageClassification  takes the  aug_projection  and  aug_image  arguments. FastAI.jl provides the  augs_projection  helper to quickly construct a set of projective data augmentations. task2   =   BlockTask ( \n     blocks , \n     ( \n         ProjectiveTransforms ( ( 128 ,   128 ) ,   augmentations = augs_projection ( ) ) , \n         ImagePreprocessing ( ) , \n         OneHot ( ) \n     ) \n ) \n xs2 ,   ys2   =   FastAI . makebatch ( task2 ,   data ,   fill ( 4 ,   3 ) ) \n showbatch ( task2 ,   ( xs2 ,   ys2 ) ) Likewise, there is an  augs_lighting  helper that adds contrast and brightness augmentation: task3   =   BlockTask ( \n     blocks , \n     ( \n         ProjectiveTransforms ( ( 128 ,   128 ) ,   augmentations = augs_projection ( ) ) , \n         ImagePreprocessing ( augmentations = augs_lighting ( ) ) , \n         OneHot ( ) \n     ) \n ) \n xs3 ,   ys3   =   FastAI . makebatch ( task3 ,   data ,   fill ( 4 ,   3 ) ) \n showbatch ( task3 ,   ( xs3 ,   ys3 ) )"},{"doctype":"documentation","id":"references/FastAI.Tabular.TabularClassificationSingle","title":"TabularClassificationSingle","text":" TabularClassificationSingle(blocks, data)\n Learning task for single-label tabular classification. Continuous columns are normalized and missing values are filled, categorical columns are label encoded taking into account any missing values which might be present. The target value is predicted from  classes .  blocks  should be an input and target block (TableRow(...), Label(...)) . TabularClassificationSingle(classes, tabledata [; catcols, contcols])\n Construct learning task with  classes  to classify into and a  TableDataset tabledata . The column names can be passed in or guessed from the data."},{"doctype":"documentation","id":"references/FastAI.describetask","title":"describetask","text":""},{"doctype":"documentation","id":"references/Flux._norm_layer_forward","title":"_norm_layer_forward","text":""},{"doctype":"documentation","id":"references/FastAI.fitonecycle!","title":"fitonecycle!","text":" fitonecycle!(learner, nepochs[, lrmax])\n Fit  learner  for  nepochs  using a one-cycle learning rate schedule. The learning rate starts at  lrmax/div  for  pct_start*nepochs  epochs, rising to  lrmax  and then goes down to  lrmax/div_final  over the remaining duration. Keyword arguments wd = 0 : weight decay pct_start = 0.25 : Percentage of time spent raising the learning rate div = 25 : Starting learning rate is  lr_max/div div_final = 1e5 : Ending learning rate is  lr_max/div_final"},{"doctype":"documentation","id":"references/FluxTraining._dataiters","title":"_dataiters","text":""},{"doctype":"documentation","id":"references/FastAI.encodetarget","title":"encodetarget","text":""},{"doctype":"documentation","id":"references/FluxTraining.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.TESTSET_MACROS","title":"TESTSET_MACROS","text":""},{"doctype":"documentation","id":"references/Flux.trainmode!","title":"trainmode!","text":" trainmode!(m, mode = true)\n Set a layer of model’s train mode (see below). Symmetric to  testmode!  (i.e.  trainmode!(m, mode) == testmode!(m, !mode) ). Note : if you manually set a model into train mode, you need to manually place it into test mode during testing phase. Possible values include: true  for training false  for testing :auto  or  nothing  for Flux to detect the mode automatically"},{"doctype":"documentation","id":"references/FastAI.shouldbatch","title":"shouldbatch","text":" shouldbatch(::LearningTask)\n Define whether encoded samples for a learning task should be batched. The default is  true ."},{"doctype":"documentation","id":"references/FastAI.frozen_optimizer","title":"frozen_optimizer","text":" frozen_optimizer(optim, grouper, model)\n Create an optimizer that only updates parameters which  ParamGrouper puts into group  2 ."},{"doctype":"documentation","id":"references/FluxTraining.Check","title":"Check","text":""},{"doctype":"document","id":"documents/docs/background/blocksencodings.md","title":"Blocks and encodings","text":" Blocks and encodings Unstructured notes on blocks and encodings Blocks A  block  describes the meaning of a piece of data in the context of a learning task. For example, for supervised learning tasks, there is an input block and a target block and we want to learn to predict targets from inputs. Learning to predict a cat/dog label ( Label([\"cat\", \"dog\"]) ) from 2D images ( Image{2}() ) is a supervised image classification task. A block is not a piece of data itself. Instead it describes the meaning of a piece of data in a context. That a piece of data is a block can be checked using [ checkblock ] (block, data) . A piece of data for the  Label  block above needs to be one of the labels, so  checkblock(Label([\"cat\", \"dog\"]), \"cat\") == true , but  checkblock(Label([\"cat\", \"dog\"]), \"cat\") == false . We can say that a data container is compatible with a learning task if every observation in it is a valid sample of the sample block of the learning task. The sample block for supervised tasks is  sampleblock = (inputblock, targetblock)  so  sample = getobs(data, i)  from a compatible data container implies that  checkblock(sampleblock, sample) . This also means that any data stored in blocks must not depend on individual samples; we can store the names of possible classes inside the  Label  block because they are the same across the whole dataset. Data pipelines We can use blocks to formalize the data processing pipeline. During  training  we want to create pairs of data  (x, y)  s.t.  output = model(x)  and  loss = lossfn(output, y) . In terms of blocks that means  model  is a function  (x,) -> output  and the loss function maps  (outputblock, yblock) -> loss . Usually,  (input, target) != (x, y)  and instead we have an encoding step that transforms a sample into representations suitable to train a model on, i.e.  encode :: sample -> (x, y) . For the above image classification example we have  sampleblock = (Image{2}(), Label([\"cat\", \"dog\"]))  but we cannot put raw images into a model and get out a class. Instead, the image is converted to an array that includes the color dimension and its values are normalized; and the class label is one-hot encoded. So  xblock = ImageTensor{2}()  and  yblock = OneHotTensor{0} . Hence to do training, we need a sample encoding function  (Image{2}, Label) -> (ImageTensor{2}, OneHotTensor{0}) During  inference , we have an input and want to use a trained model to predict a target, i.e.  input -> target . The model is again a mapping  xblock -> outputblock , so we can build the transformation with an encoding step that encodes the input and a decoding step that takes the model output back into a target. This gives us (predict :: input -> target) = decodeoutput ∘ model ∘ encodeinput where (encodeinput :: input -> x) (model :: x -> y) (decodeoutput :: y -> target) In the classification example we have, written in blocks,  predict :: Image{2} -> Label  and hence  encodeinput :: Image{2} -> ImageTensor{2}  and  decodeoutput :: OneHotTensor{0} -> Label Where do we draw the line between model and data processing? In general, the encoding and decoding steps are  non - learnable  transformations, while the model is a  learnable  transformation. Encodings Encodings  are reversible transformations that model the non-learnable parts (encoding and decoding) of the data pipeline. What an encoding does depends on what block is passed in. Most encodings only transform specific blocks. For example, the  ImagePreprocessing  encoding maps blocks  Image{N} -> ImageTensor{N} , but leaves other blocks unchanged. Encodings are called with  encode  and  decode  which take in the block and the data. The actual encoding and decoding takes in an additional context argument which can be specialized on to implement different behavior for e.g. training and validation. using   FastAI ,   Colors \n using   FastAI . Vision :   ImageTensor \n enc   =   ImagePreprocessing ( ) \n data   =   rand ( RGB ,   100 ,   100 ) \n @ show   summary ( data ) \n encdata   =   encode ( enc ,   Training ( ) ,   Image { 2 } ( ) ,   data ) \n @ show   summary ( encdata )    # (h, w, ch)-image tensor \n data_   =   decode ( enc ,   Training ( ) ,   ImageTensor { 2 } ( 3 ) ,   encdata ) Using an encoding to encode and then decode must be block-preserving, i.e. if, for an encoding,  encode :: Block1 -> Block2  then  decode :: Block2 -> Block1 . To see the resulting block of applying an encoding to a block, we can use  encodedblock  and  decodedblock . using   FastAI :   encodedblock ,   decodedblock \n enc   =   ImagePreprocessing ( ) \n @ show   encodedblock ( enc ,   Image { 2 } ( ) ) \n @ show   decodedblock ( enc ,   ImageTensor { 2 } ( 3 ) ) \n Image { 2 } ( )   ==   decodedblock ( enc ,   encodedblock ( enc ,   Image { 2 } ( ) ) ) You can use  testencoding  to test these invariants to make sure an encoding is implemented properly for a specific block. FastAI . testencoding ( enc ,   Image { 2 } ( ) ) The default implementations of  encodedblock  and  decodedblock  is to return  nothing  indicating that it doesn’t transform the data. This is overwritten for blocks for which  encode  and  decode  are implemented to indicate that the data is transformed. Using  encodedblockfilled(block, data)  will replace returned  nothing s with the unchanged block. encodedblock ( enc ,   Label ( 1 : 10 ) )   ===   nothing using   FastAI :   encodedblockfilled \n encodedblockfilled ( enc ,   Label ( 1 : 10 ) )   ==   Label ( 1 : 10 ) Encodings can be applied to tuples of blocks. The default behavior is to apply the encoding to each block separately. encodedblock ( enc ,   ( Image { 2 } ( ) ,   Image { 2 } ( ) ) ) Applying a tuple of encodings will encode the data by applying one encoding after the other. When decoding, the order is reversed. Block learning tasks BlockTask  creates a learning task from blocks and encodings. You define the sample block (recall for supervised tasks this is a tuple of input and target) and a sequence of encodings that are applied to all blocks. The below example defines the same learning task as  ImageClassificationSingle  does. The first two encodings only change  Image , and the last changes only  Label , so it’s simple to understand. task   =   BlockTask ( \n     ( Image { 2 } ( ) ,   Label ( [ \" cats \" ,   \" dogs \" ] ) ) , \n     ( \n         ProjectiveTransforms ( ( 128 ,   128 ) ) , \n         ImagePreprocessing ( ) , \n         OneHot ( ) , \n     ) \n ) Now  encode  expects a sample and just runs the encodings over that, giving us an encoded input  x  and an encoded target  y . data   =   loadfolderdata ( joinpath ( datasetpath ( \" dogscats \" ) ,   \" train \" ) ,   filterfn = isimagefile ,   loadfn = ( loadfile ,   parentname ) ) \n sample   =   getobs ( data ,   1 ) \n x ,   y   =   encodesample ( task ,   Training ( ) ,   sample ) \n summary ( x ) ,   summary ( y ) This is equivalent to: x ,   y   =   encode ( task . encodings ,   Training ( ) ,   FastAI . getblocks ( task ) . sample ,   sample ) \n summary ( x ) ,   summary ( y ) Image segmentation looks almost the same except we use a  Mask  block as target. We’re also using  OneHot  here, because it also has an  encode  task for  Mask s. For this task,  ProjectiveTransforms  will be applied to both the  Image  and the  Mask , using the same random state for cropping and augmentation. task   =   BlockTask ( \n     ( Image { 2 } ( ) ,   Mask { 2 } ( 1 : 10 ) ) , \n     ( \n         ProjectiveTransforms ( ( 128 ,   128 ) ) , \n         ImagePreprocessing ( ) , \n         OneHot ( ) , \n     ) \n ) The easiest way to understand how encodings are applied to each block is to use  describetask  and  describeencodings  which print a table of how each encoding is applied successively to each block. Rows where a block is  bolded  indicate that the data was transformed by that encoding. describetask ( task ) The above tables make it clear what happens during training (“encoding a sample”) and inference (encoding an input and “decoding an output”). The more general form  describeencodings  takes in encodings and blocks directly and can be useful for building an understanding of how encodings apply to some blocks. FastAI . describeencodings ( task . encodings ,   ( Image { 2 } ( ) , ) ) FastAI . describeencodings ( ( OneHot ( ) , ) ,   ( Label ( 1 : 10 ) ,   Mask { 2 } ( 1 : 10 ) ,   Image { 2 } ( ) ) ) Notes Since most encodings just operate on a small number of blocks and keep the rest unchanged, applying them to all blocks is usually not a problem. When it is because you want some encoding to apply to a specific block only, you can use  Named  and  Only  to get around it."},{"doctype":"documentation","id":"references/FluxTraining.throttle","title":"throttle","text":" throttle(callback, Event, freq = 1)\nthrottle(callback, Event, seconds = 1)\n Throttle  Event  type for  callback  so that it is triggered either only every freq ’th time  or every  seconds  seconds. Examples If you want to only sporadically log metrics ( LogMetrics ) or images ( LogVisualization ),  throttle  can be used as follows. Every 10 steps: callback = throttle(LogMetrics(TensorBoardBackend()), StepEnd, freq = 10)\nlearner = Learner(<args>, callback) Or every 5 seconds: callback = throttle(LogMetrics(TensorBoardBackend()), StepEnd, seconds = 5)\nlearner = Learner(<args>, callback)"},{"doctype":"documentation","id":"references/FluxTraining.Callbacks","title":"Callbacks","text":""},{"doctype":"documentation","id":"references/FluxTraining.LearningRate","title":"LearningRate","text":" abstract type LearningRate <: HyperParameter\n Hyperparameter for the optimizer’s learning rate. See  Scheduler  and  hyperparameter scheduling ."},{"doctype":"documentation","id":"references/DataAugmentation.corners","title":"corners","text":""},{"doctype":"documentation","id":"references/FluxTraining.Metric","title":"Metric","text":" Metric(metricfn[; statistic, device, name])\n Implementation of  AbstractMetric  that can be used with the Metrics  callback. Arguments Positional: metricfn(ŷs, ys)  should return a number. Keyword: statistic  is a  OnlineStats.Statistic  that is updated after every step. The default is  OnlineStats.Mean() name  is used for printing. device  is a function applied to  ŷs  and  ys before passing them to  metricfn . The default is  Flux.cpu  so that the callback works if  metricfn  doesn’t support arrays from other device types. If, for example,  metricfn  works on  CurArray s, you can pass device = Flux.gpu . phase = Phase : a (sub)type of  Phase  that restricts for which phases the metric is computed. Examples Metric(accuracy) Metric(Flux.mse, device = gpu, name = \"Mean Squared Error\") Metric(Flux.mae, device = gpu) cb   =   Metric ( Flux . mse ,   device   =   gpu ,   name   =   \" Mean Squared Error \" ) If a metric is expensive to compute and you don’t want it to slow down the training phase, you can compute it on the validation phase only: cb   =   Metric ( expensivemetric ,   P   =   ValidationPhase )"},{"doctype":"document","id":"documents/notebooks/training.ipynb","title":"How to train a model","text":" How to train a model using   FastAI ,   Metalhead \n import   CairoMakie Finding a learning rate Using a good learning rate is important for a balance between model convergence and training speed, but finding one isn’t always easy. FastAI.jl includes a learning rate finder that runs a mock training run with increasing learning rates to find a good one. You can use it with  lrfind . data ,   blocks   =   loaddataset ( \" imagenette2-160 \" ,   ( Image ,   Label ) ) \n task   =   ImageClassificationSingle ( blocks ) \n learner   =   tasklearner ( task ,   data ;   callbacks = [ ToGPU ( ) ,   Metrics ( accuracy ) ] ) \n finderresult   =   lrfind ( learner ) plot ( finderresult ) Training a model from scratch When using randomly intialized models like  Models.xresnet18 , you can use  fitonecycle!  to train: data ,   blocks   =   loaddataset ( \" imagenette2-160 \" ,   ( Image ,   Label ) ) \n task   =   ImageClassificationSingle ( blocks ) \n learner   =   tasklearner ( task ,   data ;   callbacks = [ ToGPU ( ) ,   Metrics ( accuracy ) ] ) \n fitonecycle! ( learner ,   10 ,   0.0005 ) Finetuning a pretrained model When finetuning a pretrained model, it is recommended to use  finetune!  which uses a warmup schedule to train the newly initiliazed head more quickly than the pretrained backbone. data ,   blocks   =   loaddataset ( \" imagenette2-160 \" ,   ( Image ,   Label ) ) \n task   =   ImageClassificationSingle ( blocks ) \n learner   =   tasklearner ( task ,   data ; \n     backbone = Metalhead . ResNet50 ( pretrain = true ) . layers [ 1 ] [ 1 : end - 1 ] , \n     callbacks = [ ToGPU ( ) ,   Metrics ( accuracy ) ] ) \n finetune! ( learner ,   5 ,   0.0005 )"},{"doctype":"documentation","id":"references/Flux.modules","title":"modules","text":" modules(m)\n Return an iterator over non-leaf objects that can be reached by recursing  m  over the children given by  functor . Useful for applying a function (e.g. a regularizer) over specific modules or subsets of the parameters (e.g. the weights but not the biases). Examples julia> m1 = Chain(Dense(28^2, 64), BatchNorm(64, relu));\n\njulia> m2 = Chain(m1, Dense(64, 10))\nChain(\n  Chain(\n    Dense(784 => 64),                   # 50_240 parameters\n    BatchNorm(64, relu),                # 128 parameters, plus 128\n  ),\n  Dense(64 => 10),                      # 650 parameters\n)         # Total: 6 trainable arrays, 51_018 parameters,\n          # plus 2 non-trainable, 128 parameters, summarysize 200.312 KiB.\n\njulia> Flux.modules(m2)\n7-element Vector{Any}:\n Chain(Chain(Dense(784 => 64), BatchNorm(64, relu)), Dense(64 => 10))  # 51_018 parameters, plus 128 non-trainable\n (Chain(Dense(784 => 64), BatchNorm(64, relu)), Dense(64 => 10))\n Chain(Dense(784 => 64), BatchNorm(64, relu))  # 50_368 parameters, plus 128 non-trainable\n (Dense(784 => 64), BatchNorm(64, relu))\n Dense(784 => 64)    # 50_240 parameters\n BatchNorm(64, relu)  # 128 parameters, plus 128 non-trainable\n Dense(64 => 10)     # 650 parameters\n\njulia> L2(m) = sum(sum(abs2, l.weight) for l in Flux.modules(m) if l isa Dense)\nL2 (generic function with 1 method)\n\njulia> L2(m2) isa Float32\ntrue\n"},{"doctype":"documentation","id":"references/FastAI.tuplemap","title":"tuplemap","text":""},{"doctype":"documentation","id":"references/FastAI.default_showbackend","title":"default_showbackend","text":" default_showbackend()\n Return the default  ShowBackend  to use. If a Makie.jl backend is loaded (i.e.  Makie.current_backend[] !== missing ), return  ShowMakie . Else, return  ShowText ."},{"doctype":"documentation","id":"references/FastAI.Vision.IMAGENET_STDS","title":"IMAGENET_STDS","text":""},{"doctype":"documentation","id":"references/Flux.Losses._check_sizes","title":"_check_sizes","text":""},{"doctype":"documentation","id":"references/FastAI.Tabular.tabular_embedding_backbone","title":"tabular_embedding_backbone","text":""},{"doctype":"documentation","id":"references/FluxTraining._combinename","title":"_combinename","text":""},{"doctype":"documentation","id":"references/DataAugmentation.setdata","title":"setdata","text":""},{"doctype":"document","id":"documents/notebooks/serialization.ipynb","title":"Saving and loading models for inference","text":" Saving and loading models for inference In the end, we train models because we want to use them for inference, that is, using them to generate predictions on new targets. The general formula for doing this in FastAI.jl is to first train a  model  for a  task , for example using  fitonecycle!  or  finetune!  and then save the model and the learning task configuration to a file using  savetaskmodel . In another session you can then use  loadtaskmodel  to load both. Since the learning task contains all preprocessing logic we can then use  predict  and  predictbatch  to generate predictions for new inputs. Let’s fine-tune an image classification model (see  here  for more info) and go through that process. using   FastAI \n using   Metalhead dir   =   joinpath ( datasetpath ( \" dogscats \" ) ,   \" train \" ) \n data   =   loadfolderdata ( dir ,   filterfn = isimagefile ,   loadfn = ( loadfile ,   parentname ) ) \n classes   =   unique ( eachobs ( data [ 2 ] ) ) \n task   =   BlockTask ( \n     ( Image { 2 } ( ) ,   Label ( classes ) ) , \n     ( \n         ProjectiveTransforms ( ( 196 ,   196 ) ) , \n         ImagePreprocessing ( ) , \n         OneHot ( ) \n     ) \n ) \n \n backbone   =   Metalhead . ResNet50 ( pretrain = true ) . layers [ 1 ] [ 1 : end - 1 ] \n learner   =   tasklearner ( task ,   data ;   backbone = backbone ,   callbacks = [ ToGPU ( ) ,   Metrics ( accuracy ) ] ) \n finetune! ( learner ,   3 ) Now we can save the model using  savetaskmodel . savetaskmodel ( \" catsdogs.jld2 \" ,   task ,   learner . model ,   force   =   true ) In another session we can now use  loadtaskmodel  to load both model and learning task from the file. Since the model weights are transferred to the CPU before being saved, we need to move them to the GPU manually if we want to use that for inference. task ,   model   =   loadtaskmodel ( \" catsdogs.jld2 \" ) \n model   =   gpu ( model ) ; Finally, let’s select 9 random images from the dataset and see if the model classifies them correctly: # use it for inference \n x ,   y   =  \n samples   =   [ getobs ( data ,   i )   for   i   in   rand ( 1 : nobs ( data ) ,   9 ) ] \n images   =   [ sample [ 1 ]   for   sample   in   samples ] \n labels   =   [ sample [ 2 ]   for   sample   in   samples ] \n preds   =   predictbatch ( task ,   model ,   images ;   device   =   gpu ,   context   =   Validation ( ) ) acc   =   sum ( labels   .==   preds )   /   length ( preds ) using   CairoMakie \n plotsamples ( task ,   collect ( zip ( images ,   preds ) ) )"},{"doctype":"documentation","id":"references/FastAI.OneHotLabel","title":"OneHotLabel","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Sequence","title":"Sequence","text":" Sequence(transforms...)\n Transform  that applies multiple  transformations after each other. You should not use this explicitly. Instead use  compose ."},{"doctype":"documentation","id":"references/FastAI.showencodedsample","title":"showencodedsample","text":" showencodedsample([backend], task, encsample)\n Show an encoded sample  encsample  to  backend ."},{"doctype":"documentation","id":"references/FastAI.showencodedsamples","title":"showencodedsamples","text":" showencodedsamples([backend], task, encsamples)\n Show a vector of encoded samples  encsamples  to  backend ."},{"doctype":"documentation","id":"references/Flux.Losses.mae","title":"mae","text":" mae(ŷ, y; agg = mean)\n Return the loss corresponding to mean absolute error: agg(abs.(ŷ .- y))\n Example julia> y_model = [1.1, 1.9, 3.1];\n\njulia> Flux.mae(y_model, 1:3)\n0.10000000000000009\n"},{"doctype":"documentation","id":"references/Flux.kaiming_normal","title":"kaiming_normal","text":" kaiming_normal([rng=GLOBAL_RNG], size...; gain = √2) -> Array\nkaiming_normal([rng]; kw...) -> Function\n Return an  Array{Float32}  of the given  size  containing random numbers taken from a normal distribution standard deviation  gain / sqrt(fan_in) , using [ nfan ](  Flux.nfan). This method is described in [1] and also known as He initialization. Examples julia> using Statistics\n\njulia> round(std(Flux.kaiming_normal(10, 1000)), digits=3)\n0.045f0\n\njulia> round(std(Flux.kaiming_normal(1000, 10)), digits=3)\n0.447f0\n\njulia> round(std(Flux.kaiming_normal(1000, 1000)), digits=3)\n0.045f0\n References [1] He, Kaiming, et al. “Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.”  Proceedings of the IEEE international conference on computer vision . 2015."},{"doctype":"documentation","id":"references/Flux.GRU","title":"GRU","text":" GRU(in => out)\n Gated Recurrent Unit  layer. Behaves like an RNN but generally exhibits a longer memory span over sequences. This implements the variant proposed in v1 of the referenced paper. The integer arguments  in  and  out  describe the size of the feature vectors passed as input and as output. That is, it accepts a vector of length  in  or a batch of vectors represented as a  in x B  matrix and outputs a vector of length  out  or a batch of vectors of size  out x B . This constructor is syntactic sugar for  Recur(GRUCell(a...)) , and so GRUs are stateful. Note that the state shape can change depending on the inputs, and so it is good to  reset!  the model between inference calls if the batch size changes. See the examples below. See  this article for a good overview of the internals. Examples julia> g = GRU(3 => 5)\nRecur(\n  GRUCell(3 => 5),                      # 140 parameters\n)         # Total: 4 trainable arrays, 140 parameters,\n          # plus 1 non-trainable, 5 parameters, summarysize 792 bytes.\n\njulia> g(rand(Float32, 3)) |> size\n(5,)\n\njulia> Flux.reset!(g);\n\njulia> g(rand(Float32, 3, 10)) |> size # batch size of 10\n(5, 10)\n Batch size changes Failing to call  reset!  when the input batch size changes can lead to unexpected behavior. See the example in  RNN ."},{"doctype":"documentation","id":"references/Flux.FluxCPUAdaptor","title":"FluxCPUAdaptor","text":""},{"doctype":"documentation","id":"references/FluxTraining.stateaccess","title":"stateaccess","text":" stateaccess(callback)\n Return a named tuple determining what learner state  callback can access. The default is  (;) , the empty named tuple, meaning no state can be accessed. Implementations of  stateaccess  should always return the least permissions possible. Extending For example, the  ToGPU  callback needs to write both the model and the batch data, so its  stateaccess  implementation is: stateaccess ( :: ToGPU )   =   ( \n     model   =   Write ( ) , \n     params   =   Write ( ) , \n     step   =   ( xs   =   Write ( ) ,   ys   =   Write ( ) ) , \n ) When defining  stateaccess , be careful that you do return a  NamedTuple . (x = Read(),)  is one but  (x = Read())  (without the comma) is parsed as an assignment with value  Read() . stateaccess(::Type{HyperParameter})\n Defines what  Learner  state is accessed when calling sethyperparameter!  and  gethyperparameter . This is needed so that  Scheduler  can access the state."},{"doctype":"documentation","id":"references/DataAugmentation.fmap","title":"fmap","text":""},{"doctype":"documentation","id":"references/FastAI.showbatch","title":"showbatch","text":" showbatch([backend], task, batch)\n Show a collated batch of encoded samples to  backend ."},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Graph","title":"Graph","text":""},{"doctype":"documentation","id":"references/Flux.LayerNorm","title":"LayerNorm","text":" LayerNorm(size..., λ=identity; affine=true, ϵ=1fe-5)\n A  normalisation layer  designed to be used with recurrent hidden states. The argument  sz  should be an integer or a tuple of integers. In the forward pass, the layer normalises the mean and standard deviation of the input, the applied the elementwise activation  λ . The input is normalised along the first  length(sz)  dimensions for tuple  sz , along the first dimension for integer  sz . The input  is expected to have first dimensions’ size equal to  sz . If  affine=true  also applies a learnable shift and rescaling using the  Scale  layer. See also  BatchNorm ,  InstanceNorm ,  GroupNorm , and  normalise ."},{"doctype":"documentation","id":"references/FastAI.withfields","title":"withfields","text":" withfields(f, x; kwargs...)\n Replace fields on  x  with given keyword arguments, run  f  and then restore the fields.  x  needs to be a  mutable struct . Every keyword argument is a mapping  (field, value)  or  (field, (setfn!, value)) . setfn!(x, val)  will be used to set the field; if as in the first case none is given  setfield!  is used."},{"doctype":"documentation","id":"references/FastAI.Datasets.loadmask","title":"loadmask","text":" loadmask(file, classes)\n Load a segmentation mask from an image file. Returns an efficiently stored array of type  eltype(classes) ."},{"doctype":"documentation","id":"references/Flux.Optimise.skip","title":"skip","text":" skip()\n Call  Flux.skip()  in a callback to indicate when a callback condition is met. This will trigger the train loop to skip the current data point and not update with the calculated gradient. Examples cb   =   function   ( ) \n   loss ( )   >   1e7   &&   Flux . skip ( ) \n end"},{"doctype":"documentation","id":"references/FluxTraining.Events","title":"Events","text":""},{"doctype":"documentation","id":"references/FastAI.mocktarget","title":"mocktarget","text":" mocktarget(task)\n Generate a random  target  compatible with  task ."},{"doctype":"documentation","id":"references/FastAI.Vision.getsamplebounds","title":"getsamplebounds","text":""},{"doctype":"documentation","id":"references/Flux._childarray_sum","title":"_childarray_sum","text":""},{"doctype":"documentation","id":"references/Flux.use_cuda","title":"use_cuda","text":""},{"doctype":"documentation","id":"references/FluxTraining.metricname","title":"metricname","text":""},{"doctype":"documentation","id":"references/FastAI.listdecodeblocks","title":"listdecodeblocks","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.AdaBelief","title":"AdaBelief","text":" AdaBelief(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = \n"},{"doctype":"documentation","id":"references/Flux.RNN","title":"RNN","text":" RNN(in => out, σ = tanh)\n The most basic recurrent layer; essentially acts as a  Dense  layer, but with the output fed back into the input each time step. The arguments  in  and  out  describe the size of the feature vectors passed as input and as output. That is, it accepts a vector of length  in  or a batch of vectors represented as a  in x B  matrix and outputs a vector of length  out  or a batch of vectors of size  out x B . This constructor is syntactic sugar for  Recur(RNNCell(a...)) , and so RNNs are stateful. Note that the state shape can change depending on the inputs, and so it is good to  reset!  the model between inference calls if the batch size changes. See the examples below. Examples julia> r = RNN(3 => 5)\nRecur(\n  RNNCell(3 => 5, tanh),                # 50 parameters\n)         # Total: 4 trainable arrays, 50 parameters,\n          # plus 1 non-trainable, 5 parameters, summarysize 432 bytes.\n\njulia> r(rand(Float32, 3)) |> size\n(5,)\n\njulia> Flux.reset!(r);\n\njulia> r(rand(Float32, 3, 10)) |> size # batch size of 10\n(5, 10)\n Batch size changes Failing to call  reset!  when the input batch size changes can lead to unexpected behavior. See the following example: julia >   r   =   RNN ( 3   =>   5 ) \n Recur ( \n   RNNCell ( 3   =>   5 ,   tanh ) ,                  # 50 parameters \n )           # Total: 4 trainable arrays, 50 parameters, \n           # plus 1 non-trainable, 5 parameters, summarysize 432 bytes. \n \n julia >   r . state   |>   size \n ( 5 ,   1 ) \n \n julia >   r ( rand ( Float32 ,   3 ) )   |>   size \n ( 5 , ) \n \n julia >   r . state   |>   size \n ( 5 ,   1 ) \n \n julia >   r ( rand ( Float32 ,   3 ,   10 ) )   |>   size   # batch size of 10 \n ( 5 ,   10 ) \n \n julia >   r . state   |>   size   # state shape has changed \n ( 5 ,   10 ) \n \n julia >   r ( rand ( Float32 ,   3 ) )   |>   size   # erroneously outputs a length 5*10 = 50 vector. \n ( 50 , )"},{"doctype":"documentation","id":"references/FluxTraining.History","title":"History","text":""},{"doctype":"documentation","id":"references/FluxTraining.Scheduler","title":"Scheduler","text":" Scheduler(schedules...)\n Callback for hyperparameter scheduling. Takes pairs of  HyperParameter types and  Schedule s. See  the tutorial  for more information. Example es   =   length ( learner . data . training ) \n lrschedule   =   Schedule ( [ 0  es ,   10  es ] ,   [ 0.1 ,   0.001 ] ,   Animations . sineio ( ) ) \n scheduler   =   Scheduler ( \n     LearningRate   =>   lrschedule \n ) See also  Schedule ."},{"doctype":"documentation","id":"references/Flux._gru_output","title":"_gru_output","text":""},{"doctype":"documentation","id":"references/FluxTraining.Checkpointer","title":"Checkpointer","text":" Checkpointer(folder)\n Saves  learner.model  to  folder  after every  AbstractTrainingPhase . Use  FluxTraining. loadmodel  to load a model."},{"doctype":"documentation","id":"references/Flux.gate","title":"gate","text":""},{"doctype":"documentation","id":"references/Flux.frequencies","title":"frequencies","text":""},{"doctype":"documentation","id":"references/Flux._onehotindex","title":"_onehotindex","text":""},{"doctype":"documentation","id":"references/FastAI.encode","title":"encode","text":" encode(encoding, context, block, obs)\nencode(encoding, context, blocks, obss)\nencode(encodings, context, blocks, obss)\n Apply one or more  Encoding s to observation(s)  obs ."},{"doctype":"documentation","id":"references/FastAI.OneHotTensorMulti","title":"OneHotTensorMulti","text":""},{"doctype":"documentation","id":"references/FastAI._predictx","title":"_predictx","text":""},{"doctype":"documentation","id":"references/DataAugmentation.MaskBinary","title":"MaskBinary","text":" MaskBinary(a)\n An  N -dimensional binary mask. Examples using   DataAugmentation \n \n mask   =   MaskBinary ( rand ( Bool ,   100 ,   100 ) ) showitems ( mask )"},{"doctype":"documentation","id":"references/FastAI.smoothvalues","title":"smoothvalues","text":" smoothvalues(xs, β)\n Apply exponential smoothing with parameter  β  to vector  xs ."},{"doctype":"documentation","id":"references/DataAugmentation.AbstractCrop","title":"AbstractCrop","text":""},{"doctype":"documentation","id":"references/FastAI.Tabular.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.TESTSET_MACROS","title":"TESTSET_MACROS","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.__init__","title":"__init__","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.FastAIDataset","title":"FastAIDataset","text":""},{"doctype":"documentation","id":"references/FastAI._decode","title":"_decode","text":""},{"doctype":"documentation","id":"references/FluxTraining.phasedataiter","title":"phasedataiter","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.grabbounds","title":"grabbounds","text":" grabbounds(blocks, obss, N)\n Looks through  blocks  for block data that carries  N -dimensional bounds information needed for projective transformations."},{"doctype":"documentation","id":"references/FastAI.Vision.Models.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.__init__","title":"__init__","text":""},{"doctype":"documentation","id":"references/FastAI.Validation","title":"Validation","text":" Validation <: Context\n A context for applying data transformations during validation/testing. Encoding s and  LearningTask s can dispatch on this when certain transformations, like random augmentations, should not be applied during validation, only in training."},{"doctype":"documentation","id":"references/FastAI.Block","title":"Block","text":" abstract type Block\n A block describes the meaning of a piece of data in the context of a learning task. For example, for supervised learning tasks, there is an input and a target and we want to learn to predict targets from inputs. Learning to predict a cat/dog label from 2D images is a supervised image classification task that can be represented with the  Block s  Image{2}()  and  Label([\"cat\", \"dog\"]) . Block s are used in virtually every part of the high-level interfaces, from data processing over model creation to visualization. Extending Consider the following when subtyping  Block . A block Does not hold observation data itself. Instead they are used in conjunction with data to annotate it with some meaning. If it has any fields, they should be metadata that cannot be derived from the data itself and is constant for every sample in the dataset. For example  Label  holds all possible classes which are constant for the learning problem. Interfaces There are many interfaces that can be implemented for a  Block . See the docstrings of each function for more info about how to implement it. checkblock (block, obs) : check whether an observation is a valid block mockblock (block) : randomly generate an observation blocklossfn (predblock, yblock) : loss function for comparing two blocks blockmodel (inblock, outblock[, backbone]) : construct a task-specific model blockbackbone (inblock) : construct a backbone model that takes in specific data showblock! (block, obs) : visualize an observation"},{"doctype":"documentation","id":"references/Flux.Losses.poisson_loss","title":"poisson_loss","text":" poisson_loss(ŷ, y)\n Return how much the predicted distribution  ŷ  diverges from the expected Poisson distribution  y ; calculated as  sum(ŷ .- y .* log.(ŷ)) / size(y, 2) . More information . ."},{"doctype":"documentation","id":"references/DataAugmentation._channelview","title":"_channelview","text":""},{"doctype":"documentation","id":"references/Flux.Losses.msle","title":"msle","text":" msle(ŷ, y; agg = mean, ϵ = eps(ŷ))\n The loss corresponding to mean squared logarithmic errors, calculated as agg((log.(ŷ .+ ϵ) .- log.(y .+ ϵ)) .^ 2)\n The  ϵ  term provides numerical stability. Penalizes an under-estimation more than an over-estimatation. Example julia> Flux.msle(Float32[1.1, 2.2, 3.3], 1:3)\n0.009084041f0\n\njulia> Flux.msle(Float32[0.9, 1.8, 2.7], 1:3)\n0.011100831f0\n"},{"doctype":"documentation","id":"references/Flux.loadleaf!","title":"loadleaf!","text":""},{"doctype":"documentation","id":"references/FastAI.SupervisedTask","title":"SupervisedTask","text":" SupervisedTask((inputblock, targetblock), encodings)\n A  AbstractBlockTask  learning task for the supervised task of learning to predict a  target  given an  input .  encodings are applied to samples before being input to the model. Model outputs are decoded using those same encodings to get a target prediction. In addition, to the blocks defined by  AbstractBlockTask , getblocks(::SupervisedTask)  defines the following blocks: By default the model output is assumed to be an encoded target, but the ŷblock  keyword argument to overwrite this. blocks.input : An unencoded input and the first element in the tuple sample = (input, target) blocks.target : An unencoded target and the second element in the tuple sample = (input, target) blocks.pred : A prediction. Usually the same as  blocks.target  but may differ if a custom  ŷblock  is specified. A  SupervisedTask  also enables some additional functionality: encodeinput encodetarget showprediction ,  showpredictions"},{"doctype":"documentation","id":"references/FastAI.Datasets.rglob","title":"rglob","text":" rglob(filepattern, dir = pwd(), depth = 4)\n Recursive glob up to 6 layers deep."},{"doctype":"documentation","id":"references/FluxTraining.epochvalue","title":"epochvalue","text":""},{"doctype":"documentation","id":"references/Flux.Losses.compute_beta_and_grad_kernel","title":"compute_beta_and_grad_kernel","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.stop","title":"stop","text":" stop()\n Call  Flux.stop()  in a callback to indicate when a callback condition is met. This will trigger the train loop to stop and exit. Examples cb   =   function   ( ) \n   accuracy ( )   >   0.9   &&   Flux . stop ( ) \n end"},{"doctype":"documentation","id":"references/Flux.Optimise.ADADelta","title":"ADADelta","text":" ADADelta(ρ = 0.9, ϵ = \n"},{"doctype":"documentation","id":"references/FastAI.Training","title":"Training","text":" Training <: Context\n A context for applying data transformations during training.  Encoding s and LearningTask s can dispatch on this when certain transformations, like random augmentations, should only be applied during training."},{"doctype":"documentation","id":"references/DataAugmentation.RandomCrop","title":"RandomCrop","text":""},{"doctype":"documentation","id":"references/FastAI","title":"FastAI","text":""},{"doctype":"documentation","id":"references/Flux.GRUv3","title":"GRUv3","text":" GRUv3(in => out)\n Gated Recurrent Unit  layer. Behaves like an RNN but generally exhibits a longer memory span over sequences. This implements the variant proposed in v3 of the referenced paper. The arguments  in  and  out  describe the size of the feature vectors passed as input and as output. That is, it accepts a vector of length  in  or a batch of vectors represented as a  in x B  matrix and outputs a vector of length  out  or a batch of vectors of size  out x B . This constructor is syntactic sugar for  Recur(GRUv3Cell(a...)) , and so GRUv3s are stateful. Note that the state shape can change depending on the inputs, and so it is good to  reset!  the model between inference calls if the batch size changes. See the examples below. See  this article for a good overview of the internals. Examples julia> g = GRUv3(3 => 5)\nRecur(\n  GRUv3Cell(3 => 5),                    # 140 parameters\n)         # Total: 5 trainable arrays, 140 parameters,\n          # plus 1 non-trainable, 5 parameters, summarysize 848 bytes.\n\njulia> g(rand(Float32, 3)) |> size\n(5,)\n\njulia> Flux.reset!(g);\n\njulia> g(rand(Float32, 3, 10)) |> size # batch size of 10\n(5, 10)\n Batch size changes Failing to call  reset!  when the input batch size changes can lead to unexpected behavior. See the example in  RNN ."},{"doctype":"documentation","id":"references/DataAugmentation.Rotate","title":"Rotate","text":" Rotate(γ)\nRotate(γs)\n Rotate 2D spatial data around the center by an angle chosen at uniformly from [-γ, γ], an angle given in degrees. You can also pass any  Distributions.Sampleable  from which the angle is selected. Examples tfm   =   Rotate ( 10 )"},{"doctype":"documentation","id":"references/Flux.GroupNorm","title":"GroupNorm","text":" GroupNorm(channels::Integer, G::Integer, λ=identity;\n          initβ=zeros32, initγ=ones32,\n          affine=true, track_stats=false,\n          ϵ=1f-5, momentum=0.1f0)\n Group Normalization  layer. chs  is the number of channels, the channel dimension of your input. For an array of N dimensions, the  N-1 th index is the channel dimension. G  is the number of groups along which the statistics are computed. The number of channels must be an integer multiple of the number of groups. channels  should be the size of the channel dimension in your data (see below). Given an array with  N > 2  dimensions, call the  N-1 th the channel dimension. For  WHCN  images it’s the usual channel dimension. If  affine=true , it also applies  a shift and a rescale to the input through to learnable per-channel bias  β  and scale  γ  parameters. If  track_stats=true , accumulates mean and var statistics in training phase that will be used to renormalize the input in test phase."},{"doctype":"documentation","id":"references/Flux._indices","title":"_indices","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.Models.catchannels","title":"catchannels","text":""},{"doctype":"documentation","id":"references/DataAugmentation.OneOfProjective","title":"OneOfProjective","text":""},{"doctype":"documentation","id":"references/DataAugmentation.BufferedThreadsafe","title":"BufferedThreadsafe","text":""},{"doctype":"documentation","id":"references/FluxTraining.accesses","title":"accesses","text":" accesses()\n Enumerate all valid state accesses of permissions of kind  perm . accesses((x = Read(),), Read()) === [(:x,)] accesses((x = Read(),), Write()) === []"},{"doctype":"documentation","id":"references/DataAugmentation.NormalizeRow","title":"NormalizeRow","text":" NormalizeRow(dict, cols)\n Normalizes the values of a row present in  TabularItem  for the columns specified in  cols  using  dict , which contains the column names as dictionary keys and the mean and standard deviation tuple present as dictionary values. Example using   DataAugmentation \n \n cols   =   [ : col1 ,   : col2 ,   : col3 ] \n row   =   ( ;   zip ( cols ,   [ 1 ,   2 ,   3 ] ) ... ) \n item   =   TabularItem ( row ,   cols ) \n normdict   =   Dict ( : col1   =>   ( 1 ,   1 ) ,   : col2   =>   ( 2 ,   2 ) ) \n \n tfm   =   NormalizeRow ( normdict ,   [ : col1 ,   : col2 ] ) \n apply ( tfm ,   item )"},{"doctype":"documentation","id":"references/Flux.Optimise.update!","title":"update!","text":" update!(opt, p, g)\nupdate!(opt, ps::Params, gs)\n Perform an update step of the parameters  ps  (or the single parameter  p ) according to optimizer  opt   and the gradients  gs  (the gradient  g ). As a result, the parameters are mutated and the optimizer’s internal state may change. The gradient could be mutated as well."},{"doctype":"documentation","id":"references/FastAI.Vision.ImageTensor","title":"ImageTensor","text":" ImageTensor{N} <: Block\n Block for N+1-dimensional arrays representing an N-dimensional image with the color channels expanded."},{"doctype":"documentation","id":"references/FastAI.Only","title":"Only","text":" Only(fn, encoding)\nOnly(BlockType, encoding)\nOnly(name, encoding)\n Wrapper that applies the wrapped  encoding  to a  block  if fn(block) === true . Instead of a function you can also pass in a type of block  BlockType  or the  name  of a  Named  block."},{"doctype":"documentation","id":"references/FastAI.Tabular.EncodedTableRow","title":"EncodedTableRow","text":" EncodedTableRow{M, N} <: Block\n Block for processed rows having a tuple of M categorical and N continuous value collections."},{"doctype":"documentation","id":"references/DataAugmentation.imagetotensor!","title":"imagetotensor!","text":""},{"doctype":"documentation","id":"references/Flux.SamePad","title":"SamePad","text":" SamePad()\n Passed as an option to convolutional layers (and friends), this causes the padding to be chosen such that the input and output sizes agree (on the first  N  dimensions, the kernel or window) when  stride==1 . See also  Conv ,  MaxPool ."},{"doctype":"documentation","id":"references/FluxTraining.PropDict","title":"PropDict","text":" PropDict(dict)\n Like a  Dict{Symbol} , but attribute syntax can be used to access values."},{"doctype":"documentation","id":"references/Flux.Optimise.Optimiser","title":"Optimiser","text":" Optimiser(a, b, c...)\n Combine several optimisers into one; each optimiser produces a modified gradient that will be fed into the next, and this is finally applied to the parameter as usual."},{"doctype":"documentation","id":"references/FastAI.Datasets.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.TESTSET_MACROS","title":"TESTSET_MACROS","text":""},{"doctype":"documentation","id":"references/DataAugmentation.getwrapped","title":"getwrapped","text":""},{"doctype":"documentation","id":"references/FastAI.showoutput","title":"showoutput","text":" showoutput([backend], task, output)\nshowoutput([backend], task, encsample, output)\n Show a model output to  backend . If an encoded sample  encsample  is also given, show it next to the output."},{"doctype":"documentation","id":"references/FluxTraining.fit!","title":"fit!","text":" fit!(learner, nepochs)\nfit!(learner, nepochs, (trainiter, validiter))\n Train  learner  for  nepochs  of training and validation each. Use data iterators that are passed in. If none are given, use  learner.data.training and  learner.data.validation . Examples fit! ( learner ,   10 ) \n fit! ( learner ,   10 ,   ( traindl ,   valdl ) )"},{"doctype":"documentation","id":"references/FluxTraining.SanityCheckException","title":"SanityCheckException","text":""},{"doctype":"documentation","id":"references/FluxTraining.UnsafeCallback","title":"UnsafeCallback","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets","title":"Datasets","text":""},{"doctype":"documentation","id":"references/FluxTraining.Permission","title":"Permission","text":""},{"doctype":"documentation","id":"references/Flux.calc_padding","title":"calc_padding","text":""},{"doctype":"documentation","id":"references/DataAugmentation.normalize","title":"normalize","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.Models.convxlayer","title":"convxlayer","text":""},{"doctype":"documentation","id":"references/Flux.OneHotMatrix","title":"OneHotMatrix","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.DimSize","title":"DimSize","text":""},{"doctype":"documentation","id":"references/FluxTraining.removecallback!","title":"removecallback!","text":" removecallback!(learner, C)\n Remove the first callback of type  C  from  learner  and return it. If there is none, return  nothing ."},{"doctype":"documentation","id":"references/FastAI.Vision.Models.convx","title":"convx","text":""},{"doctype":"documentation","id":"references/DataAugmentation.testapply","title":"testapply","text":" testapply(tfm, item)\ntestapply(tfm, I)\n Test  apply  invariants of  tfm  on  item  or item type  I . With a constant  randstate  parameter,  apply  should always return the same result."},{"doctype":"documentation","id":"references/FluxTraining.addcallback!","title":"addcallback!","text":" addcallback!(learner, callback)\n Adds  callback  to  learner  and updates the dependency graph."},{"doctype":"documentation","id":"references/FastAI.Tabular.removecol","title":"removecol","text":""},{"doctype":"documentation","id":"references/FluxTraining.CheckDataIteratorTrain","title":"CheckDataIteratorTrain","text":""},{"doctype":"documentation","id":"references/FastAI.showblocks","title":"showblocks","text":""},{"doctype":"documentation","id":"references/DataAugmentation.MaskMulti","title":"MaskMulti","text":" MaskMulti(a, [classes])\n An  N -dimensional multilabel mask with labels  classes . Examples using   DataAugmentation \n \n mask   =   MaskMulti ( rand ( 1 : 3 ,   100 ,   100 ) ) showitems ( mask )"},{"doctype":"documentation","id":"references/FluxTraining.Events.BackwardBegin","title":"BackwardBegin","text":" BackwardBegin()\n Event  called between calculating loss and calculating gradients"},{"doctype":"documentation","id":"references/FastAI.Datasets.FASTAI_DATA_REGISTRY","title":"FASTAI_DATA_REGISTRY","text":" const FASTAI_DATA_REGISTRY\n The default  DataRegistry  containing every dataset in the fastai dataset collection."},{"doctype":"documentation","id":"references/FastAI.encodeinput","title":"encodeinput","text":""},{"doctype":"documentation","id":"references/DataAugmentation.offsetcropbounds","title":"offsetcropbounds","text":" offsetcropbounds(sz, bounds, offsets)\n Calculate offset bounds for a crop of size  sz . For every dimension i where  sz[i] < length(indices[i]) , offsets the crop by  offsets[i]  times the difference between the two."},{"doctype":"documentation","id":"references/DataAugmentation._default_digits","title":"_default_digits","text":""},{"doctype":"documentation","id":"references/Flux.Losses.logitcrossentropy","title":"logitcrossentropy","text":" logitcrossentropy(ŷ, y; dims = 1, agg = mean)\n Return the cross entropy calculated by agg(-sum(y .* logsoftmax(ŷ; dims); dims))\n This is mathematically equivalent to  crossentropy(softmax(ŷ), y) , but is more numerically stable than using functions  crossentropy and  softmax  separately. See also:  binarycrossentropy ,  logitbinarycrossentropy ,  label_smoothing . Example julia> y_label = Flux.onehotbatch(collect(\"abcabaa\"), 'a':'c')\n3×7 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  ⋅  ⋅  1  ⋅  1  1\n ⋅  1  ⋅  ⋅  1  ⋅  ⋅\n ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅\n\njulia> y_model = reshape(vcat(-9:0, 0:9, 7.5f0), 3, 7)\n3×7 Matrix{Float32}:\n -9.0  -6.0  -3.0  0.0  2.0  5.0  8.0\n -8.0  -5.0  -2.0  0.0  3.0  6.0  9.0\n -7.0  -4.0  -1.0  1.0  4.0  7.0  7.5\n\njulia> Flux.logitcrossentropy(y_model, y_label)\n1.5791205f0\n\njulia> Flux.crossentropy(softmax(y_model), y_label)\n1.5791197f0\n"},{"doctype":"documentation","id":"references/FastAI.listencodeblocks","title":"listencodeblocks","text":""},{"doctype":"documentation","id":"references/FastAI.LabelMulti","title":"LabelMulti","text":" LabelMulti(classes)\nsetup(LabelMulti, data)\n Block  for a categorical label in a multi-class context. data  is valid for  Label(classes)  if  data ∈ classes . Examples block   =   Label ( [ \" cat \" ,   \" dog \" ] )    # an observation can be either \"cat\" or \"dog\" \n @ test   FastAI . checkblock ( block ,   \" cat \" ) \n @ test   ! ( FastAI . checkblock ( block ,   \" horsey \" ) ) You can use  setup  to create a  Label  instance from a data container containing possible classes: targets   =   [ \" cat \" ,   \" dog \" ,   \" dog \" ,   \" dog \" ,   \" cat \" ,   \" dog \" ] \n block   =   setup ( Label ,   targets ) \n @ test   block   ≈   Label ( [ \" cat \" ,   \" dog \" ] )"},{"doctype":"documentation","id":"references/FluxTraining.LogHistograms","title":"LogHistograms","text":" LogHistograms(backends...[; freq = 100]) <: Callback\n Callback that logs histograms of model weights to  LoggerBackend s backends  every  freq  steps. If histograms should be logged every step, pass  freq = nothing"},{"doctype":"documentation","id":"references/FluxTraining.shouldrun","title":"shouldrun","text":""},{"doctype":"documentation","id":"references/Flux.Chain","title":"Chain","text":" Chain(layers...)\nChain(name = layer, ...)\n Collects multiple layers / functions to be called in sequence on a given input. Supports indexing and slicing,  m[2]  or  m[1:end-1] , and if names are given,  m[:name] == m[1]  etc. Examples julia> m = Chain(x -> x^2, x -> x+1);\n\njulia> m(5) == 26\ntrue\n\njulia> m = Chain(Dense(10 => 5, tanh), Dense(5 => 2));\n\njulia> x = rand(10, 32);\n\njulia> m(x) == m[2](m[1](x))\ntrue\n\njulia> m2 = Chain(enc = Chain(Flux.flatten, Dense(10 => 5, tanh)), \n                  dec = Dense(5 => 2));\n\njulia> m2(x) == (m2[:dec] ∘ m2[:enc])(x)\ntrue\n For large models, there is a special type-unstable path which can reduce compilation times. This can be used by supplying a vector of layers  Chain([layer1, layer2, ...]) . This feature is somewhat experimental, beware!"},{"doctype":"documentation","id":"references/FluxTraining.init!","title":"init!","text":" init!(callback, learner)\n Initialize a callback. Default is to do nothing. Extending To extend for a callback, implement  init!(cb::MyCallback, learner) . init!  can set up internal state of a callback that depends on  learner and can also initialize shared callback state in  learner.cbstate . Just like  on  event handlers, the state access permissions must be correctly defined using  stateaccess  to do so. init!  must also be idempotent, i.e. running it twice on the same  Learner should have the same effect as runnning it once."},{"doctype":"documentation","id":"references/FastAI.decodedblockfilled","title":"decodedblockfilled","text":""},{"doctype":"documentation","id":"references/FluxTraining.setfieldperm!","title":"setfieldperm!","text":""},{"doctype":"documentation","id":"references/FluxTraining.getfieldperm","title":"getfieldperm","text":""},{"doctype":"documentation","id":"references/Flux.GRUCell","title":"GRUCell","text":""},{"doctype":"documentation","id":"references/FluxTraining.CHECKS","title":"CHECKS","text":""},{"doctype":"documentation","id":"references/FastAI.registerlearningtask!","title":"registerlearningtask!","text":" registerlearningtask!(registry, taskfn, blocktypes)\n Register a learning task constructor  taskfn  as compatible with blocktypes  in  registry::LearningTaskRegistry . blocks  should be the least specific set of types that a  taskfn can handle.  taskfn  needs to have a task that takes concrete block instances as the only non-keyword argument, i.e.  taskfn(blocks; kwargs...) ."},{"doctype":"documentation","id":"references/FluxTraining.Metrics","title":"Metrics","text":" Metrics(metrics...) <: Callback\n Callback that tracks metrics during training. You can pass any number of  metrics  with every argument being an  AbstractMetric  like  Metric ; or a function  f(ŷs, ys) -> val This callback is added by default to every  Learner  unless you pass in usedefaultcallbacks = false . A metric tracking  learner.lossfn   Loss is included by default. The computed metrics can be access in  learner.cbstate.metricsstep  and learner.cbstate.metricsepoch  for steps and epochs, respectively. Examples Track  accuracy : cb   =   Metrics ( accuracy ) Pass in [ Metric ]s: cb   =   Metrics ( \n     Metric ( Flux . mse ,   device   =   gpu ) , \n     Metric ( Flux . mae ,   device   =   gpu ) \n )"},{"doctype":"documentation","id":"references/Flux.Losses.hinge_loss","title":"hinge_loss","text":" hinge_loss(ŷ, y; agg = mean)\n Return the  hinge _ loss loss  given the prediction  ŷ  and true labels  y  (containing 1 or -1); calculated as sum(max.(0, 1 .- ŷ .* y)) / size(y, 2) . See also:  squared_hinge_loss"},{"doctype":"documentation","id":"references/FastAI.Vision.augs_projection","title":"augs_projection","text":" augs_projection([; kwargs...])\n Helper to create a set of projective transformations for image, mask and keypoint data. Similar to fastai’s aug_transforms . Keyword arguments flipx = true : Whether to perform a horizontal flip with probability  1/2 . See  FlipX . flipy = false : Whether to perform a vertical flip with probability  1/2 . See  FlipY . max_zoom = 1.5 : Maximum factor by which to zoom. Set to  1.  to disable. See  Zoom . max_rotate = 10 : Maximum absolute degree by which to rotate. Set to  0.  to disable. See  Rotate . max_warp = 0.05 : Intensity of corner warp. Set to  0.  to disable. See  WarpAffine ."},{"doctype":"documentation","id":"references/FastAI.Tabular.getcoltypes","title":"getcoltypes","text":" getcoltypes(td::Datasets.TableDataset)\n Returns the categorical and continuous columns present in a  TableDataset ."},{"doctype":"documentation","id":"references/FastAI.findlearningtasks","title":"findlearningtasks","text":" findlearningtasks(blocktypes)\nfindlearningtasks(registry, blocktypes)\n Find learning tasks compatible with block types  TBlocks  in registry::LearningTaskRegistry . Examples julia> findlearningtasks((Image, Label))\n[ImageClassificationSingle,]\n\njulia> findlearningtasks((Image, Any))\n[ImageClassificationSingle, ImageClassificationMulti, ImageSegmentation, ImageKeypointRegression, ...]\n"},{"doctype":"documentation","id":"references/FastAI.Vision.Models.xresnet50","title":"xresnet50","text":""},{"doctype":"document","id":"documents/notebooks/vae.ipynb","title":"Variational autoencoders","text":" Variational autoencoders So far, we’ve covered many examples of how to train models in a supervised fashion. However, there are many applications of neural networks outside the supervised regime. In this tutorial, we’ll implement and train a variational autoencoder (VAE) to embed and generate images from the MNIST dataset. You’ll learn how to implement custom Flux.jl models write a custom training loop generate new images and visualize them using   FastAI ,   StaticArrays ,   Colors \n using   FastAI :   FluxTraining ,   Image \n import   CairoMakie    # run if in an interactive environment like a notebook Setting up the data First we load the MNIST dataset. Since we’re not using it for supervised learning, we only need the input images, and don’t load the labels. We get a data container where every observation is an image: path   =   datasetpath ( \" mnist_tiny \" ) \n data   =   Datasets . loadfolderdata ( \n     path , \n     filterfn   =   isimagefile , \n     loadfn   =   loadfile , \n ) \n showblock ( FastAI . Vision . Image { 2 } ( ) ,   getobs ( data ,   1 ) ) Next we need to define a learning task that will handle data encoding and decoding as well as visualization for us. So far, we’ve used  SupervisedTask  a lot which assumes there is an input that is fed to the model and a corresponding target output. Since we want to do unsupervised learning, we’ll instead create a custom learning task using  BlockTask . It defines what kind of data we’ll have at each step in the data pipeline for example  x  is a model input and  ŷ  a model output. See  AbstractBlockTask  for more info. using   FastAI :   encodedblockfilled ,   decodedblockfilled \n \n function   EmbeddingTask ( block ,   encodings ) \n     sample   =   block \n     encodedsample   =   x   =   y   =   ŷ   =   encodedblockfilled ( encodings ,   sample ) \n     blocks   =   ( ;   sample ,   x ,   y ,   ŷ ,   encodedsample ) \n     BlockTask ( blocks ,   encodings ) \n end With this helper defined, we can create a learning task for our task:  Image{2}()  is the kind of data we want to learn with and  ImagePreprocessing  makes sure to encode and decode these images so they can be used to train a model. task   =   EmbeddingTask ( \n     Image { 2 } ( ) , \n     ( ImagePreprocessing ( means   =   SVector ( 0.0 ) ,   stds   =   SVector ( 1.0 ) ,   C   =   Gray { Float32 } ) , ) , \n ) With the learning task set up, we can use  encode  to get samples ready to be input to a model, and all  show*  functions to visualize data at various points of the pipeline: x   =   encodesample ( task ,   Training ( ) ,   getobs ( data ,   1 ) ) \n showencodedsample ( task ,   x ) For later training, the last thing we need to do with the data is to create a data iterator over batches of encoded samples. Since the dataset comfortably fits into memory, we preload it all at once by using  collect  on the  DataLoader . This saves us having to reload each image again every epoch. BATCHSIZE   =   nobs ( data ) \n dataloader   =   DataLoader ( taskdataset ( shuffleobs ( data ) ,   task ,   Training ( ) ) ,   BATCHSIZE ) \n dataiter   =   collect ( dataloader ) With that, we have a data iterator of batches that we can use in a training loop just by iterating over it: for   xs   in   dataiter \n     print ( size ( xs ) ) \n     break \n end Next, we need to construct a model and define a loss function so it can be optimized. Modelling The variational autoencoder consists of two parts: an encoder, and a decoder. The encoder takes in data and outputs parameters of a probability distribution. These are then used to sample latent vectors which are fed into the decoder to produce new samples. A loss function (ELBO) rewards the model if the outputs are similar to the inputs. The challenge is that the latent space is of much lower dimensionality than the data space, so the model needs to learn to compress the information contained in the data. If you’re interested in more mathematical background on VAEs and the loss function, Lilian Weng has written a  great write - up on autoencoders . The architecture looks like this: Diagram of VAE architecture We define the Variational Autoencoder model as a new type that wraps an encoder and decoder model and define the forward pass and loss function as regular Julia functions.  struct   VAE { E ,   D } \n     encoder :: E \n     decoder :: D \n end \n \n Flux . @ functor   VAE \n \n function   ( vae :: VAE ) ( xs ) \n     μ ,   logσ²   =   vae . encoder ( xs ) \n     zs   =   sample_latent ( μ ,   logσ² ) \n     x̄s   =   vae . decoder ( zs ) \n     return   x̄s ,   ( ;   μ ,   logσ² ) \n end \n \n \n using   Random :   randn! \n using   Statistics :   mean \n \n sample_latent ( μ :: AbstractArray { T } ,   logσ² :: AbstractArray { T } )   where   { T }   = \n     μ   .+   exp . ( logσ²   ./   2 )   .*   randn! ( similar ( logσ² ) ) \n \n function   βELBO ( x ,   x̄ ,   μ ,   logσ² ;   β   =   1 ) \n     reconstruction_error   =   mean ( sum ( @ . ( ( x̄   -   x ) ^ 2 ) ;   dims   =   1 ) ) \n     # D(N(μ, Σ)||N(0, I)) = 1/2 * (μᵀμ + tr(Σ) - length(μ) - log(|Σ|)) \n     kl_divergence   =   mean ( sum ( @ . ( ( μ ^ 2   +   exp ( logσ² )   -   1   -   logσ² )   /   2 ) ;   dims   =   1 ) ) \n \n     return   reconstruction_error   +   β   *   kl_divergence \n end Next we define the encoder and decoder models by composing basic Flux.jl layers.  Dlatent  is the size of the latent space and controls how much the model has to compress the information. Feel free to try out smaller or larger numbers and see how the quality of the generated images changes. SIZE   =   ( 28 ,   28 ,   1 ) \n Din   =   prod ( SIZE ) \n Dhidden   =   512 \n Dlatent   =   4 \n \n encoder   = \n     Chain ( \n         flatten , \n         Dense ( Din ,   Dhidden ,   relu ) ,   # backbone \n         Parallel ( \n             tuple , \n             Dense ( Dhidden ,   Dlatent ) ,   # μ \n             Dense ( Dhidden ,   Dlatent ) ,   # logσ² \n         ) , \n     )   |>   gpu \n \n decoder   =   Chain ( Dense ( Dlatent ,   Dhidden ,   relu ) ,   Dense ( Dhidden ,   Din ,   sigmoid ) ,   xs   ->   reshape ( xs ,   SIZE ... ,   : ) )   |>   gpu \n \n model   =   VAE ( encoder ,   decoder ) ; Custom training loop When dealing with a unconvential learning scheme, we usually need to write a custom training loop. FastAI.jl is build on top of FluxTraining.jl which allows you to write custom training loops with very little boilerplate while retaining compatibility with its extensive callback system. In fact, the built-in training loops for supervised learning are defined in just the same way as we will.  struct   VAETrainingPhase   <:   FluxTraining . AbstractTrainingPhase   end We just defined our own training phase. All that is required to take advantage of the FastAI.jl framework is to define the  FluxTraining.step!  function. We’ll use a utility,  FluxTraining.runstep , to reduce the boilerplate involved in handling callback events and state.  runstep ’s first argument is a function with inputs  (handle, state) .  handle  can be used to dispatch events which callbacks can react to.  state  holds data generated on each call to  step!  like the batch, gradients, and loss. These are also accessible to callbacks, for example to calculate metrics. We also give  runstep  the initial step state which just contains our batch.  struct   VAETrainingPhase   <:   FluxTraining . AbstractTrainingPhase   end \n \n function   FluxTraining . step! ( learner ,   phase :: VAETrainingPhase ,   batch ) \n     FluxTraining . runstep ( learner ,   phase ,   ( xs   =   batch , ) )   do   handle ,   state \n         gs   =   gradient ( learner . params )   do \n             μ ,   logσ²   =   learner . model . encoder ( state . xs ) \n             state . zs   =   sample_latent ( μ ,   logσ² ) \n             state . x̄s   =   learner . model . decoder ( state . zs ) \n \n             handle ( FluxTraining . LossBegin ( ) ) \n             state . loss   =   learner . lossfn ( flatten ( state . xs ) ,   flatten ( state . x̄s ) ,   μ ,   logσ² ) \n \n             handle ( FluxTraining . BackwardBegin ( ) ) \n             return   state . loss \n         end \n         handle ( FluxTraining . BackwardEnd ( ) ) \n         Flux . Optimise . update! ( learner . optimizer ,   learner . params ,   gs ) \n     end \n end Since the step state is a little different from the supervised case (there are no targets  ys ), we also overwrite the default task for the  ToDevice  callback for our training phase: function   FluxTraining . on ( \n     :: FluxTraining . StepBegin , \n     :: VAETrainingPhase , \n     cb :: ToDevice , \n     learner , \n ) \n     learner . step . xs   =   cb . movedatafn ( learner . step . xs ) \n end Training Now we can put the pieces together, creating a  Learner . learner   =   Learner ( model ,   ( ) ,   ADAM ( ) ,   βELBO ,   ToGPU ( ) ) \n FluxTraining . removecallback! ( learner ,   ProgressPrinter ) ; To override the default supervised training loops, we pass our custom training phase and the data iterator we want to run it on to  fitonecycle! : fitonecycle! ( \n     learner , \n     30 , \n     0.01 ; \n     phases   =   ( VAETrainingPhase ( )   =>   dataiter , ) , \n ) Finally, we can visualize how well inputs are reconstructed: xs   =   makebatch ( task ,   data ,   rand ( 1 : nobs ( data ) ,   4 ) )   |>   gpu \n ypreds ,   _   =   model ( xs ) \n showoutputbatch ( task ,   cpu ( xs ) ,   cpu ( ypreds ) )"},{"doctype":"documentation","id":"references/FastAI.test_task_show","title":"test_task_show","text":" test_task_show(task, backend::ShowBackend)\n Test suite that tests that all learning task-related  show*  functions work for  backend Keyword arguments sample = mockblock(getblocks(task)) : Sample data to use for tests. output = mockblock(getblocks(task).ŷ) : Model output data to use for tests."},{"doctype":"documentation","id":"references/Flux.check_use_cuda","title":"check_use_cuda","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.ImageFolders","title":"ImageFolders","text":" ImageFolders(; labelfn = parentname, split = false)\n Recipe for loading a single-label image classification dataset stored in a hierarchical folder format. If  split == true , split the data container on the name of the grandparent folder. The label defaults to the name of the parent folder but a custom function can be passed as  labelfn . julia >   recipeblocks ( ImageFolders ) \n Tuple { Image { 2 } ,   Label }"},{"doctype":"documentation","id":"references/FastAI.Datasets.filterobs","title":"filterobs","text":" filterobs(f, data)\n Return a subset of data container  data  including all indices  i  for which  f(getobs(data, i)) === true . data   =   1 : 10 \n nobs ( data )   ==   10 \n fdata   =   filterobs ( > ( 5 ) ,   data ) \n nobs ( fdata )   ==   5"},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Loggable","title":"Loggable","text":" abstract type Loggable\n Abstract type for data that  LoggerBackend s can log. See  subtypes(FluxTraining.Loggables.Loggable)  and  LoggerBackend"},{"doctype":"documentation","id":"references/DataAugmentation.projectionbounds","title":"projectionbounds","text":" projectionbounds(tfm, P, bounds, randstate)\n"},{"doctype":"documentation","id":"references/FluxTraining.Read","title":"Read","text":""},{"doctype":"documentation","id":"references/Flux.reshape_cell_output","title":"reshape_cell_output","text":""},{"doctype":"documentation","id":"references/FastAI.AbstractBlockTask","title":"AbstractBlockTask","text":" abstract type AbstractBlockTask <: LearningTask\n Abstract supertype for learning tasks that derive their functionality from  Block s and  Encoding s. These learning tasks require you only to specify blocks and encodings by defining which blocks of data show up at which stage of the pipeline. Generally, a subtype will have a field blocks  of type  NamedTuple  that contains this information and a field  encodings  of encodings that are applied to samples. They can be accessed with  getblocks  and  getencodings respectively. For example,  SupervisedTask  represents a learning task where each sample consists of an input and a target. using   FastAI \n task   =   SupervisedTask ( \n     ( Image { 2 } ( ) ,   Label ( [ \" cat \" ,   \" dog \" ] ) ) , \n     ( ImagePreprocessing ( ) ,   OneHot ( ) , ) \n ) \n FastAI . getblocks ( task ) To implement a new  AbstractBlockTask  either use the helper  BlockTask  (simpler) or subtype  AbstractBlockTask  (allows customization through dispatch) Blocks and interfaces To support different learning task interfaces, a  AbstractBlockTask ’s blocks need to contain different blocks. Below we list first block names with descriptions, and afterwards relevant interface functions and which blocks are required to use them. Blocks Each name corresponds to a key of the named tuple blocks = getblocks(task) ). A block is referred to with  blocks.$name and an instance of data from a block is referred to as  $name . blocks.sample : The most important block, representing one full observation of unprocessed data. Data containers used with a learning task should have compatible observations, i.e. checkblock(blocks.sample, getobs(data, i)) . blocks.x : Data that will be fed into the model, i.e. (neglecting batching) model(x)  should work blocks.ŷ : Data that is output by the model, i.e. (neglecting batching) checkblock(blocks.ŷ, model(x)) blocks.y : Data that is compared to the model output using a loss function, i.e.  lossfn(ŷ, y) blocks.encodedsample : An encoded version of  blocks.sample . Will usually correspond to  encodedblockfilled(getencodings(task), blocks.sample) . Interfaces/functionality and required blocks: Core: encode (task, ctx, sample)  requires  sample . Also enables use of taskdataset ,  taskdataloaders decode (task, ctx, encodedsample)  requires  encodedsample decodeypred (task, ctx, ŷ)  requires  ŷ decodey (task, ctx, y)  requires  y Training: taskmodel (task)  requires  x ,  ŷ tasklossfn (task)  requires  y ,  ŷ Visualization: showsample ,  showsamples  require  sample showencodedsample ,  showencodedsamples ,  showbatch require  encodedsample showsample ,  showsamples  require  sample showoutput ,  showoutputs ,  showoutputbatch  require ŷ ,  encodedsample Testing: mockmodel (task)  requires  x ,  ŷ mocksample (task)  requires  sample"},{"doctype":"documentation","id":"references/DataAugmentation._colorview","title":"_colorview","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.Momentum","title":"Momentum","text":" Momentum(η = 0.01, ρ = 0.9)\n Gradient descent optimizer with learning rate  η  and momentum  ρ . Parameters Learning rate ( η ): Amount by which gradients are discounted before updating the weights. Momentum ( ρ ): Controls the acceleration of gradient descent in the prominent direction, in effect damping oscillations. Examples opt   =   Momentum ( ) \n \n opt   =   Momentum ( 0.01 ,   0.99 )"},{"doctype":"documentation","id":"references/Flux.Parallel","title":"Parallel","text":" Parallel(connection, layers...)\nParallel(connection; name = layer, ...)\n Create a layer which passes an input array to each path in layers , before reducing the output with  connection . Called with one input  x , this is equivalent to  connection([l(x) for l in layers]...) . If called with multiple inputs, one is passed to each layer, thus  Parallel(+, f, g)(x, y) = f(x) + g(y) . Like  Chain , its sub-layers may be given names using the keyword constructor. These can be accessed by indexing:  m[1] == m[:name]  is the first layer. See also  SkipConnection  which is  Parallel  with one  identity , and  Maxout  which reduces by broadcasting  max . Examples julia> model = Chain(Dense(3 => 5),\n                     Parallel(vcat, Dense(5 => 4), Chain(Dense(5 => 7), Dense(7 => 4))),\n                     Dense(8 => 17));\n\njulia> model(rand(3)) |> size\n(17,)\n\njulia> model2 = Parallel(+; α = Dense(10, 2, tanh), β = Dense(5, 2))\nParallel(\n  +,\n  α = Dense(10 => 2, tanh),             # 22 parameters\n  β = Dense(5 => 2),                    # 12 parameters\n)                   # Total: 4 arrays, 34 parameters, 392 bytes.\n\njulia> model2(rand(10), rand(5)) |> size\n(2,)\n\njulia> model2[:α](rand(10)) |> size\n(2,)\n\njulia> model2[:β] == model2[2]\ntrue\n"},{"doctype":"documentation","id":"references/FastAI.LREstimator","title":"LREstimator","text":" abstract type LREstimator\n Estimator for an optimal learning rate. Needs to implement  estimatelr . See  Steepest  and  MinDivByTen ."},{"doctype":"documentation","id":"references/FluxTraining.getcallback","title":"getcallback","text":" getcallback(learner, C)\n Find callback of type  C  in  learner ’s callbacks and return it. If there is none, return  nothing ."},{"doctype":"documentation","id":"references/FastAI.Vision.Models.XResNet","title":"XResNet","text":" XResNet(expansion, layers; c_in = 3, ndim = 2)\n Create an XResNet model backbone following the  implementation in fastai . c_in::Int = 3 : The number of input channels, e.g.  1  for grayscale images and  3  for RGB images ndim::Int = 2 : The number of dimensions for the convolutional and pooling layers, e.g. 2  for 2D input images and  3  for 3D volumes."},{"doctype":"documentation","id":"references/FastAI.decodewhile","title":"decodewhile","text":" decodewhile(f, encodings, ctx, block, obs) -> (block', obs')\n Decode  block  by successively applying  encodings  to decode in reverse order until  f(block') == false ."},{"doctype":"documentation","id":"references/FluxTraining._resolveconflict","title":"_resolveconflict","text":""},{"doctype":"documentation","id":"references/Flux.flatten","title":"flatten","text":" flatten(x::AbstractArray)\n Reshape arbitrarly-shaped input into a matrix-shaped output, preserving the size of the last dimension. See also  unsqueeze . Examples julia> rand(3,4,5) |> Flux.flatten |> size\n(12, 5)\n\njulia> xs = rand(Float32, 10,10,3,7);\n\njulia> m = Chain(Conv((3,3), 3 => 4, pad=1), Flux.flatten, Dense(400 => 33));\n\njulia> xs |> m[1] |> size\n(10, 10, 4, 7)\n\njulia> xs |> m |> size\n(33, 7)\n"},{"doctype":"documentation","id":"references/Flux.truncated_normal","title":"truncated_normal","text":" truncated_normal([rng=GLOBAL_RNG], size...; mean = 0, std = 1, lo = -2, hi = 2) -> Array\ntruncated_normal([rng]; kw...) -> Function\n Return an  Array{Float32}  of the given  size  where each element is drawn from a truncated normal distribution. The numbers are distributed like  filter(x -> lo<=x<=hi, mean .+ std .* randn(100)) . The values are generated by sampling a Uniform(0, 1) ( rand() ) and then applying the inverse CDF of the truncated normal distribution. This method works best when  lo ≤ mean ≤ hi . Examples julia> using Statistics\n\njulia> Flux.truncated_normal(3, 4) |> summary\n\"3×4 Matrix{Float32}\"\n\njulia> round.(extrema(Flux.truncated_normal(10^6)); digits=3)\n(-2.0f0, 2.0f0)\n\njulia> round(std(Flux.truncated_normal(10^6; lo = -100, hi = 100)))\n1.0f0\n"},{"doctype":"documentation","id":"references/Flux.conv_transpose_dims","title":"conv_transpose_dims","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.loadfolderdata","title":"loadfolderdata","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.TESTSET_MACROS","title":"TESTSET_MACROS","text":""},{"doctype":"documentation","id":"references/FastAI.MinDivByTen","title":"MinDivByTen","text":" MinDivByTen <: LREstimator\n Estimate the optimal learning rate to be value at the minimum loss divided by 10."},{"doctype":"documentation","id":"references/FastAI.Vision.Models.PixelShuffle","title":"PixelShuffle","text":" PixelShuffle(scale, kernels_in[, kernels_out])\n Pixel shuffle layer that upscales height and width of  x  by  scale . Has reduced checkerboard artifacts compared to  ConvTranspose Introduced in  Real - Time Single Image and Video Super - Resolution Using an EfficientSub - Pixel Convolutional Neural Network ."},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Value","title":"Value","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.ClipNorm","title":"ClipNorm","text":" ClipNorm(thresh)\n Clip gradients when their L2 norm exceeds  thresh ."},{"doctype":"document","id":"documents/docs/tutorials/learningmethod.md","title":"learningmethod","text":""},{"doctype":"documentation","id":"references/FastAI.Tabular.TableRegressionRecipe","title":"TableRegressionRecipe","text":""},{"doctype":"documentation","id":"references/Flux.f64","title":"f64","text":" f64(m)\n Converts the  eltype  of model’s parameters to  Float64 . Recurses into structs marked with  @functor ."},{"doctype":"document","id":"documents/notebooks/how_to_visualize.ipynb","title":"How to visualize data","text":" How to visualize data Visualizing the data we’re working with is indispensible both to check that data pipelines are set up correctly and to check the predictions of a trained model. For visualization, the  Makie . jl  plotting package is used which requires you to  install a plotting backend . Learning tasks define how the data is visualized, allowing you to use the following functions for visualization: showsample  ( showsamples ): Visualize an unprocessed sample (usually a pair of inputs and targets) or a vector of samples. showencodedsample  ( showbatch ): Visualize processed model input and output  x, y  or a batch of  xs  and  ys . showprediction   ( showpredictions ): Compare a model output with the ground truth. To add support for these to a learning task, you have to implement the plotting interface for a block:  showblock! . Let’s look at an example using the Cat/Dog classifier from  the saving and loading tutorial . import   CairoMakie ;   CairoMakie . activate! ( type = \" png \" ) \n using   FastAI \n \n task ,   model   =   loadtaskmodel ( \" catsdogs.jld2 \" ) \n dir   =   joinpath ( datasetpath ( \" dogscats \" ) ,   \" train \" ) \n data   =   loadfolderdata ( dir ,   filterfn = isimagefile ,   loadfn = ( loadfile ,   parentname ) ) First we load a vector of unprocessed samples, a batch of training data and the corresponding model outputs: idxs   =   rand ( 1 : nobs ( data ) ,   9 ) \n samples   =   [ getobs ( data ,   i )   for   i   in   idxs ] \n xs ,   ys   =   makebatch ( task ,   data ,   idxs ) \n ŷs   =   gpu ( model ) ( gpu ( xs ) )   |>   cpu Then we can visualize the data with the functions listed above: showsamples ( task ,   samples ) showbatch ( task ,   xs ,   ys ) showpredictions ( task ,   xs ,   ŷs ,   ys )"},{"doctype":"documentation","id":"references/FluxTraining.CheckModelLossStep","title":"CheckModelLossStep","text":""},{"doctype":"documentation","id":"references/FastAI.createhandle","title":"createhandle","text":" createhandle(backend::ShowBackend)\n Creates a context to which blocks of data can be shown using the mutating functions  showblock!  and  showblocks! . It is called internally when using  showblock  or  showblocks . handle   =   createhandle ( backend ) \n showblock! ( handle ,   backend ,   block ,   obs ) \n \n # Above is equivalent to \n showblock ( backend ,   block ,   obs )"},{"doctype":"documentation","id":"references/Flux._onehot_bool_type","title":"_onehot_bool_type","text":""},{"doctype":"documentation","id":"references/Flux._maybetuple_string","title":"_maybetuple_string","text":""},{"doctype":"documentation","id":"references/FastAI.taskmodel","title":"taskmodel","text":" taskmodel(task, backbone)\n Construct a model for  task  from a backbone architecture, for example by attaching a task-specific head model."},{"doctype":"documentation","id":"references/FastAI.Vision.Models.upsample_block_small","title":"upsample_block_small","text":" upsample_block_small(insize, k_out)\n An upsampling block that increases the spatial dimensions of the input by 2 using pixel-shuffle upsampling."},{"doctype":"documentation","id":"references/FastAI.encodeinput!","title":"encodeinput!","text":""},{"doctype":"documentation","id":"references/Flux.applychain","title":"applychain","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.registerdataset!","title":"registerdataset!","text":" registerdataset!([registry,] name, loadfn)\n Register a dataset in  registry::DatasetRegistry  with a  name  and a function  loadfn()  that downloads it and returns a path."},{"doctype":"documentation","id":"references/DataAugmentation.Image","title":"Image","text":" Image(image[, bounds])\n Item representing an N-dimensional image with element type T. Examples using   DataAugmentation ,   Images \n \n imagedata   =   rand ( RGB ,   100 ,   100 ) \n item   =   Image ( imagedata ) \n showitems ( item ) If  T  is not a color, the image will be interpreted as grayscale: imagedata   =   rand ( Float32 ,   100 ,   100 ) \n item   =   Image ( imagedata ) \n showitems ( item )"},{"doctype":"documentation","id":"references/FastAI.Tabular","title":"Tabular","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.DatasetRegistry","title":"DatasetRegistry","text":" DatasetRegistry()\n A store for information on datasets and dataset recipes for loading those datasets."},{"doctype":"documentation","id":"references/FluxTraining.print_epoch_table","title":"print_epoch_table","text":""},{"doctype":"documentation","id":"references/FastAI.Tabular.linbndrop","title":"linbndrop","text":""},{"doctype":"documentation","id":"references/DataAugmentation.denormalize!","title":"denormalize!","text":""},{"doctype":"documentation","id":"references/FastAI.Label","title":"Label","text":" Label(classes) <: Block\nsetup(LabelMulti, data)\n Block  for a categorical label in a single-class context. data  is valid for  Label(classes)  if  data ∈ classes . See  LabelMulti  for the multi-class setting where an observation can belong to multiple classes. Examples block   =   Label ( [ \" cat \" ,   \" dog \" ] )    # an observation can be either \"cat\" or \"dog\" \n @ test   FastAI . checkblock ( block ,   \" cat \" ) \n @ test   ! ( FastAI . checkblock ( block ,   \" horsey \" ) ) You can use  setup  to create a  Label  instance from a data container containing possible classes: targets   =   [ \" cat \" ,   \" dog \" ,   \" dog \" ,   \" dog \" ,   \" cat \" ,   \" dog \" ] \n block   =   setup ( Label ,   targets ) \n @ test   block   ≈   Label ( [ \" cat \" ,   \" dog \" ] )"},{"doctype":"documentation","id":"references/FluxTraining.Protected","title":"Protected","text":""},{"doctype":"documentation","id":"references/FastAI.encodedblock","title":"encodedblock","text":" encodedblock(encoding, block)\nencodedblock(encoding, blocks)\nencodedblock(encodings, blocks)\n Return the block that is obtained by encoding  block  with encoding  E . This needs to be constant for an instance of  E , so it cannot depend on the sample or on randomness. The default is to return  nothing , meaning the same block is returned and not changed. Encodings that return the same block but change the data (e.g.  ProjectiveTransforms ) should return  block ."},{"doctype":"documentation","id":"references/FluxTraining.paramsrec","title":"paramsrec","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.OADAM","title":"OADAM","text":" OADAM(η = 0.0001, β::Tuple = (0.5, 0.9), ϵ = \n"},{"doctype":"documentation","id":"references/FastAI.Named","title":"Named","text":" Named(name, block)\n Wrapper  Block  to attach a name to a block. Can be used in conjunction with  Only  to apply encodings to specific blocks only."},{"doctype":"documentation","id":"references/FluxTraining.edgesrunafter","title":"edgesrunafter","text":" edgesrunafter(callbacks)\n Return a vector of  Edge s representing dependencies defined by  runafter ."},{"doctype":"documentation","id":"references/FastAI.ShowMakie","title":"ShowMakie","text":" ShowMakie([; kwargs...]) <: ShowBackend\n A backend for showing block data that uses Makie . jl  figures for visualization. Keyword arguments are passed through to the constructed  Figure s."},{"doctype":"documentation","id":"references/FastAI.taskdataloaders","title":"taskdataloaders","text":" taskdataloaders(data, task[, batchsize])\ntaskdataloaders(traindata, validdata, task[, batchsize; shuffle = true, dlkwargs...])\n Create training and validation  DataLoader s from two data containers (traindata, valdata) . If only one container  data  is passed, splits it into two, with  pctgvalid % of the data going into the validation split. Arguments Positional: batchsize = 16 Keyword: shuffle = true : Whether to shuffle the training data container validbsfactor = 2 : Factor to multiply batchsize for validation data loader with (validation batches can be larger since no GPU memory is needed for the backward pass) All remaining keyword arguments are passed to  DataLoader . Examples Basic usage: traindl ,   validdl   =   taskdataloaders ( data ,   task ,   128 ) Explicit validation data container and no shuffling of training container: traindl ,   validdl   =   taskdataloaders ( traindata ,   validdata ,   task ,   shuffle = false ) Customizing the  DataLoader traindl ,   validdl   =   taskdataloaders ( data ,   task ,   parallel = false ,   buffered = false )"},{"doctype":"documentation","id":"references/FluxTraining.resolveconflict","title":"resolveconflict","text":" resolveconflict(cb1, cb2)\n Define a conflict resolution strategy for resolving a write/write conflict between two callbacks. The default is [ NotDefined() ], which will result in an error and a message to implement this method. To implement, dispatch on the callback types that you which to resolve (in any order) and return one of the following: Unresolvable ()  if the callbacks must not be used together RunFirst (cb)  if one of the callbacks needs to run first; or NoConflict ()  if the callbacks may run together in any order"},{"doctype":"documentation","id":"references/FluxTraining.stepvalue","title":"stepvalue","text":""},{"doctype":"documentation","id":"references/Flux.outputsize","title":"outputsize","text":" outputsize(m, inputsize::Tuple; padbatch=false)\n Calculate the size of the output from model  m , given the size of the input. Obeys  outputsize(m, size(x)) == size(m(x))  for valid input  x . Keyword  padbatch=true  is equivalent to using  (inputsize..., 1) , and returns the final size including this extra batch dimension. This should be faster than calling  size(m(x)) . It uses a trivial number type, which should work out of the box for custom layers. If  m  is a  Tuple  or  Vector , its elements are applied in sequence, like  Chain(m...) . Examples julia> using Flux: outputsize\n\njulia> outputsize(Dense(10, 4), (10,); padbatch=true)\n(4, 1)\n\njulia> m = Chain(Conv((3, 3), 3 => 16), Conv((3, 3), 16 => 32));\n\njulia> m(randn(Float32, 10, 10, 3, 64)) |> size\n(6, 6, 32, 64)\n\njulia> outputsize(m, (10, 10, 3); padbatch=true)\n(6, 6, 32, 1)\n\njulia> outputsize(m, (10, 10, 3, 64))\n(6, 6, 32, 64)\n\njulia> try outputsize(m, (10, 10, 7, 64)) catch e println(e) end\n┌ Error: layer Conv((3, 3), 3=>16), index 1 in Chain, gave an error with input of size (10, 10, 7, 64)\n└ @ Flux ~/.julia/dev/Flux/src/outputsize.jl:114\nDimensionMismatch(\"Input channels must match! (7 vs. 3)\")\n\njulia> outputsize([Dense(10, 4), Dense(4, 2)], (10, 1)) # Vector of layers becomes a Chain\n(2, 1)\n outputsize(m, x_size, y_size, ...; padbatch=false)\n For model or layer  m  accepting multiple arrays as input, this returns  size(m((x, y, ...)))  given  size_x = size(x) , etc. Examples julia> x, y = rand(Float32, 5, 64), rand(Float32, 7, 64);\n\njulia> par = Parallel(vcat, Dense(5, 9), Dense(7, 11));\n\njulia> Flux.outputsize(par, (5, 64), (7, 64))\n(20, 64)\n\njulia> m = Chain(par, Dense(20, 13), softmax);\n\njulia> Flux.outputsize(m, (5,), (7,); padbatch=true)\n(13, 1)\n\njulia> par(x, y) == par((x, y)) == Chain(par, identity)((x, y))\ntrue\n Notice that  Chain  only accepts multiple arrays as a tuple, while  Parallel  also accepts them as multiple arguments; outputsize  always supplies the tuple."},{"doctype":"documentation","id":"references/FastAI.Vision.ImageTableMultiLabel","title":"ImageTableMultiLabel","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Reflect","title":"Reflect","text":" Reflect(γ)\nReflect(distribution)\n Reflect 2D spatial data around the center by an angle chosen at uniformly from [-γ, γ], an angle given in degrees. You can also pass any  Distributions.Sampleable  from which the angle is selected. Examples tfm   =   Reflect ( 10 )"},{"doctype":"documentation","id":"references/FastAI.Tabular._registerrecipes","title":"_registerrecipes","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.tests","title":"tests","text":""},{"doctype":"documentation","id":"references/Flux._nan_show","title":"_nan_show","text":""},{"doctype":"documentation","id":"references/FluxTraining.ProtectedException","title":"ProtectedException","text":""},{"doctype":"documentation","id":"references/FastAI.Tabular.TabularModel","title":"TabularModel","text":" TabularModel(catbackbone, contbackbone, [finalclassifier]; kwargs...)\n Create a tabular model which operates on a tuple of categorical values (label or one-hot encoded) and continuous values. The categorical backbones ( catbackbone ) and continuous backbone ( contbackbone ) operate on each element of the input tuple. The output from these backbones is then passed through a series of linear-batch norm-dropout layers before a  finalclassifier  block. Keyword arguments outsize : The output size of the final classifier block. For single classification tasks, this would be the number of classes, and for regression tasks, this would be the number of target continuous variables. layersizes : A vector of sizes for each hidden layer in the sequence of linear layers. dropout_rates : Dropout probabilities for the linear-batch norm-dropout layers. This could either be a single number which would be used for for all the layers, or a collection of numbers which are cycled through for each layer. batchnorm : Set to  false  to skip each batch norm in the linear-batch norm-dropout sequence. activation : The activation function to use in the classifier layers. linear_first : Controls if the linear layer comes before or after batch norm and dropout. TabularModel(n_cont, outsize, [layersizes; kwargs...])\n Create a tabular model which operates on a tuple of categorical values (label or one-hot encoded) and continuous values. The default categorical backbone ( catbackbone ) is a  Flux.Parallel  set of  Flux.Embedding  layers corresponding to each categorical variable. The default continuous backbone ( contbackbone ) is a single  Flux.BatchNorm . The output from these backbones is concatenated then passed through a series of linear-batch norm-dropout layers before a  finalclassifier  block. Arguments n_cont : The number of continuous columns. outsize : The output size of the model. layersizes : A vector of sizes for each hidden layer in the sequence of linear layers. Keyword arguments cardinalities : A collection of sizes (number of classes) for each categorical column. size_overrides : An optional argument which corresponds to a collection containing embedding sizes to override the value returned by the “rule of thumb” for a particular index corresponding to  cardinalities , or  nothing ."},{"doctype":"documentation","id":"references/FluxTraining.Events.StepEnd","title":"StepEnd","text":" StepEnd()\n Event  called at the end of a batch."},{"doctype":"documentation","id":"references/FastAI.mockblock","title":"mockblock","text":" mockblock(block)\nmockblock(blocks)\n Randomly generate an instance of  block . It always holds that checkblock(block, mockblock(block)) === true ."},{"doctype":"documentation","id":"references/DataAugmentation.boundsof","title":"boundsof","text":" boundingranges(ps)\n Find bounding index ranges for points  ps ."},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Audio","title":"Audio","text":""},{"doctype":"documentation","id":"references/DataAugmentation.__round","title":"__round","text":""},{"doctype":"document","id":"documents/notebooks/10_26_showblock.ipynb","title":"New visualization tools for FastAI.jl","text":" import   CairoMakie \n using   FastAI FastAI . default_showbackend ( )   =   ShowText ( ) New visualization tools for FastAI . jl based on blocks support for different backends (currently text and Makie.jl) high-level functions for use with learning tasks Learning tasks data ,   blocks   =   loaddataset ( \" imagenette2-320 \" ,   ( Image ,   Label ) ) \n task   =   ImageClassificationSingle ( blocks ) showsample  shows one sample from the data container: sample   =   getobs ( data ,   1 ) \n showsample ( task ,   sample ) showencodedsample  shows a sample after encodings have been applied to it: x ,   y   =   encodesample ( task ,   Validation ( ) ,   sample ) \n showencodedsample ( task ,   ( x ,   y ) ) Any encoded block that can’t be visualized (here  ImageTensor ) is decoded until it can be using the task’s encodings. using   Markdown ;   FastAI . describeencodings ( task . encodings ,   task . blocks )   |>   Markdown . parse It is also possible to show a complete (collated) batch using  showbatch : xs ,   ys   =   makebatch ( task ,   data ,   1 : 3 ) \n showbatch ( task ,   ( xs ,   ys ) ) If you used a trained model to create outputs, you can compare these to the true data with  showoutputbatch : outputs   =   reduce ( hcat ,   [ rand ( 10 )   for   _   in   1 : 3 ] ) \n showoutputbatch ( task ,   ( xs ,   ys ) ,   outputs ) These works for any learning task that deal with blocks that have a visualization defined. The following definition is all that is needed to add support for the  Image  block type to the text backend: function   showblock! ( io ,   :: ShowText ,   block :: Image { 2 } ,   data ) \n \n     ImageInTerminal . imshow ( io ,   data ) \n \n end data ,   blocks   =   loaddataset ( \" camvid \" ,   ( Image ,   Mask ) ) \n task   =   ImageSegmentation ( blocks ) sample   =   getobs ( data ,   1 ) \n showsample ( task ,   sample ) showblock data ,   blocks   =   loaddataset ( \" pascal_2007 \" ,   ( Image ,   LabelMulti ) ) \n sample   =   getobs ( data ,   6 ) \n showblock ( blocks ,   sample ) showblock  and  showblocks Using  showblock , arbitrary visualizations can be created: blocks   =   ( \" Image \"   =>   Image { 2 } ( ) ,   \" Mask \"   =>   Mask { 2 } ( 1 : 3 ) ,   \" Class label \"   =>   Label ( 1 : 10 ) ) \n obs   =   FastAI . mockblock ( last . ( blocks ) ) \n showblock ( blocks ,   obs ) showblocks  visualizes multiple observations: showblocks ( blocks ,   [ FastAI . mockblock ( last . ( blocks ) )   for   _   in   1 : 2 ] ) Makie Makie.jl is no longer a required dependency. Instead, Makie.jl visualizations are only loaded if you have it imported (using Requires.jl). This reduces the load time of FastAI.jl significantly. FastAI . default_showbackend ( )   =   ShowMakie ( ) data ,   blocks   =   loaddataset ( \" imagenette2-320 \" ,   ( Image ,   Label ) ) \n task   =   ImageClassificationSingle ( blocks ) showsample  shows one sample from the data container: sample   =   getobs ( data ,   1 ) \n showsample ( task ,   sample ) showencodedsample  shows a sample after encodings have been applied to it: x ,   y   =   encodesample ( task ,   Validation ( ) ,   sample ) \n showencodedsample ( task ,   ( x ,   y ) ) Any encoded block that can’t be visualized (here  ImageTensor ) is decoded until it can be using the task’s encodings. task . encodings It is also possible to show a complete (collated) batch using  showbatch : xs ,   ys   =   makebatch ( task ,   data ,   1 : 3 ) \n showbatch ( task ,   ( xs ,   ys ) ) If you used a trained model to create outputs, you can compare these to the true data with  showoutputbatch : outputs   =   reduce ( hcat ,   [ rand ( 10 )   for   _   in   1 : 3 ] ) \n showoutputbatch ( task ,   ( xs ,   ys ) ,   outputs ) These works for any learning task that deal with blocks that have a visualization defined. The following definition is all that is needed to add support for the  Image  block type to the text backend: function   showblock! ( io ,   :: ShowText ,   block :: Image { 2 } ,   data ) \n \n     ImageInTerminal . imshow ( io ,   data ) \n \n end data ,   blocks   =   loaddataset ( \" camvid \" ,   ( Image ,   Mask ) ) \n task   =   ImageSegmentation ( blocks ) sample   =   getobs ( data ,   1 ) \n showsample ( task ,   sample ) showblock data ,   blocks   =   loaddataset ( \" imagenette2-320 \" ,   ( Image ,   Label ) ) \n sample   =   getobs ( data ,   1 ) \n showblock ( blocks ,   sample ) data ,   blocks   =   loaddataset ( \" pascal_2007 \" ,   ( Image ,   LabelMulti ) ) \n sample   =   getobs ( data ,   6 ) \n showblock ( blocks ,   sample ) showblock  and  showblocks Using  showblock , arbitrary visualizations can be created: blocks   =   ( \" Image \"   =>   Image { 2 } ( ) ,   \" Mask \"   =>   Mask { 2 } ( 1 : 3 ) ,   \" Class label \"   =>   Label ( 1 : 10 ) ) \n obs   =   FastAI . mockblock ( last . ( blocks ) ) \n showblock ( blocks ,   obs ) showblocks  visualizes multiple observations: showblocks ( blocks ,   [ FastAI . mockblock ( last . ( blocks ) )   for   _   in   1 : 2 ] )"},{"doctype":"documentation","id":"references/FastAI.Vision.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.tests","title":"tests","text":""},{"doctype":"documentation","id":"references/Flux.onecold","title":"onecold","text":" onecold(y::AbstractArray, labels = 1:size(y,1))\n Roughly the inverse operation of  onehot  or  onehotbatch : This finds the index of the largest element of  y , or each column of  y , and looks them up in  labels . If  labels  are not specified, the default is integers  1:size(y,1)  – the same operation as  argmax(y, dims=1)  but sometimes a different return type. Examples julia> Flux.onecold([false, true, false])\n2\n\njulia> Flux.onecold([0.3, 0.2, 0.5], (:a, :b, :c))\n:c\n\njulia> Flux.onecold([ 1  0  0  1  0  1  0  1  0  0  1\n                      0  1  0  0  0  0  0  0  1  0  0\n                      0  0  0  0  1  0  0  0  0  0  0\n                      0  0  0  0  0  0  1  0  0  0  0\n                      0  0  1  0  0  0  0  0  0  1  0 ], 'a':'e') |> String\n\"abeacadabea\"\n"},{"doctype":"documentation","id":"references/FastAI.ShowText","title":"ShowText","text":" ShowText([io; kwargs...]) <: ShowBackend\n A backend for showing block data using text for REPL use. Text is displayed to  io  and  kwargs  are keyword arguments for  PrettyTables.pretty_table , which is used to display collections of blocks."},{"doctype":"documentation","id":"references/DataAugmentation.itemdata","title":"itemdata","text":" itemdata(item)\nitemdata(items)\n Access the data wrapped in  item  or a tuple of items."},{"doctype":"documentation","id":"references/FastAI.PropagateSameBlock","title":"PropagateSameBlock","text":""},{"doctype":"documentation","id":"references/Flux._fast_argmax","title":"_fast_argmax","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.ADAMW","title":"ADAMW","text":" ADAMW(η = 0.001, β::Tuple = (0.9, 0.999), decay = 0)\n ADAMW  is a variant of ADAM fixing (as in repairing) its weight decay regularization. Parameters Learning rate ( η ): Amount by which gradients are discounted before updating the weights. Decay of momentums ( β::Tuple ): Exponential decay for the first (β1) and the second (β2) momentum estimate. decay : Decay applied to weights during optimisation. Examples opt   =   ADAMW ( ) \n \n opt   =   ADAMW ( 0.001 ,   ( 0.89 ,   0.995 ) ,   0.1 )"},{"doctype":"documentation","id":"references/FluxTraining.accuracy","title":"accuracy","text":""},{"doctype":"documentation","id":"references/DataAugmentation.ResizePadDivisible","title":"ResizePadDivisible","text":""},{"doctype":"documentation","id":"references/FastAI.blocktypesmatch","title":"blocktypesmatch","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.Models.unetlayers","title":"unetlayers","text":""},{"doctype":"documentation","id":"references/FastAI.showblock!","title":"showblock!","text":" showblock!(handle, backend, block, obs)\nshowblock!(handle, backend, blocks, obss)\nshowblock!(handle, backend, title => block, obs)\n Show block of data to an existing context  handle  using  backend . See  showblock  for examples. Extending Every  ShowBackend  should implement the following versions of this method: showblock!(handle, backend, block::Block, obs)  to show a single block of obs; should be implemented for every block type you want to show showblock!(handle, backend, blocks::Tuple, obss::Tuple)  to show several blocks that belong to the same observation. Optionally, you can also implement showblock!(handle, backend, pair::Pair, obs)  where  (title, block) = pair  gives the name for a block. If this is not implemented for a backend, then calling it will default to the untitled method."},{"doctype":"documentation","id":"references/FastAI.Tabular.TableRow","title":"TableRow","text":" TableRow{M, N}(catcols, contcols, categorydict) <: Block\n Block  for table rows with M categorical and N continuous columns.  data is valid if it satisfies the  AbstractRow  interface in Tables.jl, values present in indices for categorical and continuous columns are consistent, and  data  is indexable by the elements of  catcols  and  contcols ."},{"doctype":"documentation","id":"references/FluxTraining.FitException","title":"FitException","text":" abstract type FitException\n Abstract types for exceptions that can be thrown during fitting, to change its control flow. See  CancelStepException ,  CancelEpochException ,  CancelFittingException ."},{"doctype":"document","id":"documents/notebooks/keypointregression.ipynb","title":"Keypoint regression","text":" Keypoint regression Single keypoint regression consists of localizing a keypoint in an image. Here we’ll be training on a head pose dataset, where every image has a person in it and the head of the person is annotated. Since keypoint datasets all have different formats, we have to do a bit more manual work to get the task dataset loaded. First we import everything we’ll need: import   CairoMakie ;   CairoMakie . activate! ( type = \" png \" ) \n using   DelimitedFiles :   readdlm \n using   FastAI \n using   FastAI . FilePathsBase ,   FastAI . StaticArrays ,   FastAI . DLPipelines \n import   FastAI . DataAugmentation Creating a task data container datasetpath  downloads the files, but it’s up to us to load them into a usable format. In the end, the task data container should contain tuples of an image and a keypoint each. path   =   datasetpath ( \" biwi_head_pose \" ) files   =   FileDataset ( path ) ; \n files [ 1 : 10 ] First we create a  FileDataset  from the directory where the dataset has been downloaded to: Loading a  FileDataset  simply treats every file as a single observation. However, that is not what we want here: for every observation we have one image and one annotation file that make up one observation and we want to ignore all other files, like the README. To achieve this, we’ll create two data containers containing all the image paths and annotation paths respectively by filtering the container with all paths. imagefiles   =   loadfolderdata ( path ,   filterfn = isimagefile ) \n annotfiles   =   loadfolderdata ( path ,   filterfn = p   ->   occursin ( \" pose \" ,   pathname ( p ) ) ) ( getobs ( imagefiles ,   1 ) ,   getobs ( annotfiles ,   1 ) ) Next we need to map functions over each observation that load the data from the files. An image file can be loaded using the  loadfile  utility. The keypoints have a custom format, so we write a helper function to parse them from a text file. The details of how the format is loaded aren’t important. readcalibrationfile ( p )   =   readdlm ( string ( p ) ) [ 1 : 3 ,   1 : 3 ] \n \n CAL   =   readcalibrationfile ( joinpath ( path ,   \" 01 \" ,   \" rgb.cal \" ) ) \n \n function   loadannotfile ( annotpath ,   cal   =   CAL ) \n     ctr   =   readdlm ( string ( annotpath ) ) [ 4 , : ] \n     cx   =   ctr [ 1 ]   *   cal [ 1 , 1 ] / ctr [ 3 ]   +   cal [ 1 , 3 ] \n     cy   =   ctr [ 2 ]   *   cal [ 2 , 2 ] / ctr [ 3 ]   +   cal [ 2 , 3 ] \n     return   [ SVector ( cy ,   cx )   .+   1 ] \n end Now we can use  mapobs  to lazily map the loading function over the container. Note that beside loading the image and keypoint, we also extract the subject ID from the path. We’ll use this in a bit for splitting the dataset appropriately and we don’t have access to the path information anymore once we have a container of loaded data. data   =   ( \n     mapobs ( loadfile ,   imagefiles ) , \n     mapobs ( loadannotfile ,   annotfiles ) \n ) \n ids   =   map ( p   ->   parse ( Int ,   pathname ( pathparent ( p ) ) ) ,   imagefiles ) \n obs   =   image ,   ks   =   getobs ( data ,   2000 ) We can visualize an observation using  DataAugmentation.showitems  if we wrap the data in item types: DataAugmentation . showitems ( ( \n     DataAugmentation . Image ( image ) , \n     DataAugmentation . Keypoints ( ks ,   size ( image ) ) ) \n ) Before we can start using this data container for training, we need to split into a training and validation dataset. Since there are 13 different persons with many images each, randomly splitting the container does not make sense. The validation dataset would then contain many images that are very similar to those seen in training, and would hence say little about the generalization ability of a model. We instead use the first 12 subjects as a training dataset and validate on the last. traindata   =   datasubset ( data ,   ( 1 : nobs ( data ) ) [ ids   .!=   13 ] ) \n validdata   =   datasubset ( data ,   ( 1 : nobs ( data ) ) [ ids   .==   13 ] ) \n nobs ( traindata ) ,   nobs ( validdata ) The learning task Next we need to define a  learning task  that encodes and augments each image and keypoint in a form that we can train a model on. We need to create a  LearningTask  struct for which we can define these transformations. Here we make use of  ProjectiveTransforms  for resizing, cropping and augmenting the image and keypoint and  ImagePreprocessing  to reshape and normalize the image. Finally,  KeypointPreprocessing  makes sure keypoints fall between -1 and 1. sz   =   ( 128 ,   128 ) \n task   =   BlockTask ( \n     ( Image { 2 } ( ) ,   Keypoints { 2 } ( 1 ) ) , \n     ( \n         ProjectiveTransforms ( sz ,   buffered = true ,   augmentations = augs_projection ( max_warp = 0 ) ) , \n         ImagePreprocessing ( ) , \n         KeypointPreprocessing ( sz ) , \n     ) \n ) We can check that each image is resized to  (224, 224)  and the keypoints are normalized: im ,   k   =   getobs ( traindata ,   1 ) \n x ,   y   =   encodesample ( task ,   Training ( ) ,   ( im ,   k ) ) \n summary ( x ) ,   y Decoding the encoded targets should give back a point within the original image bounds: DLPipelines . decodeŷ ( task ,   Training ( ) ,   y ) We should also visualize our data to make sure that after all the encoding it still makes sense and the keypoint is properly aligned with the head on the image. The visualizations are derived from the data blocks we used when defining our  BlockTask . xs ,   ys   =   FastAI . makebatch ( task ,   traindata ,   1 : 2 ) \n showbatch ( task ,   ( xs ,   ys ) ) That is looking good! We can see that the keypoint is aligned with center of the head even after heavy augmentation. Now it is finally time to train a model. Training We’ll use a modified ResNet as a model backbone. and add a couple layers that regress the keypoint.  taskmodel  knows how to do this by looking at the data blocks used and calling  blockmodel (KepointTensor{2, Float32}((1,)), KeypointTensor{2, Float32}((1,)), backbone) . The implementation, for reference, looks like this: function   blockmodel ( inblock :: ImageTensor { N } ,   outblock :: KeypointTensor { N } ,   backbone )   where   N \n \n     outsz   =   Flux . outputsize ( backbone ,   ( ntuple ( _   ->   256 ,   N ) ... ,   inblock . nchannels ,   1 ) ) \n \n     outch   =   outsz [ end - 1 ] \n \n     head   =   Models . visionhead ( outch ,   prod ( outblock . sz ) * N ,   p   =   0. ) \n \n     return   Chain ( backbone ,   head ) \n \n end backbone   =   Models . xresnet18 ( ) \n model   =   taskmodel ( task ,   backbone ) ; Next we create a pair of training and validation data loaders. They take care of batching and loading the data in parallel in the background. traindl ,   validdl   =   FastAI . taskdataloaders ( traindata ,   validdata ,   task ,   16 ) With the addition of an optimizer and a loss function, we can now create a  Learner  and start training. Just like  taskmodel ,  tasklossfn  selects the appropriate loss function for a  BlockTask s blocks. Here both the encoded target block and model output block are  block = KeypointTensor{2, Float32}((1,)) , so  blocklossfn(block, block)  is called which returns Mean Squared Error as a suitable loss function. learner   =   Learner ( \n     model , \n     ( traindl ,   validdl ) , \n     Flux . ADAM ( ) , \n     tasklossfn ( task ) , \n     ToGPU ( ) ) fitonecycle! ( learner ,   5 ) The loss is going down which is a good sign, but visualizing the predictions against the ground truth will give us a better idea of how well the model performs. We’ll use  showoutputs  to compare batches of encoded targets and model outputs. For this we can run the model on a batch from the validation dataset and see how it performs. showoutputs ( task ,   learner ;   n   =   3 ,   context   =   Validation ( ) ) We can also see that the trained model generalizes well to the heavy augmentation employed during training. The augmentation also explains why the training loss is so much higher than the validation loss. showoutputs ( task ,   learner ;   n   =   3 ,   context   =   Training ( ) )"},{"doctype":"documentation","id":"references/FastAI.Steepest","title":"Steepest","text":" Steepest <: LREstimator\n Estimate the optimal learning rate to be where the gradient of the loss is the steepest, i.e. the decrease is largest."},{"doctype":"documentation","id":"references/FastAI.setup","title":"setup","text":" setup(Block, data)\n Create an instance of block type  Block  from data container  data . Examples setup ( Label ,   [ \" cat \" ,   \" dog \" ,   \" cat \" ] )   ==   Label ( [ \" cat \" ,   \" dog \" ] ) setup(Encoding, block, data; kwargs...)\n Create an encoding using statistics derived from a data container  data with observations of block  block . Used when some arguments of the encoding are dependent on the dataset.  data  should be the training dataset. Additional kwargs  are passed through to the regular constructor of  Encoding . Examples ( images ,   labels ) ,   blocks   =   loaddataset ( \" imagenette2-160 \" ,   ( Image ,   Label ) ) \n setup ( ImagePreprocessing ,   Image { 2 } ( ) ,   images ;   buffered   =   false ) data ,   block   =   loaddataset ( \" adult_sample \" ,   TableRow ) \n setup ( TabularPreprocessing ,   block ,   data )"},{"doctype":"documentation","id":"references/Flux.Losses.squared_hinge_loss","title":"squared_hinge_loss","text":" squared_hinge_loss(ŷ, y)\n Return the squared hinge_loss loss given the prediction  ŷ  and true labels  y (containing 1 or -1); calculated as  sum((max.(0, 1 .- ŷ .* y)).^2) / size(y, 2) . See also:  hinge_loss"},{"doctype":"documentation","id":"references/Flux.Dense","title":"Dense","text":" Dense(in => out, σ=identity; bias=true, init=glorot_uniform)\nDense(W::AbstractMatrix, [bias, σ])\n Create a traditional fully connected layer, whose forward pass is given by: y = σ.(W * x .+ bias)\n The input  x  should be a vector of length  in , or batch of vectors represented as an  in × N  matrix, or any array with  size(x,1) == in . The out  y  will be a vector  of length  out , or a batch with size(y) == (out, size(x)[2:end]...) Keyword  bias=false  will switch off trainable bias for the layer. The initialisation of the weight matrix is  W = init(out, in) , calling the function given to keyword  init , with default [ glorot_uniform ](  Flux.glorot_uniform). The weight matrix and/or the bias vector (of length  out ) may also be provided explicitly. Examples julia> d = Dense(5 => 2)\nDense(5 => 2)       # 12 parameters\n\njulia> d(rand(Float32, 5, 64)) |> size\n(2, 64)\n\njulia> d(rand(Float32, 5, 1, 1, 64)) |> size  # treated as three batch dimensions\n(2, 1, 1, 64)\n\njulia> d1 = Dense(ones(2, 5), false, tanh)  # using provided weight matrix\nDense(5 => 2, tanh; bias=false)  # 10 parameters\n\njulia> d1(ones(5))\n2-element Vector{Float64}:\n 0.9999092042625951\n 0.9999092042625951\n\njulia> Flux.params(d1)  # no trainable bias\nParams([[1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0]])\n"},{"doctype":"documentation","id":"references/Flux._dropout_kernel","title":"_dropout_kernel","text":""},{"doctype":"documentation","id":"references/FastAI.methodlossfn","title":"methodlossfn","text":""},{"doctype":"documentation","id":"references/DataAugmentation.CropFrom","title":"CropFrom","text":""},{"doctype":"documentation","id":"references/DataAugmentation.testapply!","title":"testapply!","text":" testapply!(tfm, Items)\ntestapply!(tfm, Item)\ntestapply!(tfm, item1, item2)\n Test  apply!  invariants. With a constant  randstate  parameter,  apply!  should always return the same result. Given a different item than was used to create the buffer, the buffer’s data should be modified."},{"doctype":"document","id":"documents/docs/api.md","title":"FastAI.jl interfaces","text":" FastAI . jl interfaces Training High - level Quickly get started training and finetuning models using already implemented learning tasks and callbacks. tasklearner fit! fitonecycle! finetune! BlockTask callbacks Mid - level Learner taskmodel tasklossfn Low - level LearningTask encode encodeinput decodey Datasets High - level Quickly download and load task data containers from the fastai dataset library. `load FastAI.Datasets.DATASETS Mid - level Load and transform data containers. FastAI.Datasets.datasetpath FastAI.Datasets.FileDataset FastAI.Datasets.TableDataset mapobs groupobs joinobs groupobs Low - level Full control over data containers. LearnBase.getobs LearnBase.nobs"},{"doctype":"documentation","id":"references/FastAI.Tabular.gettransformdict","title":"gettransformdict","text":" The helper functions defined below can be used for quickly constructing a dictionary, which will be required for creating various tabular transformations available in DataAugmentation.jl. These functions assume that the table in the TableDataset object td has Tables.jl columnaccess interface defined."},{"doctype":"documentation","id":"references/DataAugmentation.PadDivisible","title":"PadDivisible","text":""},{"doctype":"documentation","id":"references/FastAI.methodlearner","title":"methodlearner","text":""},{"doctype":"documentation","id":"references/FluxTraining.Phases.AbstractTrainingPhase","title":"AbstractTrainingPhase","text":" abstract type AbstractTrainingPhase <: Phase\n An abstract type for phases where parameter updates are being made. This exists so callbacks can dispatch on it and work with custom training phases. The default implementation for supervised tasks is  TrainingPhase ."},{"doctype":"documentation","id":"references/Flux._findval","title":"_findval","text":""},{"doctype":"documentation","id":"references/DataAugmentation.testitem","title":"testitem","text":" testitem(TItem)\n Create an instance of an item with type  TItem . If it has spatial bounds, should return an instance with bounds with ranges (1:16, 1:16)."},{"doctype":"documentation","id":"references/FluxTraining.GarbageCollect","title":"GarbageCollect","text":" GarbageCollect(nsteps)\n Every  nsteps  steps, forces garbage collection. Use this if you get memory leaks from, for example, parallel data loading. Performs an additional C-call on Linux systems that can sometimes help."},{"doctype":"documentation","id":"references/FastAI.Vision.Mask","title":"Mask","text":" Mask{N, T}(classes) <: Block\n Block for an N-dimensional categorical mask.  obs  is valid for Mask{N, T}(classes) if it is an N-dimensional array with every element in  classes ."},{"doctype":"documentation","id":"references/FluxTraining.LogVisualization","title":"LogVisualization","text":" LogVisualization(visfn, backends...[; freq = 100])\n Logs images created by  visfn(learner.step)  to  backends  every  freq  steps."},{"doctype":"documentation","id":"references/FastAI.Datasets.JoinedData","title":"JoinedData","text":""},{"doctype":"documentation","id":"references/FastAI.blockmodel","title":"blockmodel","text":" blockmodel(inblock, outblock[, backbone = blockbackbone(inblock)])\n From a  backbone  model, construct a model suitable for learning a mapping from  inblock  to  outblock ."},{"doctype":"documentation","id":"references/FluxTraining.Events.Event","title":"Event","text":" abstract type Event\n Abstract type for events that callbacks can hook into"},{"doctype":"documentation","id":"references/FluxTraining.setindexperm!","title":"setindexperm!","text":""},{"doctype":"documentation","id":"references/DataAugmentation.adjustcontrast","title":"adjustcontrast","text":""},{"doctype":"documentation","id":"references/FluxTraining.CancelFittingException","title":"CancelFittingException","text":" CancelFittingException(msg)\n Throw during fitting to cancel it."},{"doctype":"documentation","id":"references/FastAI.Vision","title":"Vision","text":""},{"doctype":"documentation","id":"references/DataAugmentation.ImageToTensor","title":"ImageToTensor","text":" ImageToTensor()\n Expands an  Image{N, T}  of size  sz  to an  ArrayItem{N+1}  with size  (sz..., ch)  where  ch  is the number of color channels of  T . Supports  apply! . Examples using   DataAugmentation ,   Images \n \n image   =   Image ( rand ( RGB ,   50 ,   50 ) ) \n tfm   =   ImageToTensor ( ) \n apply ( tfm ,   image )"},{"doctype":"documentation","id":"references/Flux.plateau","title":"plateau","text":" plateau(f, width; distance = -, init_score = 0, min_dist = 1f-6)\n Return a function that internally counts by one when abs(distance(last_score, f(...))) <= min_dist , where last_score  holds the last value of  f(...) . If the count is greater than or equal to  width , the function returns  true , otherwise it returns  false . The count is reset when  abs(distance(last_score, f(...))) > min_dist . Examples julia> f = let v = 10\n         () -> v = v / abs(v) - v\n       end; # -9, 8, -7, 6, ...\n\njulia> trigger = Flux.plateau(f, 3; init_score=10, min_dist=18);\n\n\njulia> Flux.@epochs 10 begin\n         trigger() && break\n       end\n[ Info: Epoch 1\n[ Info: Epoch 2\n[ Info: Epoch 3\n[ Info: Epoch 4\n"},{"doctype":"documentation","id":"references/FastAI.Vision.RECIPES","title":"RECIPES","text":""},{"doctype":"documentation","id":"references/FastAI.wrapped","title":"wrapped","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.Models.linbndrop","title":"linbndrop","text":""},{"doctype":"documentation","id":"references/FluxTraining.hasconflict","title":"hasconflict","text":""},{"doctype":"documentation","id":"references/FastAI.group","title":"group","text":""},{"doctype":"documentation","id":"references/DataAugmentation.apply","title":"apply","text":" apply(tfm, item[; randstate])\napply(tfm, items[; randstate])\n Apply  tfm  to an  item  or a tuple  items ."},{"doctype":"documentation","id":"references/FluxTraining.LogHyperParams","title":"LogHyperParams","text":" LogHyperParams(backends...) <: Callback\n Callback that logs hyperparameters to one or more  LoggerBackend s. See also  LoggerBackend ,  Loggables.Loggable ,  log_to , TensorBoardBackend Example logcb = LogHyperParams(TensorBoardBackend(\"tblogs\"))\nschedule = ...\nLearner(model, data, opt, lossfn, Scheduler(LearningRate => schedule), logcb)"},{"doctype":"documentation","id":"references/Flux.Optimise.train!","title":"train!","text":" train!(loss, pars::Params, data, opt::AbstractOptimiser; [cb])\n Uses a  loss  function and training  data  to improve the model’s parameters according to a particular optimisation rule  opt . For each  d in data , first the gradient of the  loss  is computed like this:     gradient(() -> loss(d...), pars)  # if d isa Tuple\n    gradient(() -> loss(d), pars)     # otherwise\n Here  pars  is produced by calling  Flux.params  on your model. (Or just on the layers you want to train, like  train!(loss, params(model[1:end-2]), data, opt) .) This is the “implicit” style of parameter handling. Then, this gradient is used by optimizer  opt  to update the paramters:     update!(opt, pars, grads)\n The optimiser should be from the  Flux . Optimise  module. Different optimisers can be combined using  Flux . Optimise . Optimiser . This training loop iterates through  data  once. You can use  @epochs  to do this several times, or use for instance  Iterators.repeat  to make a longer  data  iterator. Callbacks Callbacks  are given with the keyword argument  cb . For example, this will print “training” every 10 seconds (using  Flux.throttle ):     train!(loss, params, data, opt, cb = throttle(() -> println(\"training\"), 10))\n The callback can call  Flux.stop  to interrupt the training loop. Multiple callbacks can be passed to  cb  as array."},{"doctype":"documentation","id":"references/Flux._dropout_mask","title":"_dropout_mask","text":""},{"doctype":"documentation","id":"references/FluxTraining.Events.BackwardEnd","title":"BackwardEnd","text":" BackwardEnd()\n Event  called between calculating gradients and updating parameters."},{"doctype":"documentation","id":"references/FastAI.Vision.Keypoints","title":"Keypoints","text":" Keypoints{N}(sz) <: Block\n A block representing an array of size  sz  filled with keypoints of type SVector{N} ."},{"doctype":"document","id":"documents/notebooks/presizing.ipynb","title":"Presizing vision datasets for performance","text":" Presizing vision datasets for performance In this tutorial, we’ll look at how we can improve the performance of computer vision data pipelines by presizing. Presizing is a dataset preprocessing step executed before any training in which all images are loaded and saved at a smaller size. This can improve performance if image loading is a bottleneck. Presizing is useful when the original image sizes in a dataset are much larger than the size we use during training; if the images are downsized every time they are loaded anyway, a lot of work can be avoided by doing this once before training. For example, if we train an image classification model on resized image crops of size  (160, 160)  then we can presize every image so that the shorter length is at least 160. Let’s first look at the performance difference at this makes. Conveniently, the ImageNette dataset, comes in 3 sizes: original, 320px and 160px. Let’s download them and create data containers that load the images and do nothing else: using   FastAI \n using   FastAI . Datasets \n \n p_160   =   datasetpath ( \" imagenette2-160 \" ) \n p_320   =   datasetpath ( \" imagenette2-320 \" ) \n p_orig   =   datasetpath ( \" imagenette2 \" ) imagedatacontainer ( dir )   =   mapobs ( loadfile ,   filterobs ( isimagefile ,   FileDataset ( dir ) ) ) \n \n data_160   =   imagedatacontainer ( p_160 ) \n data_320   =   imagedatacontainer ( p_320 ) \n data_orig   =   imagedatacontainer ( p_orig ) Now every observation is simply a single image, but the sizes vary: using   MosaicViews \n mosaicview ( [ getobs ( data_160 ,   2000 ) ,   getobs ( data_320 ,   2000 ) ,   getobs ( data_orig ,   2000 ) ] ;   nrow = 1 ) Let’s see how long it takes to load a random subset of 100 images: idxs   =   rand ( 1 : nobs ( data_160 ) ,   100 ) \n for   ( name ,   data )   in   zip ( ( \" 160px \" ,   \" 320px \" ,   \" Original \" ) ,   ( data_160 ,   data_320 ,   data_orig ) ) \n         println ( \" Size:  $ name \" ) \n     @ time   for   i   in   idxs \n         getobs ( data ,   i ) \n     end \n end Quite a difference! The 320px version loads about 4 times slower than the 160px version; after all there are 4 times as many pixels. Note that image loading is only a part of the data pipeline. Optimizing it with presizing only makes sense if it becomes a bottleneck. See  Performant data pipelines  for a more general discussion. Implementing presizing Next we’ll look at how to do presizing ourselves. For an image classification dataset, this entails copying the folder structure but replacing every image with a downscaled version. Let’s say, as above, we want to train a model on images of size (160, 160). Since we still want to use random crops during training, we don’t want to do the cropping yet. Instead we downscale the image while preserving the aspect ratio so that the smallest side is still at least 160 pixels long. The following function does just that for a single image: using   Images \n \n function   presizeimage ( image ,   sz ) \n     ratio   =   maximum ( sz   ./   size ( image ) ) \n     newsz   =   round . ( Int ,   size ( image )   .*   ratio ) \n     σ   =   0.25   .*   (   1   ./   ( ratio ,   ratio ) ) \n     k   =   KernelFactors . gaussian ( σ ) \n     return   imresize ( imfilter ( image ,   k ,   NA ( ) ) ,   newsz ) \n end SZ   =   ( 160 ,   160 ) \n image   =   getobs ( data_orig ,   2000 ) \n presizeimage ( image ,   SZ ) Now we need to run this over every image in a folder. To speed things up, we run this in parallel using  Threads.@threads . using   FilePathsBase \n \n DSTDIR   =   Path ( mktempdir ( ) ) \n \n function   presizeimagedir ( srcdir ,   dstdir ,   sz ) \n     pathdata   =   filterobs ( isimagefile ,   FileDataset ( srcdir ) ) \n     \n     # create directories beforehand \n     for   i   in   1 : nobs ( pathdata ) \n         mkpath ( parent ( getobs ( pathdata ,   i ) ) ) \n     end \n     \n     Threads . @ threads   for   i   in   1 : nobs ( pathdata ) \n         srcp   =   getobs ( pathdata ,   i ) \n         p   =   relpath ( srcp ,   srcdir ) \n         dstp   =   joinpath ( dstdir ,   p ) \n         \n         img   =   loadfile ( srcp ) \n         img_presized   =   presizeimage ( img ,   sz ) \n         save ( string ( dstp )   *   \" .jpg \" ,   img_presized ) \n     end   \n end @ time   presizeimagedir ( p_orig ,   DSTDIR ,   SZ ) We can now load the created dataset as a regular image classification dataset: data   =   loadtaskdata ( DSTDIR ,   ImageClassification ) ; Remarks Keypoint and segmentation data : Presizing can of course be useful with other image datasets like those for segmantic segmentation and with keypoint data. You have to be more careful when presizing those, though, since the target variable is affected by the resizing: if an image is downsized, then any segmentation masks and keypoints on it also need to be downsized. Progressive resizing : Presizing can also be used in conjunction with progressive resizing, a technique pioneered by Jeremy Howard, where the training starts with small image sizes for speed and uses larger image sizes later for better performance. This can improve convergence speed quite a bit."},{"doctype":"documentation","id":"references/DataAugmentation.FlipY","title":"FlipY","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Histogram","title":"Histogram","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.AbstractDatasetRegistry","title":"AbstractDatasetRegistry","text":""},{"doctype":"documentation","id":"references/FastAI.Tabular.RECIPES","title":"RECIPES","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.AdaMax","title":"AdaMax","text":" AdaMax(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = \n"},{"doctype":"documentation","id":"references/FastAI.decodey!","title":"decodey!","text":""},{"doctype":"documentation","id":"references/FluxTraining.CheckIteratesTuples","title":"CheckIteratesTuples","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.Models.pixelshufflehead","title":"pixelshufflehead","text":""},{"doctype":"documentation","id":"references/Flux.zeros32","title":"zeros32","text":""},{"doctype":"documentation","id":"references/Flux.Losses.siamese_contrastive_loss","title":"siamese_contrastive_loss","text":" siamese_contrastive_loss(ŷ, y; margin = 1, agg = mean)\n Return the  contrastive loss which can be useful for training Siamese Networks. It is given by agg(@. (1 - y) * ŷ^2 + y * max(0, margin - ŷ)^2)                           \n Specify  margin  to set the baseline for distance at which pairs are dissimilar."},{"doctype":"documentation","id":"references/Flux.flip","title":"flip","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.pathparent","title":"pathparent","text":""},{"doctype":"documentation","id":"references/FluxTraining.sethyperparameter!","title":"sethyperparameter!","text":" sethyperparameter!(learner, H, value)\n Sets hyperparameter  H  to  value  on  learner ."},{"doctype":"documentation","id":"references/FastAI.Datasets.recipeblocks","title":"recipeblocks","text":" recipeblocks(TRecipe) -> TBlocks\nrecipeblocks(recipe) -> TBlocks\n Return the  Block   types  for the data container that recipe type  TRecipe  creates. Does not return  Block  instances as the exact configuration may not be known until the dataset is being loaded. Examples recipeblocks ( ImageFolders )   ==   Tuple { Image { 2 } ,   Label }"},{"doctype":"documentation","id":"references/FastAI.encodesample","title":"encodesample","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.ImagePreprocessing","title":"ImagePreprocessing","text":" ImagePreprocessing([; kwargs...]) <: Encoding\n Encodes  Image s by converting them to a common color type  C , expanding the color channels and normalizing the channel values. Additionally, apply pixel-level augmentations passed in as  augmentations during  Training . Encodes Image{N}  ->  ImageTensor{N} Keyword arguments augmentations:: DataAugmentation.Transform : Augmentation to apply to every image before preprocessing. See  augs_lighting buffered = true : Whether to use inplace transformations. Reduces memory usage. means::SVector = IMAGENET_MEANS : mean value of each color channel. stds::SVector = IMAGENET_STDS : standard deviation of each color channel. C::Type{<:Colorant} = RGB{N0f8} : color type to convert images to. T::Type{<:Real} = Float32 : element type of output"},{"doctype":"documentation","id":"references/FluxTraining.testlearner","title":"testlearner","text":" testlearner(callbacks...[; opt, nbatches, coeff, batchsize, kwargs...])\n Construct a  Learner  with a simple optimization problem. This learner should be used in tests that require training a model, e.g. for callbacks."},{"doctype":"documentation","id":"references/FastAI.Datasets.loadrecipe","title":"loadrecipe","text":" loadrecipe(recipe, path)\n Load a recipe from a path. Return a data container  data  and concrete blocks ."},{"doctype":"documentation","id":"references/FastAI.Tabular.TabularRegression","title":"TabularRegression","text":" TabularRegression(blocks, data)\n Learning task for tabular regression. Continuous columns are normalized and missing values are filled, categorical columns are label encoded taking into account any missing values which might be present. blocks  should be an input and target block  (TableRow(...), Continuous(...)) . TabularRegression(n, tabledata [; catcols, contcols])\n Construct learning task with  classes  to classify into and a  TableDataset tabledata . The column names can be passed in or guessed from the data. The regression target is a vector of  n  values."},{"doctype":"documentation","id":"references/Flux.Losses.logaddexp","title":"logaddexp","text":" logaddexp(a, b) Adds log-space  a  and  b  such that the result equals  log(exp(a)+exp(b))"},{"doctype":"documentation","id":"references/Flux.InstanceNorm","title":"InstanceNorm","text":" InstanceNorm(channels::Integer, λ=identity;\n             initβ=zeros32, initγ=ones32,\n             affine=false, track_stats=false,\n             ϵ=1f-5, momentum=0.1f0)\n Instance Normalization  layer. channels  should be the size of the channel dimension in your data (see below). Given an array with  N > 2  dimensions, call the  N-1 th the channel dimension. For  WHCN  images it’s the usual channel dimension. InstanceNorm  computes the mean and variance for each  D_1×...×D_{N-2}×1×1 input slice and normalises the input accordingly. If  affine=true , it also applies  a shift and a rescale to the input through to learnable per-channel bias  β  and scale  γ  parameters. If  track_stats=true , accumulates mean and var statistics in training phase that will be used to renormalize the input in test phase. Warning : the defaults for  affine  and  track_stats  used to be  true in previous Flux versions (< v0.12)."},{"doctype":"documentation","id":"references/FluxTraining.epoch!","title":"epoch!","text":" epoch!(learner, phase[, dataiter])\n Train  learner  for one epoch on  dataiter . Iterates through dataiter  and  step! s for each batch/item. If no data iterator is passed in, use  learner.data[phasedataiter(phase)] . Extending The default implementation iterates over every batch in  dataiter and calls  step!  for each. This behavior can be overloaded by implementing  epoch!(learner, ::MyPhase, dataiter) . If you’re implementing a custom  epoch!  method, it is recommended you make use of  runepoch  to get begin and end events as well as proper handling of  CancelEpochException s. See the default implementation for reference."},{"doctype":"documentation","id":"references/FastAI.Vision.Models.visionhead","title":"visionhead","text":""},{"doctype":"documentation","id":"references/FluxTraining.setcallbacks!","title":"setcallbacks!","text":" setcallbacks!(learner, callbacks)\n Set  learner ’s callbacks to  callbacks , removing all current callbacks."},{"doctype":"documentation","id":"references/Flux.activations","title":"activations","text":" activations(c::Chain, input)\n Calculate the forward results of each layers in Chain  c  with  input  as model input."},{"doctype":"documentation","id":"references/DataAugmentation.imagetotensor","title":"imagetotensor","text":""},{"doctype":"documentation","id":"references/FluxTraining.Unresolvable","title":"Unresolvable","text":" abstract type Unresolvable <: ConflictResolution\n Return from  resolveconflict  to indicate that two callbacks are incompatible and cannot be used together."},{"doctype":"documentation","id":"references/FastAI.Datasets.pathname","title":"pathname","text":""},{"doctype":"documentation","id":"references/DataAugmentation.RandomResizeCrop","title":"RandomResizeCrop","text":""},{"doctype":"documentation","id":"references/Flux.rng_from_array","title":"rng_from_array","text":" rng_from_array([x])\n Create an instance of the RNG most appropriate for  x . The current defaults are: x isa CuArray :  CUDA.default_rng() , else: x isa AbstractArray , or no  x  provided: Julia version is < 1.7:  Random.GLOBAL_RNG Julia version is >= 1.7:  Random.default_rng()"},{"doctype":"documentation","id":"references/FluxTraining.testbatches","title":"testbatches","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.ImageClassificationSingle","title":"ImageClassificationSingle","text":" ImageClassificationSingle(size, classes; kwargs...)\nImageClassificationSingle(blocks[, data]; kwargs...)\n Learning task for single-label image classification. Images are resized to  size  and classified into one of  classes . Use  ImageClassificationMulti  for the multi-class setting. Keyword arguments computestats = false : Whether to compute image statistics on dataset  data  or use default ImageNet stats. aug_projections =  DataAugmentation.Identity : augmentation to apply during ProjectiveTransforms  (resizing and cropping) aug_image =  DataAugmentation.Identity : pixel-level augmentation to apply during ImagePreprocessing C = RGB{N0f8} : Color type images are converted to before further processing. Use  Gray{N0f8} for grayscale images."},{"doctype":"documentation","id":"references/FastAI.Many","title":"Many","text":" Many(block) <: WrapperBlock\n Many  indicates that you can variable number of instances for block . Consider a bounding box detection task where there may be any number of targets in an image and this number varies for different samples. The blocks  (Image{2}(), BoundingBox{2}()  imply that there is exactly one bounding box for every image, which is not the case. Instead you would want to use  (Image{2}(), Many(BoundingBox{2}()) ."},{"doctype":"documentation","id":"references/FluxTraining.Phases.AbstractValidationPhase","title":"AbstractValidationPhase","text":" abstract type AbstractValidationPhase <: Phase\n An abstract type for phases where no parameter updates are being made. This exists so callbacks can dispatch on it and work with custom validation phases. The default implementation for supervised tasks is  ValidationPhase ."},{"doctype":"documentation","id":"references/Flux.randn32","title":"randn32","text":""},{"doctype":"documentation","id":"references/FluxTraining.NotDefined","title":"NotDefined","text":" abstract type NotDefined <: ConflictResolution\n The default implementation of  resolveconflict . If a conflict is detected, this ensures an error message is printed."},{"doctype":"documentation","id":"references/FluxTraining.findconflicts","title":"findconflicts","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Text","title":"Text","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.Bounded","title":"Bounded","text":" Bounded(block, size) <: WrapperBlock\n A  WrapperBlock  for annotating spatial data blocks with size information for their spatial bounds. As an example, Image{2}()  doesn’t carry any size information since it supports variable-size images, but sometimes it can be useful to have the exact size as information where it can be known. Encoding using  ProjectiveTransforms  returns  Bounded s since it crops any input to the same size. Examples block   =   Image { 2 } ( )    # a 2D-image block with out size informatio \n wrapper   =   Bounded ( Image { 2 } ( ) ,   ( 128 ,   128 ) )    # a 2D-image block with fixed size \n \n @ test   checkblock ( block ,   rand ( 10 ,   10 ) ) \n @ test   ! checkblock ( wrapper ,   rand ( 10 ,   10 ) )    # Expects size `(128, 128)` Wrapping a  Bounded  into another  Bounded  with the same dimensionality will update the bounds: block   =   Image { 2 } ( ) \n Bounded ( Bounded ( block ,   ( 16 ,   16 ) ) ,   ( 8 ,   8 ) )   ==   Bounded ( block ,   ( 8 ,   8 ) )"},{"doctype":"documentation","id":"references/Flux.underscorise","title":"underscorise","text":""},{"doctype":"documentation","id":"references/DataAugmentation.AbstractArrayItem","title":"AbstractArrayItem","text":" abstract type AbstractArrayItem{N, T}\n Abstract type for all [ Item ]s that wrap an  N -dimensional array with element type  T ."},{"doctype":"documentation","id":"references/DataAugmentation.NormalizeIntensity","title":"NormalizeIntensity","text":""},{"doctype":"documentation","id":"references/Flux._all","title":"_all","text":""},{"doctype":"documentation","id":"references/DataAugmentation.ArrayItem","title":"ArrayItem","text":" ArrayItem(a)\n An item that contains an array."},{"doctype":"documentation","id":"references/FastAI.Datasets.DESCRIPTIONS","title":"DESCRIPTIONS","text":""},{"doctype":"documentation","id":"references/Flux._show_children","title":"_show_children","text":""},{"doctype":"documentation","id":"references/FastAI.lrfind","title":"lrfind","text":" lrfind(learner[, dataiter; kwargs...]) -> LRFinderResult\n Run the learning rate finder. Exponentially increases the learning rate from a very low value to a very high value and uses the losses to estimate an optimal learning rate. Return a  LRFinderResult . Keyword arguments nsteps = 100 : maximum number of steps to run the learning rate finder for startlr = 1e-7 : minimum learning rate endlr = 10 : maximum learning rate divergefactor : stop finder early if loss goes higher than lowest loss times this factor estimators = [Steepest(), MinDivByTen()] : list of  LREstimator s"},{"doctype":"documentation","id":"references/DataAugmentation","title":"DataAugmentation","text":""},{"doctype":"documentation","id":"references/DataAugmentation.WarpAffine","title":"WarpAffine","text":" WarpAffine(σ = 0.1) <: ProjectiveTransform\n A three-point affine warp calculated by randomly moving 3 corners of an item. Similar to a random translation, shear and rotation."},{"doctype":"documentation","id":"references/FastAI.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.__init__","title":"__init__","text":""},{"doctype":"documentation","id":"references/FluxTraining.runstep","title":"runstep","text":" runstep(stepfn, learner, phase) -> state\n Run  stepfn  inside the context of a step. Calls  stepfn(handle, state) where  handle(e)  can be called to dispatch events and  state  is a  PropDict which step data, gradients and losses can be written to. Return  state . Takes care of dispatching  StepBegin  and  StepEnd events as well as handling  CancelStepException s."},{"doctype":"documentation","id":"references/FastAI.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.tests","title":"tests","text":""},{"doctype":"document","id":"documents/docs/learning_methods.md","title":"Custom learning tasks","text":" Custom learning tasks This tutorial explains the low - level interface behind  BlockTask s and how to use it to create your custom learning tasks without the data block interface . In the  quickstart  section, you’ve already seen a learning task in action:  BlockTask . The learning task abstraction powers FastAI.jl’s high-level interface allowing you to make training models for a task simple.  BlockTask  is a particularly convenient and composable interface for creating learning tasks and should be preferred for most use cases. However, to get a look behind the scenes, in this tutorial we’ll use the lower-level learning task interface to implement our own version of an image classification learning task. You’re encouraged to follow along in a REPL or notebook. This tutorial can also serve as a template for implementing a custom learning task for your own project. A learning task describes how we need to process data so we can train a model for some task. In our case, the task we want to solve is to classify an image. The task defines what kind of data we need, here pairs of images and class labels. That alone, however, isn’t enough to train a model since we can’t just throw an image in any format into a model and get a class out. Almost always the input data needs to be processed in some way before it is input to a model (we call this  encoding ) and the same goes for the model outputs (we call this  decoding ). So let’s say we have an image and a trained model. How do we make a prediction? First we encode the image, run it through the model, and then decode the output. Similarly, how we can use a pair of image and class to train a model? We encode both, run the encoded input through the model and then compare the output with the encoded class using a  loss function . The result tells us how we’ll need to update the weights of the model to improve its performance. In essence, the learning task interface allows us to implement these steps and derive useful functionality from it, like training and evaluating models. Later we’ll also cover some optional interfaces that allow us to define other parts of a deep learning project. Datasets Before we get started, let’s load up a  data container  that we can test our code on as we go. It’s always a good idea to interactively test your code! Since we’ll be implementing a task for image classification, the observations in our data container will of course have to be pairs of images and classes. We’ll use one of the many image classification datasets available from the fastai dataset repository. I’ll use ImageNette, but you can use any of the datasets listed in  FastAI.Datasets.DATASETS_IMAGECLASSIFICATION . The way the interface is built allows you to easily swap out the dataset you’re using. using   FastAI ,   FastAI . DataAugmentation ,   Colors \n import   FastAI :   Image \n data   =   Datasets . loadfolderdata ( \n     datasetpath ( \" imagenette2-160 \" ) , \n     filterfn = isimagefile , \n     loadfn = ( loadfile ,   parentname ) ) We’ll also collect the unique class names: images ,   targets   =   data \n classes   =   unique ( eachobs ( targets ) ) Implementation Learning task struct Now let’s get to it! The first thing we need to do is to create a  LearningTask  struct. The  LearningTask   struct  should contain all the configuration needed for encoding and decoding the data. We’ll keep it simple here and include a list of the classes and the image dimensions input to the model.  struct   ImageClassification   <:   FastAI . LearningTask \n     classes \n     size \n end Now we can create an instance of it, though of course it can’t do anything (yet!). task   =   ImageClassification ( classes ,   ( 128 ,   128 ) ) Encoding and decoding There are 3 tasks we need to define before we can use our learning task to train models and make predictions: encodesample  which encodes an image and a class encodeinput  will encode an image so it can be input to a model decodeypred  decodes a model output into a class label Note: These functions always operate on  single  images and classes, even if we want to pass batches to the model later on. While it’s not the focus of this tutorial, let’s give a quick recap of how the data is encoded and decoded for image classification. Images are cropped to a common size so they can be batched, converted to a 3D array with dimensions (height, width, color channels) and normalized Classes are encoded as one-hot vectors, teaching the model to predict a confidence distribution over all classes. To decode a predicted one-hot vector, we can simply find the index with the highest value and look up the class label. Each of the tasks also takes a  context:: FastAI.Context  argument which allows it to behave differently during training, validation and inference. We’ll make use of that to choose a different image crop for each situation. During training we’ll use a random crop for augmentation, while during validation a center crop will ensure that any metrics we track are the same every epoch. During inference, we won’t crop the image so we don’t lose any information. Inputs We implement  encodeinput  using  DataAugmentation . jl . Feel free to look at  its documentation , we won’t focus on it here. using   FastAI . Vision :   IMAGENET_MEANS ,   IMAGENET_STDS    # color statistics for normalization \n \n # Helper for crop based on context \n getresizecrop ( context :: Training ,   sz )   =   DataAugmentation . RandomResizeCrop ( sz ) \n getresizecrop ( context :: Validation ,   sz )   =   CenterResizeCrop ( sz ) \n getresizecrop ( context :: Inference ,   sz )   =   ResizePadDivisible ( sz ,   32 ) \n \n function   FastAI . encodeinput ( \n         task :: ImageClassification , \n         context :: Context , \n         image ) \n     tfm   =   DataAugmentation . compose ( \n         getresizecrop ( context ,   task . size ) , \n         ToEltype ( RGB { Float32 } ) , \n         ImageToTensor ( ) , \n         Normalize ( IMAGENET_MEANS ,   IMAGENET_STDS ) ; \n     ) \n     return   apply ( tfm ,   DataAugmentation . Image ( image ) )   |>   itemdata \n end If we test this out on an image, it should give us a 3D array of size  (128, 128, 3) , and indeed it does: sample   =   image ,   class   =   getobs ( data ,   1 ) \n x   =   FastAI . encodeinput ( task ,   Training ( ) ,   image ) \n summary ( x ) Outputs encodetarget  is much simpler: function   FastAI . encodetarget ( \n         task :: ImageClassification , \n         :: Context , \n         class ) \n     idx   =   findfirst ( isequal ( class ) ,   task . classes ) \n     v   =   zeros ( Float32 ,   length ( task . classes ) ) \n     v [ idx ]   =   1. \n     return   v \n end \n \n FastAI . encodesample ( task :: ImageClassification ,   ctx ,   ( input ,   target ) )   =   ( \n     encodeinput ( task ,   ctx ,   input ) , \n     encodetarget ( task ,   ctx ,   target ) , \n ) y   =   FastAI . encodetarget ( task ,   Training ( ) ,   class ) The same goes for the decoding step: function   FastAI . decodeypred ( task :: ImageClassification ,   :: Context ,   ypred ) \n     return   task . classes [ argmax ( ypred ) ] \n end FastAI . decodeypred ( task ,   Training ( ) ,   y )   ==   class Training And that’s all we need to start training models! There are some optional interfaces that make that even easier, but let’s use what we have for now. With our  LearningTask  defined, we can use  taskdataloaders  to turn a dataset into a set of training and validation data loaders that can be thrown into a training loop. traindl ,   valdl   =   taskdataloaders ( data ,   task ) Now, with a makeshift model, an optimizer and a loss function we can create a  Learner . using   FastAI ,   Flux \n \n model   =   Chain ( \n     Models . xresnet18 ( ) , \n     Chain ( \n             AdaptiveMeanPool ( ( 1 , 1 ) ) , \n             flatten , \n             Dense ( 512 ,   length ( task . classes ) ) , \n     ) \n ) \n opt   =   ADAM ( ) \n lossfn   =   Flux . Losses . logitcrossentropy \n \n learner   =   Learner ( model ,   ( traindl ,   valdl ) ,   opt ,   lossfn ) From here, you’re free to start training using   fit!  or  fitonecycle! . These tasks are also enough to use  predict  and  predictbatch  once you’ve trained a model. Additional interfaces Training interface We can implement some additional tasks to make our life easier. Specifically, let’s implement every task needed to use  tasklearner : tasklossfn : return a loss function  lossfn(ys, ys)  comparing a batch of model outputs and encoded targets taskmodel : from a backbone, construct a model suitable for the task Let’s start with the loss function. We want to compare two one-hot encoded categorical variables, for which categorical cross entropy is the most commonly used loss function. FastAI.tasklossfn(task::ImageClassification) = Flux.Losses.logitcrossentropy\n For the model, we’ll assume we’re getting a convolutional feature extractor passed in as a backbone so its output will be of size (height, width, channels, batch size).  Flux.outputsize  can be used to calculate the output size of arbitrary models without having to evaluate the model. We’ll use it to check the number of output channels of the backbone. Then we add a global pooling layer and some dense layers on top to get a classification output. function   FastAI . taskmodel ( task :: ImageClassification ,   backbone ) \n     h ,   w ,   outch ,   b   =   Flux . outputsize ( backbone ,   ( 256 ,   256 ,   inblock . nchannels ,   1 ) ) \n     head   =   Chain ( \n         AdaptiveMeanPool ( ( 1 ,   1 ) ) , \n         Dense ( outch ,   512 ) , \n         BatchNorm ( 512 ) , \n         Dense ( 512 ,   length ( task . classes ) ) \n     ) \n     return   Chain ( backbone ,   head ) \n end"},{"doctype":"documentation","id":"references/DataAugmentation.makebuffer","title":"makebuffer","text":" makebuffer(tfm, item)\n Create a buffer  buf  that can be used in a call to  apply!(buf, tfm, item) . Default to  buffer = apply(tfm, item) . You only need to implement this if the default  apply(tfm, item)  isn’t enough. See  apply(tfm::Sequence, item)  for an example of this."},{"doctype":"documentation","id":"references/Flux.reset!","title":"reset!","text":" reset!(rnn)\n Reset the hidden state of a recurrent layer back to its original value. Assuming you have a  Recur  layer  rnn , this is roughly equivalent to: rnn . state   =   hidden ( rnn . cell )"},{"doctype":"documentation","id":"references/FastAI.Datasets.DatasetRecipe","title":"DatasetRecipe","text":" abstract type DatasetRecipe\n A recipe that contains configuration for loading a data container. Calling it with a path returns a data container and the blocks that each sample is made of. Examples For example implementations, see  Vision.ImageFolders . Extending Interface loadrecipe (::DatasetRecipe, args...; kwargs...) -> (data, blocks) This loads a data container the [ Block ]s that each observation corresponds to. For most recipes the only argument beside the recipe is a path to a folder on disk. recipeblocks (::Type{DatasetRecipe}) -> TBlocks The type of  blocks  returned by  loadrecipe . Should be as specific as possible. Used for discovery. Invariants Given data ,   blocks   =   loadrecipe ( recipe ,   args ... ;   kwargs ... ) the following must hold: ∀i ∈ [1..nobs(data)]: checkblock(blocks, getobs(data, i)) , i.e. data  must be a data container of observations that are valid  blocks . nobs(data) ≥ 1 , i.e. there is at least one observation if the data was loaded without error."},{"doctype":"documentation","id":"references/Flux.Zeros","title":"Zeros","text":""},{"doctype":"documentation","id":"references/Flux.throttle","title":"throttle","text":" throttle(f, timeout; leading=true, trailing=false)\n Return a function that when invoked, will only be triggered at most once during  timeout  seconds. Normally, the throttled function will run as much as it can, without ever going more than once per  wait  duration; but if you’d like to disable the execution on the leading edge, pass  leading=false . To enable execution on the trailing edge, pass  trailing=true ."},{"doctype":"documentation","id":"references/FastAI.Datasets.MappedData","title":"MappedData","text":""},{"doctype":"documentation","id":"references/FluxTraining.reset!","title":"reset!","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.loadfile","title":"loadfile","text":" loadfile(file)\n Load a file from disk into the appropriate format."},{"doctype":"documentation","id":"references/FastAI.decodedblock","title":"decodedblock","text":" decodedblock(encoding, block)\ndecodedblock(encoding, blocks)\ndecodedblock(encodings, blocks)\n Return the block that is obtained by decoding  block  with encoding  E . This needs to be constant for an instance of  E , so it cannot depend on the sample or on randomness. The default is to return  nothing , meaning the same block is returned and not changed. Encodings that return the same block but change the data when decoding should return  block ."},{"doctype":"documentation","id":"references/FastAI.decodeŷ!","title":"decodeŷ!","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.Image","title":"Image","text":" Image{N}() <: Block\n Block  for an N-dimensional image.  obs  is valid for  Image{N}() if it is an N-dimensional array with color or number element type. Examples Creating a block: Image { 2 } ( )    # 2D-image \n Image { 3 } ( )    # 3D-image Example valid images: @ test   checkblock ( Image { 2 } ( ) ,   rand ( RGB ,   10 ,   10 ) )           # Color image \n @ test   checkblock ( Image { 2 } ( ) ,   rand ( 10 ,   10 ) )                # Numbers treated as grayscale \n @ test   checkblock ( Image { 3 } ( ) ,   rand ( Gray { N0f8 } ,   10 ,   10 ,   10 ) )    # Grayscale 3D-image The color channels (if any) are not counted as a dimension and represented through color types like  RGB{N0f8} : @ test   ! checkblock ( Image { 2 } ,   rand ( 10 ,   10 ,   3 ) )    # Not a 2D image You can create a random observation using  mockblock : using   FastAI \n FastAI . mockblock ( Image { 2 } ( ) ) To visualize a 2D-image observation, use  showblock . This is supported for both the  ShowText  and the  ShowMakie  backend. showblock ( Image { 2 } ( ) ,   rand ( RGB { N0f8 } ,   10 ,   10 ) )"},{"doctype":"documentation","id":"references/FastAI.mocksample","title":"mocksample","text":" mocksample(task)\n Generate a random  sample  compatible with  task ."},{"doctype":"documentation","id":"references/DataAugmentation.Buffered","title":"Buffered","text":""},{"doctype":"documentation","id":"references/FastAI._encode","title":"_encode","text":""},{"doctype":"documentation","id":"references/FastAI.describeencodings","title":"describeencodings","text":""},{"doctype":"documentation","id":"references/Flux.RNNCell","title":"RNNCell","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.RE_IMAGEFILE","title":"RE_IMAGEFILE","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loggables.File","title":"File","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.colorchannels","title":"colorchannels","text":""},{"doctype":"documentation","id":"references/Flux.Losses.ctc_loss","title":"ctc_loss","text":" ctc_loss(ŷ, y) Computes the connectionist temporal classification loss between  ŷ and  y . ŷ  must be a classes-by-time matrices, i.e., each row represents a class and each column represents a time step. Additionally, the  logsoftmax  function will be applied to  ŷ , so ŷ  must be the raw activation values from the neural network and not, for example, the activations after being passed through a softmax  activation function.  y  must be a 1D array of the labels associated with  ŷ . The blank label is assumed to be the last label category in  ŷ , so it is equivalent to  size(ŷ, 1) . Used for sequence-to-sequence classification problems such as speech recognition and handwriting recognition where the exact time-alignment of the output (e.g., letters) is not needed to solve the problem. See  Graves et al .  (2006) or  Graves (2012) for mathematical details."},{"doctype":"documentation","id":"references/FastAI.Datasets.TableDataset","title":"TableDataset","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.testrecipe","title":"testrecipe","text":""},{"doctype":"documentation","id":"references/FluxTraining.log_parameters","title":"log_parameters","text":""},{"doctype":"documentation","id":"references/FluxTraining.model!","title":"model!","text":""},{"doctype":"documentation","id":"references/FluxTraining.CallbackCondition","title":"CallbackCondition","text":" abstract type CallbackCondition\n Supertype for conditions to use with  ConditionalCallback . To implement a  CallbackCondition , implement shouldrun (::MyCondition, event, phase) . See  FrequencyThrottle ,  TimeThrottle  and  throttle ."},{"doctype":"documentation","id":"references/Flux._dropout_shape","title":"_dropout_shape","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Transform","title":"Transform","text":" abstract type Transform\n Abstract supertype for all transformations."},{"doctype":"documentation","id":"references/Flux.kaiming_uniform","title":"kaiming_uniform","text":" kaiming_uniform([rng=GLOBAL_RNG], size...; gain = √2) -> Array\nkaiming_uniform([rng]; kw...) -> Function\n Return an  Array{Float32}  of the given  size  containing random numbers drawn from a uniform distribution on the interval  [-x, x] , where  x = gain * sqrt(3/fan_in)  using [ nfan ](  Flux.nfan). This method is described in [1] and also known as He initialization. Examples julia> round.(extrema(Flux.kaiming_uniform(100, 10)), digits=3)\n(-0.774f0, 0.774f0)\n\njulia> round.(extrema(Flux.kaiming_uniform(10, 100)), digits=3)\n(-0.245f0, 0.244f0)\n\njulia> round.(extrema(Flux.kaiming_uniform(100, 100)), digits=3)\n(-0.245f0, 0.245f0)\n References [1] He, Kaiming, et al. “Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.”  Proceedings of the IEEE international conference on computer vision . 2015."},{"doctype":"documentation","id":"references/Flux.MaxPool","title":"MaxPool","text":" MaxPool(window::NTuple; pad=0, stride=window)\n Max pooling layer, which replaces all pixels in a block of size  window  with one. Expects as input an array with  ndims(x) == N+2 , i.e. channel and batch dimensions, after the  N  feature dimensions, where  N = length(window) . By default the window size is also the stride in each dimension. The keyword  pad  accepts the same options as for the  Conv  layer, including  SamePad() . See also  Conv ,  MeanPool ,  AdaptiveMaxPool ,  GlobalMaxPool . Examples julia> xs = rand(Float32, 100, 100, 3, 50);  # batch of 50 RGB images\n\njulia> m = Chain(Conv((5, 5), 3 => 7, pad=SamePad()), MaxPool((5, 5), pad=SamePad()))\nChain(\n  Conv((5, 5), 3 => 7, pad=2),          # 532 parameters\n  MaxPool((5, 5), pad=2),\n)\n\njulia> m[1](xs) |> size\n(100, 100, 7, 50)\n\njulia> m(xs) |> size\n(20, 20, 7, 50)\n\njulia> lay = MaxPool((5,), pad=2, stride=(3,))  # one-dimensional window\nMaxPool((5,), pad=2, stride=3)\n\njulia> lay(rand(Float32, 100, 7, 50)) |> size\n(34, 7, 50)\n"},{"doctype":"documentation","id":"references/Flux.Optimise.batchmemaybe","title":"batchmemaybe","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.AbstractOptimiser","title":"AbstractOptimiser","text":""},{"doctype":"documentation","id":"references/Flux.cpu","title":"cpu","text":" cpu(m)\n Moves  m  onto the CPU, the opposite of  gpu . Recurses into structs marked  @functor . julia> m = Dense(1,2)\nDense(1, 2)\n\njulia> m_gpu = gpu(m)\nDense(1, 2)\n\njulia> typeof(m_gpu.W)\nCuArray{Float32, 2}\n\njulia> m_cpu = cpu(m_gpu)\nDense(1, 2)\n\njulia> typeof(m_cpu.W)\nMatrix{Float32}\n"},{"doctype":"documentation","id":"references/Flux.Conv","title":"Conv","text":" Conv(weight::AbstractArray, [bias, activation; stride, pad, dilation])\n Constructs a convolutional layer with the given weight and bias. Accepts the same keywords (and has the same defaults) as the  Conv((4,4), 3 => 7, relu) method. Examples julia> weight = rand(3, 4, 5);\n\njulia> bias = zeros(5);\n\njulia> c1 = Conv(weight, bias, sigmoid)  # expects 1 spatial dimension\nConv((3,), 4 => 5, σ)  # 65 parameters\n\njulia> c1(randn(100, 4, 64)) |> size\n(98, 5, 64)\n\njulia> Flux.params(c1) |> length\n2\n Conv(filter, in => out, σ = identity;\n     stride = 1, pad = 0, dilation = 1, groups = 1, [bias, init])\n Standard convolutional layer.  filter  is a tuple of integers specifying the size of the convolutional kernel; in  and  out  specify the number of input and output channels. Image data should be stored in WHCN order (width, height, channels, batch). In other words, a 100×100 RGB image would be a  100×100×3×1  array, and a batch of 50 would be a  100×100×3×50  array. This has  N = 2  spatial dimensions, and needs a kernel size like  (5,5) , a 2-tuple of integers. To take convolutions along  N  feature dimensions, this layer expects as input an array with  ndims(x) == N+2 , where  size(x, N+1) == in  is the number of input channels, and  size(x, ndims(x))  is (as always) the number of observations in a batch. Then: filter  should be a tuple of  N  integers. Keywords  stride  and  dilation  should each be either single integer, or a tuple with  N  integers. Keyword  pad  specifies the number of elements added to the borders of the data array. It can be a single integer for equal padding all around, a tuple of  N  integers, to apply the same padding at begin/end of each spatial dimension, a tuple of  2*N  integers, for asymmetric padding, or the singleton  SamePad() , to calculate padding such that size(output,d) == size(x,d) / stride  (possibly rounded) for each spatial dimension. Keyword  groups  is expected to be an  Int . It specifies the number of groups to divide a convolution into. Keywords to control initialization of the layer: init  - Function used to generate initial weights. Defaults to  glorot_uniform . bias  - The initial bias vector is all zero by default. Trainable bias can be disabled entirely by setting this to  false , or another vector can be provided such as  bias = randn(Float32, out) . See also  ConvTranspose ,  DepthwiseConv ,  CrossCor . Examples julia> xs = rand(Float32, 100, 100, 3, 50); # a batch of images\n\njulia> layer = Conv((5,5), 3 => 7, relu; bias = false)\nConv((5, 5), 3 => 7, relu, bias=false)  # 525 parameters\n\njulia> layer(xs) |> size\n(96, 96, 7, 50)\n\njulia> Conv((5,5), 3 => 7; stride = 2)(xs) |> size\n(48, 48, 7, 50)\n\njulia> Conv((5,5), 3 => 7; stride = 2, pad = SamePad())(xs) |> size\n(50, 50, 7, 50)\n\njulia> Conv((1,1), 3 => 7; pad = (20,10,0,0))(xs) |> size\n(130, 100, 7, 50)\n\njulia> Conv((5,5), 3 => 7; stride = 2, dilation = 4)(xs) |> size\n(42, 42, 7, 50)\n"},{"doctype":"documentation","id":"references/FastAI.estimatelr","title":"estimatelr","text":" estimatelr(::LREstimator, losses, lrs)\n Estimate the optimal learning rate using  losses  and  lrs ."},{"doctype":"documentation","id":"references/FluxTraining.SafeCallback","title":"SafeCallback","text":""},{"doctype":"documentation","id":"references/FastAI.assigngroups!","title":"assigngroups!","text":""},{"doctype":"documentation","id":"references/DataAugmentation.CenterCrop","title":"CenterCrop","text":""},{"doctype":"document","id":"documents/docs/setup.md","title":"Setup","text":" Setup FastAI.jl is a  Julia  package. You can download Julia from the  official website . You can install FastAI.jl like any other Julia package using the REPL as follows. using   Pkg \n Pkg . add ( \" FastAI \" ) Visualization  Aside from text-based visualizations, FastAI.jl also defines  Makie . jl  plotting recipes to visualize data. If you want to use them, you’ll have to install and one of the Makie.jl backends  CairoMakie . jl ,  GLMakie . jl  or  WGLMakie . jl  and load the package. # Install backend package once \n using   Pkg \n Pkg . add ( \" CairoMakie \" ) \n \n # Then load it therafter \n import   CairoMakie \n using   FastAI Colab  If you don’t have access to a GPU or want to try out FastAI.jl without installing Julia, try out  this FastAI . jl Colab notebook . We’re working on adding a “Launch Colab” button to every documentation page based off a notebook file, but for now you can copy the code over manually. Threaded data loading  To make use of multi-threaded data loading, you need to start Julia with multiple threads, either with the  -t auto  commandline flag or by setting the environment variable  JULIA_NUM_THREADS . See the  IJulia . jl documentation  for instructions on setting these for Jupyter notebook kernels."},{"doctype":"documentation","id":"references/FastAI.Vision.getimagepreprocessing","title":"getimagepreprocessing","text":""},{"doctype":"documentation","id":"references/Flux.BatchNorm","title":"BatchNorm","text":" BatchNorm(channels::Integer, λ=identity;\n          initβ=zeros32, initγ=ones32,\n          affine = true, track_stats = true,\n          ϵ=1f-5, momentum= 0.1f0)\n Batch Normalization  layer. channels  should be the size of the channel dimension in your data (see below). Given an array with  N  dimensions, call the  N-1 th the channel dimension. For a batch of feature vectors this is just the data dimension, for  WHCN  images it’s the usual channel dimension. BatchNorm  computes the mean and variance for each  D_1×...×D_{N-2}×1×D_N input slice and normalises the input accordingly. If  affine=true , it also applies  a shift and a rescale to the input through to learnable per-channel bias β and scale γ parameters. After normalisation, elementwise activation  λ  is applied. If  track_stats=true , accumulates mean and var statistics in training phase that will be used to renormalize the input in test phase. Use  testmode!  during inference. Examples m   =   Chain ( \n   Dense ( 28 ^ 2   =>   64 ) , \n   BatchNorm ( 64 ,   relu ) , \n   Dense ( 64   =>   10 ) , \n   BatchNorm ( 10 ) , \n   softmax )"},{"doctype":"documentation","id":"references/Flux.DepthwiseConv","title":"DepthwiseConv","text":" DepthwiseConv(filter, in => out, σ=identity; stride=1, pad=0, dilation=1, [bias, init])\nDepthwiseConv(weight::AbstractArray, [bias, activation; stride, pad, dilation])\n Return a depthwise convolutional layer, that is a  Conv  layer with number of groups equal to the number of input channels. See  Conv  for a description of the arguments. Examples julia> xs = rand(Float32, 100, 100, 3, 50);  # a batch of 50 RGB images\n\njulia> lay = DepthwiseConv((5,5), 3 => 6, relu; bias=false)\nConv((5, 5), 3 => 6, relu, groups=3, bias=false)  # 150 parameters \n\njulia> lay(xs) |> size\n(96, 96, 6, 50)\n\njulia> DepthwiseConv((5, 5), 3 => 9, stride=2, pad=2)(xs) |> size\n(50, 50, 9, 50)\n"},{"doctype":"documentation","id":"references/FluxTraining.TestModel","title":"TestModel","text":""},{"doctype":"documentation","id":"references/FastAI.Tabular.TabularPreprocessing","title":"TabularPreprocessing","text":" TabularPreprocessing <: Encoding\n Encodes a  TableRow  by applying the following preprocessing steps: DataAugmentation.NormalizeRow  (for normalizing a row of data for continuous columns) DataAugmentation.FillMissing  (for filling missing values) DataAugmentation.Categorify  (for label encoding categorical columns, which can be later used for indexing into embedding matrices) or a sequence of these transformations."},{"doctype":"documentation","id":"references/FastAI.StatefulEncoding","title":"StatefulEncoding","text":" abstract type StatefulEncoding <: Encoding\n Encoding that needs to compute some state from the whole sample, even if it only transforms some of the blocks. This could be random state for stochastic augmentations that needs to be the same for every block that is encoded. The state is created by calling  encodestate(encoding, context, blocks, sample) and passed to recursive calls with the keyword argument  state . As a result, you need to implement  encode ,  decode ,  encode! ,  decode!  with a keyword argument  state  that defaults to the above call. Same goes for  decode , which should accept a  state  keyword argument defaulting to  decodestate(encoding, context, blocks, sample)"},{"doctype":"documentation","id":"references/DataAugmentation.FillMissing","title":"FillMissing","text":" FillMissing(dict, cols)\n Fills the missing values of a row present in  TabularItem  for the columns specified in  cols  using  dict , which contains the column names as dictionary keys and the value to fill the column with present as dictionary values. Example using   DataAugmentation \n \n cols   =   [ : col1 ,   : col2 ,   : col3 ] \n row   =   ( ;   zip ( cols ,   [ 1 ,   2 ,   3 ] ) ... ) \n item   =   TabularItem ( row ,   cols ) \n fmdict   =   Dict ( : col1   =>   100 ,   : col2   =>   100 ) \n \n tfm   =   FillMissing ( fmdict ,   [ : col1 ,   : col2 ] ) \n apply ( tfm ,   item )"},{"doctype":"documentation","id":"references/FluxTraining.HyperParameter","title":"HyperParameter","text":" HyperParameter{T}\n A hyperparameter is any state that influences the training and is not a parameter of the model. Hyperparameters can be scheduled using the  Scheduler callback."},{"doctype":"documentation","id":"references/FluxTraining.savemodel","title":"savemodel","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Polygon","title":"Polygon","text":" Polygon(points, sz)\nPolygon{N, T, M}(points, bounds)\n Item wrapper around  Keypoints . Examples using   DataAugmentation ,   StaticArrays \n points   =   [ SVector ( 10. ,   10. ) ,   SVector ( 80. ,   20. ) ,   SVector ( 90. ,   70. ) ,   SVector ( 20. ,   90. ) ] \n item   =   Polygon ( points ,   ( 100 ,   100 ) ) showitems ( item )"},{"doctype":"documentation","id":"references/DataAugmentation.threepointwarpaffine","title":"threepointwarpaffine","text":" threepointwarpaffine(srcps, dstps)\n Calculate an affine  CoordinateTransformations.LinearMap from 3 source points to 3 destination points. Adapted from   CoordinateTransformations . jl#30 ."},{"doctype":"documentation","id":"references/FluxTraining.CancelStepException","title":"CancelStepException","text":" CancelStepException(message)\n Throw during fitting to cancel the currently running step. This prematurely ends the current step without throwing an error. Must be thrown inside the context of  runstep . Examples runepoch ( learner ,   phase )   do   _ \n     for   ( xs ,   ys )   in   batches \n         runstep ( learner ,   phase ,   ( ;   xs ,   ys ) )   do   _ ,   state \n             # training logic... \n             if   isnan ( state . loss ) . \n                 throw ( CancelStepException ( \" Skipping NaN loss \" ) ) \n             end \n         end \n     end \n end"},{"doctype":"documentation","id":"references/DataAugmentation.Normalize","title":"Normalize","text":" Normalize(means, stds)\n Normalizes the last dimension of an  AbstractArrayItem{N} . Supports  apply! . Examples Preprocessing a 3D image with 3 color channels. using   DataAugmentation ,   Images \n image   =   Image ( rand ( RGB ,   20 ,   20 ,   20 ) ) \n tfms   =   ImageToTensor ( )   |>   Normalize ( ( 0.1 ,   -0.2 ,   -0.1 ) ,   ( 1 , 1 , 1. ) ) \n apply ( tfms ,   image )"},{"doctype":"documentation","id":"references/FastAI.decodey","title":"decodey","text":""},{"doctype":"documentation","id":"references/FastAI.Vision._registerrecipes","title":"_registerrecipes","text":""},{"doctype":"documentation","id":"references/Flux.PixelShuffle","title":"PixelShuffle","text":" PixelShuffle(r::Int)\n Pixel shuffling layer with upscale factor  r . See  NNlib.pixel_shuffle ."},{"doctype":"documentation","id":"references/DataAugmentation.FromOrigin","title":"FromOrigin","text":""},{"doctype":"document","id":"documents/docs/interfaces.md","title":"Interfaces","text":" Interfaces FastAI.jl provides many interfaces that allow extending its functionality. Learning task interfaces Learning tasks form the core of FastAI.jl’s high-level API. See  this tutorial  for a motivation and introduction. Functions for the learning task interfaces always dispatch on a  LearningTask . A  LearningTask  defines everything that needs to happen to turn an input into a target and much more.  LearningTask  should be a  struct  containing configuration. Core interface Enables training and prediction. Prerequisite for other, optional learning task interfaces. Required tasks: encode  or both  encodeinput  and  encodetarget . decodeŷ Optional tasks: shouldbatch Enables use of: taskdataset taskdataloaders predict predictbatch Plotting interface For visualizing observations and predictions using  Makie . jl . Training interface Convenience for creating  Learner s. Required methods: tasklossfn taskmodel Enables use of: tasklearner Testing interface Automatically test interfaces. Required tasks: mockmodel mocksample  or both  mockinput  and  mocktarget Enables use of: checktask_core Callback interface See the  FluxTraining . jl tutorial . Data container interface"},{"doctype":"documentation","id":"references/FastAI.Tabular.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.tests","title":"tests","text":""},{"doctype":"documentation","id":"references/Flux.Embedding","title":"Embedding","text":" Embedding(in => out; init=randn)\n A lookup table that stores embeddings of dimension  out for a vocabulary of size  in . This layer is often used to store word embeddings and retrieve them using indices. The input to the layer can be either a vector of indexes or the corresponding [onehot encoding](  Flux.OneHotArray). Examples julia> vocab_size, embed_size = 1000, 4;\n\njulia> model = Flux.Embedding(vocab_size => embed_size)\nEmbedding(1000 => 4)  # 4_000 parameters\n\njulia> vocab_idxs = [1, 722, 53, 220, 3];\n\njulia> x = Flux.OneHotMatrix(vocab_idxs, vocab_size); summary(x)\n\"1000×5 OneHotMatrix(::Vector{Int64}) with eltype Bool\"\n\njulia> model(x) |> summary\n\"4×5 Matrix{Float32}\"\n\njulia> model(vocab_idxs) == model(x)\ntrue\n"},{"doctype":"documentation","id":"references/FastAI.Tabular.sigmoidrange","title":"sigmoidrange","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.DATASETS_IMAGECLASSIFICATION","title":"DATASETS_IMAGECLASSIFICATION","text":""},{"doctype":"documentation","id":"references/FastAI.LearningTaskRegistry","title":"LearningTaskRegistry","text":""},{"doctype":"documentation","id":"references/FastAI.encodesample!","title":"encodesample!","text":""},{"doctype":"documentation","id":"references/Flux._paddims","title":"_paddims","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.datasetpath","title":"datasetpath","text":" datasetpath(name)\n Return the folder that dataset  name  is stored. If it hasn’t been downloaded yet, you will be asked if you want to download it. See  Datasets.DATASETS  for a list of available datasets. datasetpath([registry], name)\n (Down)load registered dataset source named  name  from the dataset registry. Use  listdatasources  for a list of all dataset sources."},{"doctype":"documentation","id":"references/FluxTraining.Events.StepBegin","title":"StepBegin","text":" StepBegin()\n Event  called at the beginning of a batch."},{"doctype":"documentation","id":"references/FluxTraining.AbstractMetric","title":"AbstractMetric","text":" abstract type AbstractMetric\n Abstract type for metrics passed to  Metrics . For most use cases, you should use  Metric , the standard implementation. Interface If  Metric  doesn’t fit your use case, you can create a new subtype of  AbstractMetric  and implement the following methods to make it compatible with  Metrics : reset! (metric) step! (metric, learner) stepvalue (metric) epochvalue (metric) metricname (metric)"},{"doctype":"documentation","id":"references/FastAI.Vision.KeypointPreprocessing","title":"KeypointPreprocessing","text":" KeypointPreprocessing(bounds) <: Encoding\n Scale a  Keypoints  block falling in a rectangle of  bounds  so that they lie between -1 and 1."},{"doctype":"documentation","id":"references/FastAI.Tabular.tabular_continuous_backbone","title":"tabular_continuous_backbone","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.NADAM","title":"NADAM","text":" NADAM(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = \n"},{"doctype":"documentation","id":"references/DataAugmentation.showgrid","title":"showgrid","text":""},{"doctype":"documentation","id":"references/FluxTraining.@pack_History","title":"@pack_History","text":""},{"doctype":"documentation","id":"references/DataAugmentation.project","title":"project","text":" project(P, item, indices)\n Project  item  using projection  P  and crop to  indices  if given."},{"doctype":"documentation","id":"references/FastAI.Datasets.joinobs","title":"joinobs","text":" joinobs(datas...)\n Concatenate data containers  datas . data1 ,   data2   =   1 : 10 ,   11 : 20 \n jdata   =   joinobs ( data1 ,   data2 ) \n getobs ( jdata ,   15 )   ==   15"},{"doctype":"documentation","id":"references/FluxTraining.formataccess","title":"formataccess","text":""},{"doctype":"documentation","id":"references/FastAI.tasklossfn","title":"tasklossfn","text":" tasklossfn(task)\n Default loss function to use when training models for  task ."},{"doctype":"documentation","id":"references/FluxTraining.Loggables","title":"Loggables","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.Models.UNetBlock","title":"UNetBlock","text":" UNetBlock(m, k_in)\n Given convolutional module  m  that halves the spatial dimensions and outputs  k_in  filters, create a module that upsamples the spatial dimensions and then aggregates features via  a skip connection."},{"doctype":"documentation","id":"references/Flux.Losses.dice_coeff_loss","title":"dice_coeff_loss","text":" dice_coeff_loss(ŷ, y; smooth = 1)\n Return a loss based on the dice coefficient. Used in the  V - Net  image segmentation architecture. Similar to the F1_score. Calculated as: 1 - 2*sum(|ŷ .* y| + smooth) / (sum(ŷ.^2) + sum(y.^2) + smooth)\n"},{"doctype":"documentation","id":"references/FastAI.Vision.Models.UNetMiddleBlock","title":"UNetMiddleBlock","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.ClipValue","title":"ClipValue","text":" ClipValue(thresh)\n Clip gradients when their absolute value exceeds  thresh ."},{"doctype":"documentation","id":"references/DataAugmentation.BoundingBox","title":"BoundingBox","text":" BoundingBox(points, sz)\nBoundingBox{N, T, M}(points, bounds)\n Item wrapper around  Keypoints . Examples using   DataAugmentation ,   StaticArrays \n points   =   [ SVector ( 10. ,   10. ) ,   SVector ( 80. ,   60. ) ] \n item   =   BoundingBox ( points ,   ( 100 ,   100 ) ) showitems ( item )"},{"doctype":"documentation","id":"references/DataAugmentation.AbstractItem","title":"AbstractItem","text":" abstract type AbstractItem\n Abstract supertype for all items. To implement items, subtype either  Item  to create a new item or  ItemWrapper to wrap an existing item."},{"doctype":"documentation","id":"references/Flux.Losses.focal_loss","title":"focal_loss","text":" focal_loss(ŷ, y; dims=1, agg=mean, γ=2, ϵ=eps(ŷ))\n Return the  focal _ loss which can be used in classification tasks with highly imbalanced classes. It down-weights well-classified examples and focuses on hard examples. The input, ‘ŷ’, is expected to be normalized (i.e.  softmax  output). The modulating factor,  γ , controls the down-weighting strength. For  γ == 0 , the loss is mathematically equivalent to  Losses.crossentropy . Example julia> y = [1  0  0  0  1\n            0  1  0  1  0\n            0  0  1  0  0]\n3×5 Matrix{Int64}:\n 1  0  0  0  1\n 0  1  0  1  0\n 0  0  1  0  0\n\njulia> ŷ = softmax(reshape(-7:7, 3, 5) .* 1f0)\n3×5 Matrix{Float32}:\n 0.0900306  0.0900306  0.0900306  0.0900306  0.0900306\n 0.244728   0.244728   0.244728   0.244728   0.244728\n 0.665241   0.665241   0.665241   0.665241   0.665241\n\njulia> Flux.focal_loss(ŷ, y) ≈ 1.1277571935622628\ntrue\n See also:  Losses.binary_focal_loss  for binary (not one-hot) labels"},{"doctype":"documentation","id":"references/Flux.Optimise.call","title":"call","text":""},{"doctype":"documentation","id":"references/DataAugmentation.FromRandom","title":"FromRandom","text":""},{"doctype":"documentation","id":"references/Flux.SkipConnection","title":"SkipConnection","text":" SkipConnection(layer, connection)\n Create a skip connection which consists of a layer or  Chain  of consecutive layers and a shortcut connection linking the block’s input to the output through a user-supplied 2-argument callable. The first argument to the callable will be propagated through the given  layer  while the second is the unchanged, “skipped” input. The simplest “ResNet”-type connection is just  SkipConnection(layer, +) . Here is a more complicated example: julia> m = Conv((3,3), 4 => 7, pad=(1,1));\n\njulia> x = ones(Float32, 5, 5, 4, 10);\n\njulia> size(m(x)) == (5, 5, 7, 10)\ntrue\n\njulia> sm = SkipConnection(m, (mx, x) -> cat(mx, x, dims=3));\n\njulia> size(sm(x)) == (5, 5, 11, 10)\ntrue\n See also  Parallel ,  Maxout ."},{"doctype":"documentation","id":"references/FastAI.Vision.Models.conv_final","title":"conv_final","text":""},{"doctype":"documentation","id":"references/FastAI.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.TESTSET_MACROS","title":"TESTSET_MACROS","text":""},{"doctype":"documentation","id":"references/FluxTraining.CustomCallback","title":"CustomCallback","text":" CustomCallback(f, Event, [TPhase = Phase, access = (;)])\n A callback that runs  f(learner)  every time an event of type  Event during a phase of type in  Phase . If  f  needs to access learner state, pass  access , a named tuple in the same form as  stateaccess . Instead of using  CustomCallback  it is recommended to properly implement a  Callback . Examples We can get a quick idea of when a new epoch starts as follows: cb   =   CustomCallback ( learner   ->   println ( \" New epoch! \" ) ,   EpochBegin )"},{"doctype":"documentation","id":"references/DataAugmentation.onehot","title":"onehot","text":""},{"doctype":"document","id":"documents/docs/introduction.md","title":"Introduction","text":" Introduction This tutorial explains the qickstart examples and some core abstractions FastAI . jl is built on . using   FastAI \n import   FastAI :   Image On the  quickstart page , we showed how to train models on common tasks in a few lines of code like these: using   FastAI \n data ,   blocks   =   loaddataset ( \" imagenette2-160 \" ,   ( Image ,   Label ) ) \n task   =   ImageClassificationSingle ( blocks ) \n learner   =   tasklearner ( task ,   data ,   callbacks = [ ToGPU ( ) ] ) \n fitonecycle! ( learner ,   10 ) \n showoutputs ( task ,   learner ) Each of the five lines encapsulates one part of the deep learning pipeline to give a high-level API while still allowing customization. Let’s have a closer look. Dataset ENV [ \" DATADEPS_ALWAYS_ACCEPT \" ]   =   \" true \" data ,   blocks   =   loaddataset ( \" imagenette2-160 \" ,   ( Image ,   Label ) ) This line downloads and loads the  ImageNette  image classification dataset, a small subset of ImageNet with 10 different classes.  data  is a  data container  that can be used to load individual observations, here of images and the corresponding labels. We can use  getobs(data, i)  to load the  i -th observation and  nobs  to find out how many observations there are. image ,   class   =   sample   =    getobs ( data ,   1000 ) \n @ show   class \n image blocks  describe the format of the data that you want to use for learning. For supervised training tasks, they are a tuple of  (inputblock, targetblock) . Since we want to do image classification, the input block is  Image{2}() , representing a 2-dimensional image and the target block is  Label(classes) , representing the class the image belongs to. blocks Learning task task   =   ImageClassificationSingle ( blocks ) The next line defines a learning task which encapsulates the data preprocessing pipeline and other logic related to the task.  ImageClassificationSingle  is a simple wrapper around  BlockTask  which takes in blocks and data processing steps, so-called  encodings . Using it, we can replace the above line with task   =   BlockTask ( \n     ( Image { 2 } ( ) ,   Label ( classes ) ) , \n     ( \n         ProjectiveTransforms ( ( 128 ,   128 ) ) , \n         ImagePreprocessing ( ) , \n         OneHot ( ) \n     ) \n ) Based on the blocks and encodings, the learning task can derive lots of functionality: data processing visualization constructing task-specific models from a backbone creating a loss function Learner learner   =   tasklearner ( task ,   data ,   callbacks = [ ToGPU ( ) ,   Metrics ( accuracy ) ] ) Next we create a  Learner  that encapsulates everything needed for training, including: parallelized training and validation data loaders using  taskdataloaders a loss function using  tasklossfn a task-specific model using  taskmodel The customizable, expanded version of the code looks like this: dls   =   taskdataloaders ( data ,   task ) \n model   =   taskmodel ( task ,   Models . xresnet18 ( ) ) \n lossfn   =   tasklossfn ( task ) \n learner   =   Learner ( model ,   dls ,   ADAM ( ) ,   lossfn ,   ToGPU ( ) ,   Metrics ( accuracy ) ) At this step, we can also pass in any number of  callbacks  to customize the training. Here  ToGPU  ensures an available GPU is used, and  Metrics  adds additional metrics to track during training. Training fitonecycle! ( learner ,   10 ) Training now is quite simple. You have several options for high-level training schedules: lrfind  to run a learning rate finder finetune!  for when you’re using a pretrained backbone fitonecycle!  for when you’re training a model from scratch Visualization showoutputs ( task ,   learner ) Finally, the last line visualizes the predictions of the trained model. It takes some samples from the training data loader, runs them through the model and decodes the outputs. How each piece of data is visualized is also inferred through the blocks in the learning task."},{"doctype":"documentation","id":"references/Flux.Optimise.Nesterov","title":"Nesterov","text":" Nesterov(η = 0.001, ρ = 0.9)\n Gradient descent optimizer with learning rate  η  and Nesterov momentum  ρ . Parameters Learning rate ( η ): Amount by which gradients are discounted before updating the weights. Nesterov momentum ( ρ ): Controls the acceleration of gradient descent in the prominent direction, in effect damping oscillations. Examples opt   =   Nesterov ( ) \n \n opt   =   Nesterov ( 0.003 ,   0.95 )"},{"doctype":"documentation","id":"references/DataAugmentation.ProjectiveTransform","title":"ProjectiveTransform","text":" abstract type ProjectiveTransform <: Transform\n Abstract supertype for projective transformations. See Projective transformations ."},{"doctype":"documentation","id":"references/FastAI.showblock","title":"showblock","text":" showblock([backend], block, obs)\nshowblock([backend], blocks, obss)\nshowblock([backend], title => block, obs)\n Show a block or blocks of obs to  backend <: ShowBackend . block  can be a  Block , a tuple of  block s, or a  Pair  of  title => block ."},{"doctype":"documentation","id":"references/Flux.orthogonal","title":"orthogonal","text":" orthogonal([rng=GLOBAL_RNG], size...; gain = 1) -> Array\northogonal([rng]; kw...) -> Function\n Return an  Array{Float32}  of the given  size  which is a (semi) orthogonal matrix, as described in [1]. Cannot construct a vector, i.e.  length(size) == 1  is forbidden. For  length(size) > 2 , a  prod(size[1:(end - 1)])  by  size[end]  orthogonal matrix is computed before reshaping it to the original dimensions. Examples julia> W = Flux.orthogonal(5, 7);\n\njulia> summary(W)\n\"5×7 Matrix{Float32}\"\n\njulia> W * W' ≈ I(5)\ntrue\n\njulia> W2 = Flux.orthogonal(7, 5);\n\njulia> W2 * W2' ≈ I(7)\nfalse\n\njulia> W2' * W2 ≈ I(5)\ntrue\n\njulia> W3 = Flux.orthogonal(3, 3, 2, 4);\n\njulia> transpose(reshape(W3, :, 4)) * reshape(W3, :, 4) ≈ I(4)\ntrue\n References [1] Saxe, McClelland, Ganguli. “Exact solutions to the nonlinear dynamics of learning in deep linear neural networks”, ICLR 2014, https://arxiv.org/abs/1312.6120"},{"doctype":"documentation","id":"references/FastAI.Tabular.TableDatasetRecipe","title":"TableDatasetRecipe","text":" TableDatasetRecipe(tablefile; catcols, contcols, kwargs...])\n Recipe for loading a  TableDataset .  tablefile  is the path of a file that can be read as a table.  catcols  and  contcols  indicate the categorical and continuous columns of the table. If they are not given, they are detected automatically."},{"doctype":"documentation","id":"references/Flux.glorot_normal","title":"glorot_normal","text":" glorot_normal([rng=GLOBAL_RNG], size...; gain = 1) -> Array\nglorot_normal([rng]; kw...) -> Function\n Return an  Array{Float32}  of the given  size  containing random numbers drawn from a normal distribution with standard deviation  gain * sqrt(2 / (fan_in + fan_out)) , using [ nfan ](  Flux.nfan). This method is described in [1] and also known as Xavier initialization. Examples julia> using Statistics\n\njulia> round(std(Flux.glorot_normal(10, 1000)), digits=3)\n0.044f0\n\njulia> round(std(Flux.glorot_normal(1000, 10)), digits=3)\n0.044f0\n\njulia> round(std(Flux.glorot_normal(1000, 1000)), digits=3)\n0.032f0\n\njulia> Dense(10 => 1000, tanh; init = Flux.glorot_normal(gain=100))\nDense(10 => 1000, tanh)  # 11_000 parameters\n\njulia> round(std(ans.weight), sigdigits=3)\n4.45f0\n References [1] Glorot, Xavier, and Yoshua Bengio. “Understanding the difficulty of training deep feedforward neural networks.”  Proceedings of the thirteenth international conference on artificial intelligence and statistics . 2010."},{"doctype":"documentation","id":"references/DataAugmentation.allequal","title":"allequal","text":""},{"doctype":"documentation","id":"references/Flux.epseltype","title":"epseltype","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.@epochs","title":"@epochs","text":" @epochs N body\n Run  body   N  times. Mainly useful for quickly doing multiple epochs of training in a REPL. Examples julia> Flux.@epochs 2 println(\"hello\")\n[ Info: Epoch 1\nhello\n[ Info: Epoch 2\nhello\n"},{"doctype":"documentation","id":"references/FastAI.Vision.Models.make_layer","title":"make_layer","text":""},{"doctype":"documentation","id":"references/Flux._show_layers","title":"_show_layers","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Maybe","title":"Maybe","text":" Maybe(tfm, p = 0.5) <: Transform\n With probability  p , apply transformation  tfm ."},{"doctype":"documentation","id":"references/FastAI.savemethodmodel","title":"savemethodmodel","text":""},{"doctype":"documentation","id":"references/FastAI.encodingscolumn","title":"encodingscolumn","text":""},{"doctype":"documentation","id":"references/Flux.onehotbatch","title":"onehotbatch","text":" onehotbatch(xs, labels, [default])\n Returns a  OneHotMatrix  where  k th column of the matrix is [ onehot(xs[k], labels) ](  onehot). This is a sparse matrix, which stores just a  Vector{UInt32}  containing the indices of the nonzero elements. If one of the inputs in  xs  is not found in  labels , that column is  onehot(default, labels) if  default  is given, else an error. If  xs  has more dimensions,  M = ndims(xs) > 1 , then the result is an AbstractArray{Bool, M+1}  which is one-hot along the first dimension, i.e.  result[:, k...] == onehot(xs[k...], labels) . Note that  xs  can be any iterable, such as a string. And that using a tuple for  labels  will often speed up construction, certainly for less than 32 classes. Examples julia> oh = Flux.onehotbatch(\"abracadabra\", 'a':'e', 'e')\n5×11 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  ⋅  ⋅  1  ⋅  1  ⋅  1  ⋅  ⋅  1\n ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅\n ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅\n ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅\n\njulia> reshape(1:15, 3, 5) * oh  # this matrix multiplication is done efficiently\n3×11 Matrix{Int64}:\n 1  4  13  1  7  1  10  1  4  13  1\n 2  5  14  2  8  2  11  2  5  14  2\n 3  6  15  3  9  3  12  3  6  15  3\n"},{"doctype":"documentation","id":"references/Flux.OneHotVector","title":"OneHotVector","text":""},{"doctype":"documentation","id":"references/Flux.extraChain","title":"extraChain","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Identity","title":"Identity","text":" Identity()\n The identity transformation."},{"doctype":"documentation","id":"references/Flux.hasaffine","title":"hasaffine","text":" hasaffine(l) Return  true  if a normalisation layer has trainable shift and scale parameters,  false  otherwise. See  BatchNorm ,  InstanceNorm ,  GroupNorm , and  LayerNorm ."},{"doctype":"documentation","id":"references/FastAI.blockbackbone","title":"blockbackbone","text":" blockbackbone(inblock)\n Create a default backbone that takes in block  inblock ."},{"doctype":"documentation","id":"references/FastAI.Vision.Models.iterlayers","title":"iterlayers","text":""},{"doctype":"documentation","id":"references/FastAI.ShowBackend","title":"ShowBackend","text":" abstract type ShowBackend\n Abstract type for backends that allow showing blocks of data in an interpretable way. Extending For a  ShowBackend   Backend , you should implement the following methods: createhandle (::Backend)  creates a context that blocks of data can be shown to showblock! (handle, ::Backend, block::B, obs)  shows a block of type  B . This needs to be implemented for every block type you want to be able to show showblocks! (handle, ::Backend, blocks, obss)  shows a collection of blocks"},{"doctype":"documentation","id":"references/Flux.Losses.binary_focal_loss","title":"binary_focal_loss","text":" binary_focal_loss(ŷ, y; agg=mean, γ=2, ϵ=eps(ŷ))\n Return the  binary _ focal _ loss The input, ‘ŷ’, is expected to be normalized (i.e.  softmax  output). For  γ == 0 , the loss is mathematically equivalent to  Losses.binarycrossentropy . Example julia> y = [0  1  0\n            1  0  1]\n2×3 Matrix{Int64}:\n 0  1  0\n 1  0  1\n\njulia> ŷ = [0.268941  0.5  0.268941\n            0.731059  0.5  0.731059]\n2×3 Matrix{Float64}:\n 0.268941  0.5  0.268941\n 0.731059  0.5  0.731059\n\njulia> Flux.binary_focal_loss(ŷ, y) ≈ 0.0728675615927385\ntrue\n See also:  Losses.focal_loss  for multi-class setting"},{"doctype":"documentation","id":"references/Flux.paramtype","title":"paramtype","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.ImageSegmentation","title":"ImageSegmentation","text":" ImageSegmentation(size, classes; kwargs...)\n Learning task for image segmentation. Images are resized to  size  and a class is predicted for every pixel. Keyword arguments computestats = false : Whether to compute image statistics on dataset  data  or use default ImageNet stats. aug_projections =  DataAugmentation.Identity : augmentation to apply during ProjectiveTransforms  (resizing and cropping) aug_image =  DataAugmentation.Identity : pixel-level augmentation to apply during ImagePreprocessing C = RGB{N0f8} : Color type images are converted to before further processing. Use  Gray{N0f8} for grayscale images."},{"doctype":"documentation","id":"references/Flux.LSTM","title":"LSTM","text":" LSTM(in => out)\n Long Short Term Memory recurrent layer. Behaves like an RNN but generally exhibits a longer memory span over sequences. The arguments  in  and  out  describe the size of the feature vectors passed as input and as output. That is, it accepts a vector of length  in  or a batch of vectors represented as a  in x B  matrix and outputs a vector of length  out  or a batch of vectors of size  out x B . This constructor is syntactic sugar for  Recur(LSTMCell(a...)) , and so LSTMs are stateful. Note that the state shape can change depending on the inputs, and so it is good to  reset!  the model between inference calls if the batch size changes. See the examples below. See  this article for a good overview of the internals. Examples julia> l = LSTM(3 => 5)\nRecur(\n  LSTMCell(3 => 5),                     # 190 parameters\n)         # Total: 5 trainable arrays, 190 parameters,\n          # plus 2 non-trainable, 10 parameters, summarysize 1.062 KiB.\n\njulia> l(rand(Float32, 3)) |> size\n(5,)\n\njulia> Flux.reset!(l);\n\njulia> l(rand(Float32, 3, 10)) |> size # batch size of 10\n(5, 10)\n Batch size changes Failing to call  reset!  when the input batch size changes can lead to unexpected behavior. See the example in  RNN ."},{"doctype":"documentation","id":"references/FastAI.showprediction","title":"showprediction","text":" showprediction([backend], task, pred)\nshowprediction([backend], task, sample, pred)\n Show a prediction  pred . If a  sample  is also given, show it next to the prediction. ŷ"},{"doctype":"documentation","id":"references/FastAI.Vision.Models.UNetCombineLayer","title":"UNetCombineLayer","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.Models.PixelShuffleICNR","title":"PixelShuffleICNR","text":""},{"doctype":"documentation","id":"references/Flux._print_conv_opt","title":"_print_conv_opt","text":""},{"doctype":"document","id":"documents/docs/discovery.md","title":"Discovery","text":" Discovery As you may have seen in  the introduction , FastAI.jl makes it possible to train models in just 5 lines of code. However, if you have a task in mind, you need to know what datasets you can train on and if there are convenience learning task constructors. For example, the introduction loads the  \"imagenette2-160\"  dataset and uses  ImageClassificationSingle  to construct a learning task. Now what if, instead of classifying an image into one class, we want to classify every single pixel into a class (semantic segmentation)? Now we need a dataset with pixel-level annotations and a learning task that can process those segmentation masks. For finding both, we can make use of  Block s. A  Block  represents a kind of data, for example images, labels or keypoints. For supervised learning tasks, we have an input block and a target block. If we wanted to classify whether 2D images contain a cat or a dog, we could use the blocks  (Image{2}(), Label([\"cat\", \"dog\"])) , while for semantic segmentation, we’ll have an input  Image  block and a target  Mask  block. Finding a dataset To find a dataset with compatible samples, we can pass the types of these blocks to  finddatasets  which will return a list of dataset names and recipes to load them in a suitable way. using   FastAI \n import   FastAI :   Image \n finddatasets ( blocks = ( Image ,   Mask ) ) We can see that the  \"camvid_tiny\"  dataset can be loaded so that each sample is a pair of an image and a segmentation mask. Let’s use  loaddataset  to load a  data container  and concrete blocks. ENV [ \" DATADEPS_ALWAYS_ACCEPT \" ]   =   \" true \" data ,   blocks   =   loaddataset ( \" camvid_tiny \" ,   ( Image ,   Mask ) ) As with every data container, we can load a sample using  getobs  which gives us a tuple of an image and a segmentation mask. image ,   mask   =   sample   =   getobs ( data ,   1 ) \n size . ( sample ) ,   eltype . ( sample ) loaddataset  also returned  blocks  which are the concrete  Block  instances for the dataset. We passed in  types  of blocks ( (Image, Mask) ) and get back  instances  since the specifics of some blocks depend on the dataset. For example, the returned target block carries the labels for every class that a pixel can belong to. inputblock ,   targetblock   =   blocks \n targetblock With these  blocks , we can also validate a sample of data using  checkblock  which is useful as a sanity check when using custom data containers. checkblock ( ( inputblock ,   targetblock ) ,   ( image ,   mask ) ) Summary In short, if you have a learning task in mind and want to load a dataset for that task, then define the types of input and target block, e.g.  blocktypes = (Image, Label) , use  finddatasets (blocks=blocktypes)  to find compatbile datasets; and run  loaddataset (datasetname, blocktypes)  to load a data container and the concrete blocks Exercises Find and load a dataset for multi-label image classification. (Hint: the block for multi-category outputs is called  LabelMulti ). List all datasets with  Image  as input block and any target block. (Hint: the supertype of all types is  Any ) Finding a learning task Armed with a dataset, we can go to the next step: creating a learning task. Since we already have blocks defined, this amounts to defining the encodings that are applied to the data before it is used in training. Here, FastAI.jl already defines some convenient constructors for learning tasks and you can find them with  findlearningtasks . Here we can pass in either block types as above or the block instances we got from  loaddataset . findlearningtasks ( blocks ) Looks like we can use the  ImageSegmentation  function to create a learning task for our learning task. Every function returned can be called with  blocks  and, optionally, some keyword arguments for customization. task   =   ImageSegmentation ( blocks ;   size   =   ( 64 ,   64 ) ) And that’s the basic workflow for getting started with a supervised task. Exercises Find all learning task functions with images as inputs."},{"doctype":"documentation","id":"references/FluxTraining.runtests","title":"runtests","text":""},{"doctype":"documentation","id":"references/DataAugmentation.getbounds","title":"getbounds","text":" getbounds(item)\n Return the spatial bounds of  item . For a 2D-image ( Image{2} ) the bounds are the 4 corners of the bounding rectangle. In general, for an N-dimensional item, the bounds are a vector of the N^2 corners of the N-dimensional hypercube bounding the data."},{"doctype":"documentation","id":"references/Flux.patience","title":"patience","text":" patience(predicate, wait)\n Return a function that internally counts by one when predicate(...) == true , otherwise the count is reset to zero. If the count is greater than or equal to  wait , the function returns  true , otherwise it returns  false . Examples julia> loss() = rand();\n\njulia> trigger = Flux.patience(() -> loss() < 1, 3);\n\n\njulia> Flux.@epochs 10 begin\n         trigger() && break\n       end\n[ Info: Epoch 1\n[ Info: Epoch 2\n[ Info: Epoch 3\n"},{"doctype":"documentation","id":"references/Flux.zeros","title":"zeros","text":""},{"doctype":"documentation","id":"references/FastAI.IndexGrouper","title":"IndexGrouper","text":""},{"doctype":"documentation","id":"references/FastAI.taskdataset","title":"taskdataset","text":" taskdataset(data, task, context)\n Transform data container  data  of samples into a data container of  (x, y) -pairs. Maps  encodesample(task, context, sample)  over the observations in  data ."},{"doctype":"documentation","id":"references/DataAugmentation.project!","title":"project!","text":" project!(bufitem, P, item, indices)\n Project  item  using projection  P  and crop to  indices  if given. Store result in  bufitem . Inplace version of  project . Default implementation falls back to  project ."},{"doctype":"documentation","id":"references/Flux.expand","title":"expand","text":""},{"doctype":"documentation","id":"references/Flux.Losses.kldivergence","title":"kldivergence","text":" kldivergence(ŷ, y; agg = mean, ϵ = eps(ŷ))\n Return the Kullback - Leibler divergence between the given probability distributions. The KL divergence is a measure of how much one probability distribution is different from the other. It is always non-negative, and zero only when both the distributions are equal. Example julia> p1 = [1 0; 0 1]\n2×2 Matrix{Int64}:\n 1  0\n 0  1\n\njulia> p2 = fill(0.5, 2, 2)\n2×2 Matrix{Float64}:\n 0.5  0.5\n 0.5  0.5\n\njulia> Flux.kldivergence(p2, p1) ≈ log(2)\ntrue\n\njulia> Flux.kldivergence(p2, p1; agg = sum) ≈ 2log(2)\ntrue\n\njulia> Flux.kldivergence(p2, p2; ϵ = 0)  # about -2e-16 with the regulator\n0.0\n\njulia> Flux.kldivergence(p1, p2; ϵ = 0)  # about 17.3 with the regulator\nInf\n"},{"doctype":"documentation","id":"references/Flux.AdaptiveMeanPool","title":"AdaptiveMeanPool","text":" AdaptiveMeanPool(out::NTuple)\n Adaptive mean pooling layer. Calculates the necessary window size such that its output has  size(y)[1:N] == out . Expects as input an array with  ndims(x) == N+2 , i.e. channel and batch dimensions, after the  N  feature dimensions, where  N = length(out) . See also  MaxPool ,  AdaptiveMaxPool . Examples julia> xs = rand(Float32, 100, 100, 3, 50);  # batch of 50 RGB images\n\njulia> AdaptiveMeanPool((25, 25))(xs) |> size\n(25, 25, 3, 50)\n\njulia> MeanPool((4,4))(xs) ≈ AdaptiveMeanPool((25, 25))(xs)\ntrue\n"},{"doctype":"documentation","id":"references/FastAI.ParamGrouper","title":"ParamGrouper","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.WeightDecay","title":"WeightDecay","text":" WeightDecay(λ = 0)\n Decay weights by  λ . Typically composed  with other optimizers as the first transformation to the gradient, making it equivalent to adding  L_2  regularization with coefficient   λ  to the loss. Examples opt   =   Optimiser ( WeightDecay ( 1f-4 ) ,   ADAM ( ) )"},{"doctype":"documentation","id":"references/FluxTraining.ES","title":"ES","text":""},{"doctype":"documentation","id":"references/FluxTraining.runafter","title":"runafter","text":""},{"doctype":"documentation","id":"references/FluxTraining.MetricsPrinter","title":"MetricsPrinter","text":" MetricsPrinter() <: Callback\n Callback that prints metrics after every epoch. Relies on the metrics computed by Metrics , so will error if no  Metrics  callback is used. This callback is added by default to every  Learner  unless you pass in usedefaultcallbacks = false ."},{"doctype":"documentation","id":"references/FastAI.Vision.Models.UNetFinalBlock","title":"UNetFinalBlock","text":""},{"doctype":"documentation","id":"references/FluxTraining.ConditionalCallback","title":"ConditionalCallback","text":" ConditionalCallback(callback, condition) <: Callback\n Wrapper callback that only forwards events to the wrapped callback if  CallbackCondition   condition  is met. See  throttle ."},{"doctype":"documentation","id":"references/FastAI.Vision.Models","title":"Models","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.RMSProp","title":"RMSProp","text":" RMSProp(η = 0.001, ρ = 0.9, ϵ = \n"},{"doctype":"documentation","id":"references/Flux.istraining","title":"istraining","text":""},{"doctype":"documentation","id":"references/DataAugmentation.mask_extrapolation","title":"mask_extrapolation","text":""},{"doctype":"documentation","id":"references/DataAugmentation.transformbounds","title":"transformbounds","text":" transformbounds(bounds, P)\n Apply  CoordinateTransformations.Transformation  to  bounds ."},{"doctype":"documentation","id":"references/Flux.Scale","title":"Scale","text":" Scale(size::Integer..., σ=identity; bias=true, init=ones32)\nScale(scale::AbstractArray, [bias, σ])\n Create an element-wise layer, whose forward pass is given by: y = σ.(scale .* x .+ bias)\n This uses  .*  instead of matrix multiplication  *  of  Dense . The learnable scale & bias are initialised  init(size...)  and  zeros32(size...) , with  init=ones32  by default. You may specify the function  init , turn off trainable bias with  bias=false , or provide the array(s) explicitly. Used by  LayerNorm  with  affine=true . Examples julia> a = Flux.Scale(2)\nScale(2)            # 4 parameters\n\njulia> Flux.params(a)\nParams([Float32[1.0, 1.0], Float32[0.0, 0.0]])\n\njulia> a([1 2 3])\n2×3 Matrix{Float32}:\n 1.0  2.0  3.0\n 1.0  2.0  3.0\n\njulia> b = Flux.Scale([1 2 3 4], false, abs2)\nScale(1, 4, abs2; bias=false)  # 4 parameters\n\njulia> b([1, 10])\n2×4 Matrix{Int64}:\n   1    4    9    16\n 100  400  900  1600\n\njulia> Flux.params(b)\nParams([[1 2 3 4]])\n"},{"doctype":"documentation","id":"references/FastAI.methodmodel","title":"methodmodel","text":""},{"doctype":"documentation","id":"references/FastAI.Tabular.gettransforms","title":"gettransforms","text":" gettransforms(td::Datasets.TableDataset)\n Returns a composition of basic tabular transformations constructed for the given TableDataset."},{"doctype":"documentation","id":"references/FastAI.PropagateNever","title":"PropagateNever","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.grandparentname","title":"grandparentname","text":""},{"doctype":"documentation","id":"references/DataAugmentation.ScaleRatio","title":"ScaleRatio","text":""},{"doctype":"documentation","id":"references/FastAI.finetune!","title":"finetune!","text":" finetune!(learner, nepochs[, base_lr = 0.002; kwargs...])\n Behaves like the fastai implementation fastai.Learner.fine_tune . Keyword arguments freezeepochs = 1 : Number of epochs to train with the backbone completely frozen. grouper = FastAI.defaultgrouper(learner.model) :  ParamGrouper  which assigns groups  1  (backbone) or  2  (head) for every parameter in  learner.model . The default expects  learner.model  to be a  Chain(backbone, head) . backbone_factor = 0.1 : Factor by which updates to backbone model are discounted during the second phase of training. Any additional keyword arguments are passed to  fitonecycle! ."},{"doctype":"documentation","id":"references/FastAI.Context","title":"Context","text":" abstract type Context\n Represents a context in which a data transformation is made. This allows using dispatching for varying behavior, for example, to apply augmentations only during training or use non-destructive cropping during inference. See  Training ,  Validation  and Inference ."},{"doctype":"documentation","id":"references/DataAugmentation.ComposedProjectiveTransform","title":"ComposedProjectiveTransform","text":" ComposedProjectiveTransform(tfms...)\n Wrap multiple projective  tfms  and apply them efficiently. The projections are fused into a single projection and only points inside the final crop are evaluated."},{"doctype":"documentation","id":"references/DataAugmentation.showbounds!","title":"showbounds!","text":""},{"doctype":"documentation","id":"references/FluxTraining.EarlyStopping","title":"EarlyStopping","text":" EarlyStopping(criteria...; kwargs...)\nEarlyStopping(n)\n Stop training early when  criteria  are met. See EarlyStopping . jl  for available stopping criteria. Passing an integer  n  uses the simple patience criterion: stop if the validation loss hasn’t increased for  n  epochs. You can control which phases are taken to measure the out-of-sample loss and the training loss with keyword arguments  trainphase  (default AbstractTrainingPhase ) and  testphase  (default  AbstractValidationPhase ). Examples Learner ( model ,   data ,   optimizer ,   lossfn ,   EarlyStopping ( 3 ) ) import   FluxTraining . ES :   Disjunction ,   InvalidValue ,   TimeLimit \n \n callback   =   EarlyStopping ( Disjunction ( InvalidValue ( ) ,   TimeLimit ( 0.5 ) ) ) \n Learner ( model ,   data ,   optimizer ,   lossfn ,   callback )"},{"doctype":"document","id":"documents/notebooks/imagesegmentation.ipynb","title":"Image segmentation","text":" Image segmentation In the  quickstart section , you saw a short example of how to train an image segmentation model .  In this tutorial, we ’ ll recreate that using the mid - level APIs . using   FastAI ,   Metalhead \n import   CairoMakie ;   CairoMakie . activate! ( type = \" png \" ) In image segmentation, instead of assigning a class to a whole image as in  image classification , we want to classify each pixel of an image. We’ll use the CamVid dataset which contains street images with every pixel annotated as one of 32 classes. Inside the dataset folder, we find  images/ , a subfolder with all the input images,  labels/ , a subfolder containing the segmentation masks (saved as images) and  codes.txt  which contains the class names: dir   =   datasetpath ( \" camvid_tiny \" ) \n readdir ( dir ) Every line in  codes.txt  corresponds to one class, so let’s load it: classes   =   readlines ( open ( joinpath ( dir ,   \" codes.txt \" ) ) ) To create our data containers we’ll use  loadfolderdata , which creates data containers from folder contents. The function  loadfn  is applied to the file paths and we use  loadfile  to  loadmask  to get images and masks. images   =   Datasets . loadfolderdata ( \n     joinpath ( dir ,   \" images \" ) , \n     filterfn = isimagefile , \n     loadfn = loadfile ) \n \n masks   =   Datasets . loadfolderdata ( \n     joinpath ( dir ,   \" labels \" ) , \n     filterfn = isimagefile , \n     loadfn = f   ->   loadmask ( f ,   classes ) ) \n \n data   =   ( images ,   masks ) Now we can get an observation: image ,   mask   =   sample   =   getobs ( data ,   1 ) ; image Each mask is an array with elements the same type as our classes (here  String ), but efficiently stored using integers under the hood. view ( mask ,   50 : 55 ,   50 : 55 ) Next we need to create a learning task for image segmentation. This means using images to predict masks, so we’ll use the  Image  and  Mask  blocks as input and target. Since the dataset is 2D, we’ll use 2-dimensional blocks. task   =   BlockTask ( \n     ( Image { 2 } ( ) ,   Mask { 2 } ( classes ) ) , \n     ( \n         ProjectiveTransforms ( ( 128 ,   128 ) ) , \n         ImagePreprocessing ( ) , \n         OneHot ( ) \n     ) \n ) The encodings passed in transform samples into formats suitable as inputs and outputs for a model, and also allow decoding model outputs to get back to our target format: an array of class labels for every pixel. Let’s check that samples from the created data container conform to the blocks of the learning task: checkblock ( task . blocks ,   sample ) We can ascertain the encodings work as expected by creating a batch of encoded data and visualizing it. Here the segmentation masks are color-coded and overlayed on top of the image. xs ,   ys   =   FastAI . makebatch ( task ,   data ,   1 : 3 ) \n showbatch ( task ,   ( xs ,   ys ) ) We can use  describetask  to get more information about learning tasks created through the data block API. We see which representations our data goes through and which encodings transform which parts. describetask ( task ) With a  task  and a matching data container, the only thing we need before we can create a  Learner  is a backbone architecture to build the segmentation model from. We’ll use a slightly modified ResNet, but you can use any convolutional architecture. We’ll use  taskmodel  to construct the model from the backbone. Since we want mask outputs, the intermediate feature representation needs to be scaled back up. Based on the  Block s we built our task from,  taskmodel  knows that it needs to build a mapping  ImageTensor{2} -> OneHotTensor{2}  and constructs a U-Net model. backbone   =   Models . xresnet18 ( ) \n model   =   taskmodel ( task ,   backbone ) ; In a similar vein,  tasklossfn  creates a loss function suitable for comparing model outputs and encoded targets. lossfn   =   tasklossfn ( task ) Next we turn our data container into training and validation data loaders that will iterate over batches of encoded data and construct a  Learner . traindl ,   validdl   =   taskdataloaders ( data ,   task ,   16 ) \n optimizer   =   ADAM ( ) \n learner   =   Learner ( model ,   ( traindl ,   validdl ) ,   optimizer ,   lossfn ,   ToGPU ( ) ) Note that we could also have used  tasklearner  which is a shorthand that calls  taskdataloaders  and  taskmodel  for us. Now let’s finetune the model: fitonecycle! ( learner ,   10 ,   0.033 ) And look at the results on a batch of validation data: showoutputs ( task ,   learner ;   n   =   4 ) The data block API can also be used for 3D segmentation, but the tutorial for that is still in the works."},{"doctype":"documentation","id":"references/DataAugmentation.setwrapped","title":"setwrapped","text":""},{"doctype":"documentation","id":"references/FluxTraining.Write","title":"Write","text":""},{"doctype":"documentation","id":"references/FluxTraining.callbackgraph","title":"callbackgraph","text":" callbackgraph(callbacks) -> SimpleDiGraph\n Creates a directed acyclic graph from a list of  callbacks . Ordering is given through  runafter  and  resolveconflict . If a write conflict cannot be resolved (i.e.  resolveconflict ) is not implemented), throws an error."},{"doctype":"documentation","id":"references/DataAugmentation.PinOrigin","title":"PinOrigin","text":" PinOrigin()\n Projective transformation that translates the data so that the upper left bounding corner is at the origin  (0, 0)  (or the multidimensional equivalent). Projective transformations on images return  OffsetArray s, but not on keypoints. Hardware like GPUs do not support OffsetArrays, so they will be unwrapped and no longer match up with the keypoints. Pinning the data to the origin makes sure that the resulting OffsetArray  has the same indices as a regular array, starting at one."},{"doctype":"documentation","id":"references/FluxTraining.TensorBoardBackend","title":"TensorBoardBackend","text":" TensorBoardBackend(logdir[, tb_overwrite];\n    time=time(),\n    purge_step=nothing,\n    step_increment=1,\n    min_level=Logging.Info)\n TensorBoard backend for logging callbacks. Takes the same arguments as  TensorBoardLogger.TBLogger ."},{"doctype":"documentation","id":"references/FluxTraining.garbagecollect","title":"garbagecollect","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.KeypointTensor","title":"KeypointTensor","text":" KeypointTensor{N, T, M} <: Block\n Block for encoded  Keypoints {N, T, M}  returned by KeypointPreprocessing ."},{"doctype":"documentation","id":"references/DataAugmentation.Categorify","title":"Categorify","text":" Categorify(dict, cols)\n Label encodes the values of a row present in  TabularItem  for the columns specified in  cols  using  dict , which contains the column names as dictionary keys and the unique values of column present as dictionary values. if there are any  missing  values in the values to be transformed, they are replaced by 1. Example using   DataAugmentation \n \n cols   =   [ : col1 ,   : col2 ,   : col3 ] \n row   =   ( ;   zip ( cols ,   [ \" cat \" ,   2 ,   3 ] ) ... ) \n item   =   TabularItem ( row ,   cols ) \n catdict   =   Dict ( : col1   =>   [ \" dog \" ,   \" cat \" ] ) \n \n tfm   =   Categorify ( catdict ,   [ : col1 ] ) \n apply ( tfm ,   item )"},{"doctype":"documentation","id":"references/DataAugmentation.scaleprojection","title":"scaleprojection","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.StopException","title":"StopException","text":""},{"doctype":"documentation","id":"references/FastAI.Vision._gettfm","title":"_gettfm","text":""},{"doctype":"documentation","id":"references/DataAugmentation.showbounds","title":"showbounds","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Keypoints","title":"Keypoints","text":" Keypoints(points, sz)\nKeypoints{N, T, M}(points, bounds)\n N -dimensional keypoints represented as  SVector{N, T} . Spatial bounds are given by the polygon  bounds::Vector{SVector{N, T}} or  sz::NTuple{N, Int} . Examples using   DataAugmentation ,   StaticArrays \n points   =   [ SVector ( y ,   x )   for   ( y ,   x )   in   zip ( 4 : 5 : 80 ,   10 : 6 : 90 ) ] \n item   =   Keypoints ( points ,   ( 100 ,   100 ) ) showitems ( item )"},{"doctype":"documentation","id":"references/FastAI.showoutputs","title":"showoutputs","text":" showoutputs(task, learner[; n = 4, context = Validation()])\n Run a trained model in  learner  on  n  samples and visualize the outputs. showoutputs([backend], task, outputs)\nshowoutputs([backend], task, encsamples, outputs)\n Show model outputs to  backend . If a vector of encoded samples  encsamples  is also given, show them next to the outputs. Use  showoutputbatch  to show collated batches of outputs."},{"doctype":"documentation","id":"references/Flux.GRUv3Cell","title":"GRUv3Cell","text":""},{"doctype":"documentation","id":"references/FastAI.checktask_core","title":"checktask_core","text":" checktask_core(task, sample, model; device = identity)\nchecktask_core(task; device = identity)\n Check if  task  conforms to the  core interface . sample  and  model  are used for testing. If you have implemented the testing interface and don’t supply these as arguments,  mocksample(task)  and mockmodel(task)  will be used."},{"doctype":"documentation","id":"references/FastAI.Vision.Models.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.TESTSET_MACROS","title":"TESTSET_MACROS","text":""},{"doctype":"documentation","id":"references/FastAI.BlockTask","title":"BlockTask","text":" BlockTask(blocks, encodings)\n Create an  AbstractBlockTask  directly, passing in a named tuple  blocks and  encodings . See  SupervisedTask  for supervised training tasks."},{"doctype":"documentation","id":"references/FastAI.Vision.Models.xresnet18","title":"xresnet18","text":""},{"doctype":"documentation","id":"references/Flux.Losses.add_blanks","title":"add_blanks","text":" add_blanks(z) Adds blanks to the start and end of  z , and between items in  z"},{"doctype":"documentation","id":"references/FluxTraining.Events.LossBegin","title":"LossBegin","text":" LossBegin()\n Event  called between calculating  y_pred  and calculating loss"},{"doctype":"documentation","id":"references/FluxTraining.Recorder","title":"Recorder","text":" Recorder()\n Maintains a  History . It’s stored in  learner.cbstate.history ."},{"doctype":"documentation","id":"references/Flux._isbitsarray","title":"_isbitsarray","text":""},{"doctype":"documentation","id":"references/FluxTraining.LogMetrics","title":"LogMetrics","text":" LogMetrics(backends...) <: Callback\n Callback that logs step and epoch metrics to one or more  LoggerBackend s. See also  LoggerBackend ,  Loggables.Loggable ,  log_to , TensorBoardBackend Example: logcb   =   LogMetrics ( TensorBoardBackend ( \" tblogs \" ) ) \n Learner ( model ,   data ,   opt ,   lossfn ,   Metrics ( accuracy ) ,   logcb )"},{"doctype":"documentation","id":"references/DataAugmentation._round","title":"_round","text":""},{"doctype":"documentation","id":"references/Flux.Losses.xlogx","title":"xlogx","text":" xlogx(x)\n Return  x * log(x)  for  x ≥ 0 , handling  x == 0  by taking the limit from above, to get zero."},{"doctype":"documentation","id":"references/Flux.loadparams!","title":"loadparams!","text":""},{"doctype":"documentation","id":"references/Flux.create_bias","title":"create_bias","text":" create_bias(weights, bias, size...)\n Return a bias parameter for a layer, based on the value given to the constructor’s keyword  bias=bias . bias == true  creates a trainable array of the given size, of the same type as  weights , initialised to zero. bias == false  returns  false , which is understood by AD to be non-differentiable. bias::AbstractArray  uses the array provided, provided it has the correct size. It does not at present correct the  eltype  to match that of  weights ."},{"doctype":"documentation","id":"references/Flux.Losses.ctc_alpha","title":"ctc_alpha","text":""},{"doctype":"documentation","id":"references/DataAugmentation.MapElem","title":"MapElem","text":" MapElem(f)\n Applies  f  to every element in an [ AbstractArrayItem ]."},{"doctype":"documentation","id":"references/Flux.NilNumber.Nil","title":"Nil","text":" nil = Nil()\n Nil  is a singleton type with a single instance  nil . Unlike  Nothing  and  Missing  it is a number:  Nil <: Real <: Number ."},{"doctype":"documentation","id":"references/Flux.Bilinear","title":"Bilinear","text":" Bilinear((in1, in2) => out, σ=identity; bias=true, init=glorot_uniform)\nBilinear(W::AbstractArray, [bias, σ])\n Creates a layer which is fully connected between two inputs and the output, and otherwise similar to  Dense . Its output, given vectors  x  &  y , is another vector  z  with, for all  i ∈ 1:out : z[i] = σ(x' * W[i,:,:] * y + bias[i])\n If  x  and  y  are matrices, then each column of the output  z = B(x, y)  is of this form, with  B  the Bilinear layer. If the second input  y  is not given, it is taken to be equal to  x , i.e.  B(x) == B(x, x) The two inputs may also be provided as a tuple,  B((x, y)) == B(x, y) , which is accepted as the input to a  Chain . If the two input sizes are the same,  in1 == in2 , then you may write  Bilinear(in => out, σ) . The initialisation works as for  Dense  layer, with  W = init(out, in1, in2) . By default the bias vector is  zeros(Float32, out) , option  bias=false  will switch off trainable bias. Either of these may be provided explicitly. Examples julia> x, y = randn(Float32, 5, 32), randn(Float32, 5, 32);\n\njulia> B = Flux.Bilinear((5, 5) => 7)\nBilinear(5 => 7)    # 182 parameters\n\njulia> B(x) |> size  # interactions based on one input\n(7, 32)\n\njulia> B(x,y) == B((x,y))  # two inputs, may be given as a tuple\ntrue\n\njulia> sc = SkipConnection(\n                Chain(Dense(5 => 20, tanh), Dense(20 => 9, tanh)),\n                Flux.Bilinear((9, 5) => 3, bias=false),\n            );  # used as the recombinator, with skip as the second input\n\njulia> sc(x) |> size\n(3, 32)\n\njulia> Flux.Bilinear(rand(4,8,16), false, tanh)  # first dim of weight is the output\nBilinear((8, 16) => 4, tanh; bias=false)  # 512 parameters\n"},{"doctype":"documentation","id":"references/FastAI.findlearningmethods","title":"findlearningmethods","text":""},{"doctype":"documentation","id":"references/FluxTraining.CallbackRunner","title":"CallbackRunner","text":""},{"doctype":"documentation","id":"references/FastAI.methoddataloaders","title":"methoddataloaders","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.__init__","title":"__init__","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.ProjectiveTransforms","title":"ProjectiveTransforms","text":" ProjectiveTransforms(sz; [augmentations, buffered]) <: Encoding\n Encoding for spatial data that resizes blocks to a common size  sz  and applies projective augmentations. Encodes all spatial blocks, preserving the block type: Image{N}  ->  Image{N} Mask{N}  ->  Mask{N} Keypoints{N}  ->  Keypoints{N} The behavior differs based on the  context  of encoding: Training : Resizes the data so the smallest side equals a side length in  sz  while keeping the aspect ratio. Applies  augmentations . Crops a random  sz -sized portion of the data Validation : Resizes the data so the smallest side equals a side length in  sz  while keeping the aspect ratio. Crops a  sz -sized portion from the center. Inference : Resizes the data so the smallest side equals a side length in  sz  while keeping the aspect ratio. Note that in this context, the data does not have size  sz , since no cropping happens and aspect ratio is preserved. ProjectiveTransforms  is not limited to 2D data, and works on 3D data as well. Note, however, that some transformations in  augs_projection  (rotation, warping, flipping) are 2D only so  augs_projection  cannot be used for 3D data. Keyword arguments augmentations:: DataAugmentation.Transform  = Identity() : Projective augmentation to apply during training. See  augs_projection . buffered = true : Whether to use inplace transformations. Reduces memory usage. sharestate = true : Whether to use the same random state and bounds for all blocks in a sample"},{"doctype":"documentation","id":"references/FluxTraining.Loss","title":"Loss","text":""},{"doctype":"documentation","id":"references/FluxTraining.runepoch","title":"runepoch","text":" runepoch(epochfn, learner, phase)\n Run  epochfn  inside the context of an epoch. Calls  epochfn(handle) where  handle(e)  can be called to dispatch events. Takes care of dispatching  EpochBegin  and  EpochEnd events as well as handling  CancelEpochException s."},{"doctype":"documentation","id":"references/FastAI.Encoding","title":"Encoding","text":" abstract type Encoding\n Transformation of  Block s. Can encode some  Block s ([ encode ]), and optionally decode them [ decode ] Interface encode(::E, ::Context, block::Block, data)  encodes  block  of  data . The default is to do nothing. This should be overloaded for an encoding  E , concrete  Block  types and possibly a context. decode(::E, ::Context, block::Block, data)  decodes  block  of  data . This should correspond as closely as possible to the inverse of  encode(::E, ...) . The default is to do nothing, as not all encodings can be reversed. This should be overloaded for an encoding  E , concrete  Block  types and possibly a context. encodedblock(::E, block::Block) -> block'  returns the block that is obtained by encoding  block  with encoding  E . This needs to be constant for an instance of  E , so it cannot depend on the sample or on randomness. The default is to return  nothing , meaning the same block is returned and not changed. Encodings that return the same block but change the data (e.g.  ProjectiveTransforms ) should return  block . decodedblock(::E, block::Block) -> block'  returns the block that is obtained by decoding  block  with encoding  E . This needs to be constant for an instance of  E , so it cannot depend on the sample or on randomness. The default is to return  nothing , meaning the same block is returned and not changed. encode!(buf, ::E, ::Context, block::Block, data)  encodes  data  inplace. decode!(buf, ::E, ::Context, block::Block, data)  decodes  data  inplace."},{"doctype":"documentation","id":"references/DataAugmentation.FlipX","title":"FlipX","text":""},{"doctype":"documentation","id":"references/FastAI.predict","title":"predict","text":" predict(task, model, input[; device, context])\n Predict a  target  from  input  using  model . Optionally apply function  device to  x  before passing to  model  and use  context  instead of the default context  Inference ."},{"doctype":"documentation","id":"references/FastAI.lrfindtextplot","title":"lrfindtextplot","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.Models.pixelshuffle","title":"pixelshuffle","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.mockarray","title":"mockarray","text":""},{"doctype":"documentation","id":"references/FastAI.Vision._checksizedim","title":"_checksizedim","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.SkipException","title":"SkipException","text":""},{"doctype":"document","id":"documents/notebooks/quickstart.ipynb","title":"Quickstart","text":" Quickstart This page follows fastai ’ s  quickstart page  by quickly showing a few learning tasks .  More will be added here as they are added to the library . FastAI.jl’s learning tasks all use the same basic steps and code: create a  data container create a learning task create learner call a  fit  task make predictions or view results In this quick start, we’ll show these steps for a wide range of difference applications and datasets. As you’ll see, the code in each case is extremely similar, despite the very different models and data being used. import   CairoMakie \n using   FastAI Computer vision Classification Single - label data ,   blocks   =   loaddataset ( \" imagenette2-320 \" ,   ( Image ,   Label ) ) \n task   =   ImageClassificationSingle ( blocks ,   size = ( 256 ,   256 ) ) \n learner   =   tasklearner ( task ,   data ,   callbacks = [ ToGPU ( ) ,   Metrics ( accuracy ) ] ) \n fitonecycle! ( learner ,   5 ,   0.033 ) showoutputs ( task ,   learner ) Multi - label classification data ,   blocks   =   loaddataset ( \" pascal_2007 \" ,   ( Image ,   LabelMulti ) ) \n task   =   ImageClassificationMulti ( blocks ) \n learner   =   tasklearner ( task ,   data ,   callbacks = [ ToGPU ( ) ,   Metrics ( accuracy_thresh ) ] ) \n fitonecycle! ( learner ,   5 ,   0.033 ) showoutputs ( task ,   learner ) Segmentation data ,   blocks   =   loaddataset ( \" camvid_tiny \" ,   ( Image ,   Mask ) ) \n task   =   ImageSegmentation ( blocks ) \n learner   =   tasklearner ( task ,   data ,   callbacks = [ ToGPU ( ) ] ) \n fitonecycle! ( learner ,   10 ,   0.1 ) showoutputs ( task ,   learner ) Tabular data Classification data ,   blocks   =   loaddataset ( \" adult_sample \" ,   ( TableRow ,   Label ) ) \n task   =   TabularClassificationSingle ( blocks ,   data ) \n learner   =   tasklearner ( task ,   data ;   callbacks = [ Metrics ( accuracy ) ] ,   batchsize = 128 ) \n fitonecycle! ( learner ,   3 ,   0.2 ) showoutputs ( task ,   learner ,   backend = ShowText ( ) )"},{"doctype":"documentation","id":"references/FluxTraining.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.__init__","title":"__init__","text":""},{"doctype":"documentation","id":"references/Flux._big_finale","title":"_big_finale","text":""},{"doctype":"documentation","id":"references/Flux.ones32","title":"ones32","text":" ones32(size...) = ones(Float32, size...)\nzeros32(size...) = zeros(Float32, size...)\n Return an  Array{Float32}  of the given  size ."},{"doctype":"documentation","id":"references/Flux.params!","title":"params!","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Project","title":"Project","text":""},{"doctype":"documentation","id":"references/Flux.Losses.crossentropy","title":"crossentropy","text":" crossentropy(ŷ, y; dims = 1, ϵ = eps(ŷ), agg = mean)\n Return the cross entropy between the given probability distributions; calculated as agg(-sum(y .* log.(ŷ .+ ϵ); dims))\n Cross entropy is typically used as a loss in multi-class classification, in which case the labels  y  are given in a one-hot format. dims  specifies the dimension (or the dimensions) containing the class probabilities. The prediction  ŷ  is supposed to sum to one across  dims , as would be the case with the output of a  softmax  operation. For numerical stability, it is recommended to use  logitcrossentropy rather than  softmax  followed by  crossentropy  . Use  label_smoothing  to smooth the true labels as preprocessing before computing the loss. See also:  logitcrossentropy ,  binarycrossentropy ,  logitbinarycrossentropy . Example julia> y_label = Flux.onehotbatch([0, 1, 2, 1, 0], 0:2)\n3×5 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  ⋅  ⋅  ⋅  1\n ⋅  1  ⋅  1  ⋅\n ⋅  ⋅  1  ⋅  ⋅\n\njulia> y_model = softmax(reshape(-7:7, 3, 5) .* 1f0)\n3×5 Matrix{Float32}:\n 0.0900306  0.0900306  0.0900306  0.0900306  0.0900306\n 0.244728   0.244728   0.244728   0.244728   0.244728\n 0.665241   0.665241   0.665241   0.665241   0.665241\n\njulia> sum(y_model; dims=1)\n1×5 Matrix{Float32}:\n 1.0  1.0  1.0  1.0  1.0\n\njulia> Flux.crossentropy(y_model, y_label)\n1.6076053f0\n\njulia> 5 * ans ≈ Flux.crossentropy(y_model, y_label; agg=sum)\ntrue\n\njulia> y_smooth = Flux.label_smoothing(y_label, 0.15f0)\n3×5 Matrix{Float32}:\n 0.9   0.05  0.05  0.05  0.9\n 0.05  0.9   0.05  0.9   0.05\n 0.05  0.05  0.9   0.05  0.05\n\njulia> Flux.crossentropy(y_model, y_smooth)\n1.5776052f0\n"},{"doctype":"documentation","id":"references/FastAI.Vision.imagestats","title":"imagestats","text":" imagestats(image, C)\n Compute the color channel-wise means and standard deviations of all pixels. image  is converted to color type  C  (e.g.  RGB{N0f8} ,  Gray{N0f8} ) before statistics are calculated."},{"doctype":"documentation","id":"references/FastAI.OneHotTensor","title":"OneHotTensor","text":" OneHotTensor{N, T}(classes) <: Block\n A block representing a one-hot encoded, N-dimensional array categorical variable. For example, a single categorical label is a  OneHotTensor{0, T}  (aliased to  OneHotTensor{T} ). Use the  OneHot  encoding to one-hot encode  Label s or  LabelMulti s."},{"doctype":"documentation","id":"references/FastAI.describemethod","title":"describemethod","text":""},{"doctype":"documentation","id":"references/FluxTraining.loadmodel","title":"loadmodel","text":" loadmodel(path)\n Loads a model that was saved to  path  using  FluxTraining. savemodel ."},{"doctype":"documentation","id":"references/Flux.Losses.mse","title":"mse","text":" mse(ŷ, y; agg = mean)\n Return the loss corresponding to mean square error: agg((ŷ .- y) .^ 2)\n See also:  mae ,  msle ,  crossentropy . Example julia> y_model = [1.1, 1.9, 3.1];\n\njulia> y_true = 1:3;\n\njulia> Flux.mse(y_model, y_true)\n0.010000000000000018\n"},{"doctype":"documentation","id":"references/FastAI.Datasets.registerrecipe!","title":"registerrecipe!","text":" registerrecipe!([registry,] name, recipe)\n Register a recipe in  registry::DatasetRegistry  for dataset  name ."},{"doctype":"documentation","id":"references/Flux.Optimise.ADAGrad","title":"ADAGrad","text":" ADAGrad(η = 0.1, ϵ = \n"},{"doctype":"documentation","id":"references/FastAI.TaskDataset","title":"TaskDataset","text":" taskdataset(data, task, context)\n Transform data container  data  of samples into a data container of encoded samples. Maps  encodesample(task, context, sample)  over the observations in data . Also handles in-place  getobs!  through  encode! ."},{"doctype":"documentation","id":"references/FastAI.discrlr_optimizer","title":"discrlr_optimizer","text":" frozen_optimizer(optim, grouper, model, factor)\n Create an optimizer that discounts updates parameters which  ParamGrouper puts into group  1  by  factor ."},{"doctype":"documentation","id":"references/Flux._isonehot","title":"_isonehot","text":""},{"doctype":"documentation","id":"references/Flux.AlphaDropout","title":"AlphaDropout","text":" AlphaDropout(p; rng = rng_from_array())\n A dropout layer. Used in Self - Normalizing Neural Networks . The AlphaDropout layer ensures that mean and variance of activations remain the same as before. Does nothing to the input once  testmode!  is true."},{"doctype":"documentation","id":"references/Flux._channels_in","title":"_channels_in","text":""},{"doctype":"documentation","id":"references/DataAugmentation.CenterResizeCrop","title":"CenterResizeCrop","text":""},{"doctype":"documentation","id":"references/FastAI.accuracy_thresh","title":"accuracy_thresh","text":""},{"doctype":"documentation","id":"references/FluxTraining.step!","title":"step!","text":" step!(learner, phase::Phase, batch)\n Run one step of training for  learner  on batch. Behavior is customized through  phase . Extending This is a required method for custom  Phase s to implement. To implement  step! , it is recommended you make use of  runstep to get begin and end events as well as proper handling of CancelStepException s. See the implementations of  TrainingPhase  and  ValidationPhase for reference."},{"doctype":"documentation","id":"references/Flux.normalise","title":"normalise","text":" normalise(x; dims=ndims(x), ϵ=1e-5)\n Normalise  x  to mean 0 and standard deviation 1 across the dimension(s) given by  dims . Per default,  dims  is the last dimension. ϵ  is a small additive factor added to the denominator for numerical stability."},{"doctype":"documentation","id":"references/DataAugmentation._autorange","title":"_autorange","text":""},{"doctype":"documentation","id":"references/Flux._isleaf","title":"_isleaf","text":""},{"doctype":"documentation","id":"references/Flux.GlobalMeanPool","title":"GlobalMeanPool","text":" GlobalMeanPool()\n Global mean pooling layer. Transforms (w,h,c,b)-shaped input into (1,1,c,b)-shaped output, by performing mean pooling on the complete (w,h)-shaped feature maps. julia> xs = rand(Float32, 100, 100, 3, 50);\n\njulia> m = Chain(Conv((3,3), 3 => 7), GlobalMeanPool());\n\njulia> m(xs) |> size\n(1, 1, 7, 50)\n"},{"doctype":"documentation","id":"references/FluxTraining.CheckDataIteratorValid","title":"CheckDataIteratorValid","text":""},{"doctype":"documentation","id":"references/Flux.OneHotArray","title":"OneHotArray","text":" OneHotArray{T,L,N,M,I} <: AbstractArray{Bool,M}\n These are constructed by  onehot  and  onehotbatch . Parameter  I  is the type of the underlying storage, and  T  its eltype."},{"doctype":"documentation","id":"references/Flux.Optimise.runall","title":"runall","text":""},{"doctype":"documentation","id":"references/DataAugmentation.testprojective","title":"testprojective","text":" testprojective(tfm)\n Test invariants of a  ProjectiveTransform . getprojection  is defined, and, given a constant  randstate  parameter, always returns the same result. It preserves the item type, i.e.  apply(tfm, ::I) -> I . Applying it to multiple items with the same bounds results in the same bounds for all items."},{"doctype":"documentation","id":"references/FastAI.Vision.Models.UNetDynamic","title":"UNetDynamic","text":" UNetDynamic(backbone, inputsize, k_out[; kwargs...])\n Create a U-Net model from convolutional  backbone  architecture. After every downsampling layer (i.e. pooling or strided convolution), a skip connection and an upsampling block are inserted, resulting in a convolutional network with the same spatial output dimensions as its input. Outputs an array with  k_out channels. Keyword arguments fdownscale = 0 : Number of upsampling steps to leave out. By default there will be one upsampling step for every downsampling step in  backbone . Hence if the input spatial size is  (h, w) , the output size will be  (h/2^fdownscale, w/2^fdownscale) , i.e. to get outputs at half the resolution, set  fdownscale = 1 . kwargs... : Other keyword arguments are passed through to  upsample . Examples using   FastAI ,   Metalhead \n \n backbone   =   Metalhead . ResNet50 ( pretrain = true ) . layers [ 1 ] [ 1 : end - 1 ] \n unet   =   UNetDynamic ( backbone ,   ( 256 ,   256 ,   3 ,   1 ) ;   k_out   =   10 ) \n Flux . outputsize ( unet ,   ( 256 ,   256 ,   3 ,   1 ) )   ==   ( 256 ,   256 ,   10 ,   1 ) \n \n unet   =   UNetDynamic ( backbone ,   ( 256 ,   256 ,   3 ,   1 ) ;   fdownscalk_out   =   10 ) \n Flux . outputsize ( unet ,   ( 256 ,   256 ,   3 ,   1 ) )   ==   ( 256 ,   256 ,   10 ,   1 )"},{"doctype":"documentation","id":"references/DataAugmentation.ToEltype","title":"ToEltype","text":" ToEltype(T)\n Converts any  AbstractArrayItem  to an  AbstractArrayItem{N, T} . Supports  apply! . Examples using   DataAugmentation \n \n tfm   =   ToEltype ( Float32 ) \n item   =   ArrayItem ( rand ( Int ,   10 ) ) \n apply ( tfm ,   item )"},{"doctype":"document","id":"documents/developing.md","title":"Development workflow","text":" Development workflow FastAI.jl, as a end-user friendly umbrella package, puts importance on helpful documentation. To make it easier to write documentation and see the results interactively, you can follow this guide. Setup You’ll have to do the following: Fork FastAI . jl and add it as a  dev  dependency .  You can fork it from  the GitHub repository . Then use  Pkg  to add your fork to your Julia environment: using   Pkg \n Pkg . develop ( url = \" https://github.com/<myusername>/FastAI.jl.git \" ) Activate the documentation environment and install the dependencies .  You can find the folder that FastAI.jl was cloned to using  using FastAI; pkgdir(FastAI) . In a Julia session, change the current directory to that path, activate the  docs/  environment and install unregistered dependencies: using   FastAI ,   Pkg \n \n cd ( pkgdir ( FastAI ) ) \n Pkg . activate ( \" ./docs/ \" ) \n Pkg . add ( url = \" https://github.com/lorenzoh/Pollen.jl \" ) \n Pkg . add ( url = \" https://github.com/lorenzoh/LiveServer.jl \" ) \n Pkg . instantiate ( ) Finally you can start the development server which will serve the documentation locally and reload any changes you make: include ( \" ./docs/serve.jl \" ) On subsequent runs, it’ll be enough to activate the environment and include the startup file: using   FastAI ,   Pkg \n Pkg . activate ( joinpath ( pkgdir ( FastAI ) ,   \" docs \" ) ) \n include ( joinpath ( pkgdir ( FastAI ) ,   \" docs \" ,   \" serve.jl \" ) ) Notes For performance reasons, the development server will only build each page once you open it in the browser, you might have to refresh the tab after a few seconds. The terminal output will show when a page is being built; for documentation pages that have a lot of code cells that need to be run, it can take some time for the page to be built. If any changes are made to the source file of the documentation package, the page will automatically be rebuilt and the tab reloads. Adding documentation Adding source files Documentation pages correspond to a Markdown  .md  or Jupyter Notebook  .ipynb  file that should be stored in the  docs/  folder. Jupyter Notebooks should be used when they use resources that are not available on the GitHub CI, like a GPU needed for training. You should run them locally and the outputs will be captured and inserted into the HTML page. Markdown documents should be preferred for everything else, as they allow the code examples to be run on the GitHub CI, meaning they’ll stay up-to-date unlike a notebook that has to be manually rerun. Both formats support the  Markdown syntax of Publish . jl  and in markdown files the  cell syntax of Publish . jl  can be used to mark code cells. These will be run and the output is inserted into the HTML page. Linking to documentation For a new documentation file to be discoverable, you have to add an entry to the nested Markdown list in  toc.md , which corresponds to the sidebar in the documentation (Updating the sidebar currently requires interrupting and reincluding the file that starts the development server). Documentation pages can also link to each other using standard Markdown link syntax. Referencing code symbols Symbols like  fitonecycle!  can be referenced by using the cross-referencing syntax  [`fitonecycle!`](#)   which will link to and create a reference page from the symbol’s docstrings. It will also be added as an entry on the references page."},{"doctype":"document","id":"documents/CHANGELOG.md","title":"Changelog","text":" Changelog All notable changes to this project will be documented in this file. The format is based on  Keep a Changelog , and this project adheres to  Semantic Versioning . v0 . 4 . 1 Added New documentation frontend based on Pollen.jl: https://fluxml.ai/FastAI.jl/dev/i/ Now supports Flux.jl v0.13 (https://github.com/FluxML/FastAI.jl/pull/202) Changed Now has ImageIO.jl as a dependency to ensure that fast jpg loading using JpegTurbo.jl is used v0 . 4 . 0 (2022 - 03 - 19) Added Made block-based learning method more modular.  SupervisedMethod  now supplants  BlockMethod .   PR getencodings  and  getblocks  should now be used to get block information and encodings from a method See the [new tutorial training a Variational Autoencoder]. See also the docstrings for  AbstractBlockTask  and  SupervisedTask Changed (BREAKING): all learning method names have been renamed to task, i.e  method*  ->  task*  and  Method*  ->  Task* . Specifically, these exported symbols are affected: BlockMethod  ->  BlockTask , describemethod  ->  describetask , methodmodel  ->  taskmodel , methoddataset  ->  taskdataset , methoddataloaders  ->  taskdataloaders , methodlossfn  ->  tasklossfn , findlearningmethods  ->  findlearningtasks , methodlearner  ->  tasklearner , savemethodmodel  ->  savetaskmodel , loadmethodmodel  ->  loadtaskmodel BlockMethod  now deprecated in favor of  SupervisedMethod (INTERNAL) domain-specific functionality has moved to submodules  FastAI.Vision  (computer vision) and  FastAI.Tabular  (tabular data). Exports of  FastAI  are not affected. (INTERNAL) test suite now runs on InlineTest.jl Removed v0 . 3 . 0 (2021/12/11) Added A new API for visualizing data. See  this issue  for motivation. This includes: High-level functions for visualizing data related to a learning method:  showsample ,   showsamples ,  showencodedsample ,  showencodedsamples ,  showbatch ,  showprediction ,  showpredictions ,  showoutput ,  showoutputs ,  showoutputbatch Support for multiple backends, including a new text-based show backend that you can use to visualize data in a non-graphical environment. This is also the default unless  Makie  is imported. Functions for showing blocks directly:  showblock ,  showblocks Interfaces for extension:  ShowBackend ,  showblock! ,  showblocks! Removed The old visualization API incl. all its  plot*  methods:  plotbatch ,  plotsample ,  plotsamples ,  plotpredictions 0 . 2 . 0 (2021/09/21) Added High-level API “FasterAI” dataset recipes learning method helpers Find datasets and learning methods based on  Block s:  finddatasets ,  findlearningmethods loaddataset  for quickly loading data containers from configured recipes Data container recipes ( DatasetRecipe ,  loadrecipe ) Documentation setions for FasterAI interfaces: Discovery Blocks and encodings New interfaces blockbackbone  creates a default backbone for an input block Support for tabular data along with recipes and learning methods: Tabular classification tutorial TabularPreprocessing ,  TableRow ,  TableDataset ,  TabularClassificiationSingle ,  TabularRegression Changed Documentation sections to reference FasterAI interfaces: README Introduction Data containers Combined how-tos on training  into a single page Breaking changes to  methodlearner : now accepts  callbacks  as kwarg validdata  no longer keyword model  and  backbone  now kwargs;  isbackbone  removed. if neither  backbone  or  model  are given, uses  blockbackbone  for default backbone. see updated docstring for details"},{"doctype":"documentation","id":"references/Flux._tie_check","title":"_tie_check","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.checksize","title":"checksize","text":""},{"doctype":"documentation","id":"references/FastAI.BlockMethod","title":"BlockMethod","text":""},{"doctype":"documentation","id":"references/Flux.sparse_init","title":"sparse_init","text":" sparse_init([rng=GLOBAL_RNG], rows, cols; sparsity, std = 0.01) -> Array\nsparse_init([rng]; kw...) -> Function\n Return a  Matrix{Float32}  of size  rows, cols  where each column contains a fixed fraction of zero elements given by  sparsity . Non-zero elements are normally distributed with a mean of zero and standard deviation  std . This method is described in [1]. Examples julia> count(iszero, Flux.sparse_init(10, 10, sparsity=1/5))\n20\n\njulia> sum(0 .== Flux.sparse_init(10, 11, sparsity=0.9), dims=1)\n1×11 Matrix{Int64}:\n 9  9  9  9  9  9  9  9  9  9  9\n\njulia> Dense(3 => 10, tanh; init=Flux.sparse_init(sparsity=0.5))\nDense(3 => 10, tanh)  # 40 parameters\n\njulia> count(iszero, ans.weight, dims=1)\n1×3 Matrix{Int64}:\n 5  5  5\n References [1] Martens, J, “Deep learning via Hessian-free optimization”  Proceedings of the 27th International Conference on International Conference on Machine Learning . 2010."},{"doctype":"documentation","id":"references/FluxTraining.ConflictResolution","title":"ConflictResolution","text":" abstract type ConflictResolution\n A conflict resolution strategy for resolving write/write conflicts of two callbacks. See  resolveconflict ."},{"doctype":"documentation","id":"references/FluxTraining.replacecallback!","title":"replacecallback!","text":" replacecallback!(learner, callback::C)\n Replace existing callback of type  C  on learner with  callback . Return the replaced callback. If  learner  doesn’t have a callback of type  C , add  callback  and return  nothing ."},{"doctype":"documentation","id":"references/FastAI.propagatewrapper","title":"propagatewrapper","text":""},{"doctype":"documentation","id":"references/FastAI.showblockinterpretable","title":"showblockinterpretable","text":" showblockinterpretable(backend, encodings, block, obs)\n Decode  block  successively by applying  encodings  until a block is gotten that can be shown by  backend . Useful to visualize encoded data that is not directly interpretable, for example an  Image{2}  representing an encoded  Image . Examples using   FastAI \n encodings   =   ( ImagePreprocessing ( ) , ) \n block   =   ImageTensor { 2 } ( 3 ) \n x   =   FastAI . mockblock ( block ) \n \n showblockinterpretable ( ShowText ( ) ,   encodings ,   block ,   x )    # will decode to an `Image` \n @ test_throws   showblock ( ShowText ( ) ,   encodings ,   block ,   x )    # will error"},{"doctype":"documentation","id":"references/DataAugmentation.showitems","title":"showitems","text":" showitems(items)\n Visualize  items ."},{"doctype":"documentation","id":"references/FastAI.showblocks!","title":"showblocks!","text":" showblocks([backend], block, obss)\nshowblocks!(handle, backend, block, obss)\n Show a vector of observations  obss  of the same  block  type. Examples data ,   blocks   =   loaddataset ( \" imagenette2-160 \" ) \n samples   =   [ getobs ( data ,   i )   for   i   in   range ( 1 : 4 ) ] \n showblocks ( blocks ,   samples ) Extending This is used for showing batches of observations, unlike the  Tuple  variant of  showblock!  which assumes an observation consists of multiple blocks. Usually, a  ShowBackend  will show an observation in one row with  showblock! and  showblocks!  will show multiple rows."},{"doctype":"documentation","id":"references/FluxTraining.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.tests","title":"tests","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.blockitemtype","title":"blockitemtype","text":" blockitemtype(block, N)\n Return a constructor for a  DataAugmentation.Item  that can be projected. Return  nothing  by default, indicating that  block  cannot be turned into a projectable item for bounds with dimensionality  N . For example, we have blockitemtype(Image{2}(), 2) -> DataAugmentation.Image\n but blockitemtype(Image{3}(), 2) -> nothing\n"},{"doctype":"documentation","id":"references/DataAugmentation.copyitemdata!","title":"copyitemdata!","text":""},{"doctype":"documentation","id":"references/Flux.Losses","title":"Losses","text":""},{"doctype":"document","id":"documents/docs/howto/logtensorboard.md","title":"How to log to TensorBoard","text":" How to log to TensorBoard TensorBoard is a format and viewer for logs of model training and can be used to inspect and compare the results of training runs. We can log step and epoch metrics, hyperparameters and even visualizations to TensorBoard using FluxTraining.jl’s logging callbacks. Generating logs To use logging callbacks, we need to pass a log backend, here  TensorBoardBackend . This design allows flexibly supporting other logging backends like Weights and Biases or neptune.ai in the future. using   FastAI \n \n dir   =   mktempdir ( ) \n backend   =   TensorBoardBackend ( dir ) Then we create callbacks for logging metrics and hyperparameters to that backend: metricscb   =   LogMetrics ( backend ) \n hparamscb   =   LogHyperParams ( backend ) Like any other callbacks, these can then be passed to a  Learner  along with other callbacks and we can start training. By using the  Metrics  callback we can log metrics other than the loss. callbacks   =   [ \n     metricscb , \n     hparamscb , \n     ToGPU ( ) , \n     Metrics ( accuracy ) \n ] \n \n data   =   _ \n task   =   _ \n learner   =   tasklearner ( task ,   data ;   callbacks = callbacks ) \n fitonecycle! ( learner ,   5 ) Inspecting logs To inspect the logs you will have to install the  tensorboard  command-line using  pip  (you can access the command-line in the Julia REPL by pressing  ; ). shell> pip install tensorboard\n After this one-time installation, you can run it by pointing it to the log directory created above: shell> tensorboard --logdir $dir\n This should give you an URL that you can open in a browser which should look like this: Note that you can also open TensorBoard and it will update as the training progresses."},{"doctype":"documentation","id":"references/Flux.gpu","title":"gpu","text":" gpu(x)\n Moves  m  to the current GPU device, if available. It is a no-op otherwise. See the  CUDA . jl docs to help identify the current device. This works for functions, and any struct marked with  @functor . julia> m = Dense(1,2)\nDense(1, 2)\n\njulia> typeof(m.W)\nMatrix{Float32}\n\njulia> m_gpu = gpu(m)\nDense(1, 2)\n\njulia> typeof(m_gpu.W) # notice the type of the array changed to a CuArray\nCuArray{Float32, 2}\n"},{"doctype":"documentation","id":"references/FluxTraining.log_to","title":"log_to","text":" log_to(backend, loggable, group, i)\nlog_to(backends, loggable, group, i)\n Log  loggable  to  backend  with  group  to index  i . loggable  is any  Loggables.Loggable group  can be a  String  or a tuple of  String s implying some grouping which can be used by a supporting backend. i  is a step counter and unique for every group."},{"doctype":"document","id":"documents/notebooks/siamese.ipynb","title":"Siamese image similarity","text":" Siamese image similarity This tutorial is adapted from  this tutorial  in fast . ai ’ s documentation .  It tries to stay as close to the original as possible, but diverges where the APIs differ . In this tutorial, we will see how to deal with a new type of task using the middle layer of the fastai library. The example we will use is a Siamese network, that takes two images and determine if they are of the same class or not. In particular we will see: how to quickly get DataLoaders from a standard PyTorch Datasets how to adapt this in a Transform to get some of the show features of fastai how to add some new behavior to show_batch/show_results for a custom task how to write a custom DataBlock how to create your own model from a pretrained model how to pass along a custom splitter to Learner to take advantage of transfer learning UPDATE Setup Since we’ll be implementing some image operations manually, we’ll add the  Images  package: using   Pkg ;   Pkg . add ( \" Images \" ) Preparing the data To make our data ready for training a model, we need to create data iterators for training and validation, for example using  DataLoader . Usually, the first step is to create a  data container  that is then wrapped inside a  DataLoader . Unlike in fast.ai, FastAI.jl separates the loading part from the encoding part. In this case, loading means getting pairs of images and encoding includes preprocessing and augmenting them. We’ll first create a data container that just loads pairs of images and later show how to apply transforms on top of that. using   FastAI \n import   CairoMakie ;   CairoMakie . activate! ( type = \" png \" ) Using the low - level API First we’ll use  datasetpath  to download and untar the dataset and then find all image files. path   =   joinpath ( datasetpath ( \" oxford-iiit-pet \" ) ,   \" images \" ) \n files   =   loadfolderdata ( path ;   filterfn = isimagefile ) \n files [ 1 ] We can open the first image and have a look at it. Note that array indices start at 1 in Julia. image   =   loadfile ( files [ 1 ] ) Let’s wrap all the standard preprocessing (resize and conversion to tensor and reordering of the channels) in one helper function. Note some differences to Python here: images by default are 2D-arrays of pixels, so we need to use  channelview  to get a 3D array with the color dimension expanded pixel values are assumed to be between 0 and 1, so we do not need to divide them by 255 For flexibility, we also separate the loading ( loadfile ) from the transformations applied to the image. using   Images \n \n function   transform_image ( image ,   sz = 224 ) \n     image_resized   =   imresize ( convert . ( RGB { N0f8 } ,   image ) ,   ( sz ,   sz ) ) \n     a   =   permuteddimsview ( channelview ( image_resized ) ,   ( 2 ,   3 ,   1 ) ) \n end transform_image ( loadfile ( files [ 1 ] ) )   |>   summary We can see the label of our image is in the filename, before the last _ and some number. We can then use a regex expression to create a label function: label_func ( path )   =   match ( r \" ^(.*)_\\d+\\.jpg$ \" ,   pathname ( path ) ) [ 1 ] \n label_func ( files [ 1 ] ) Now let’s gather all unique labels: labels   =   map ( label_func ,   files ) \n length ( unique ( labels ) ) We could now use  mapobs  to create a data container from our list of files. It applies a function like loading an image lazily and we can get single observations using  getobs  and the number of observtions with  nobs . It is the same as a  torch.utils.data.Dataset . For example, the following example creates a data container with tuples of an image and a category that could be used for image classification data   =   mapobs ( files )   do   file \n     return   ( loadfile ( file ) ,   label_func ( file ) ) \n end \n \n @ show   nobs ( data ) \n image ,   label   =   getobs ( data ,   1 ) To create our Siamese datasets, however, we will need to create tuples of images for inputs and the target will be  true  if the images are of the same class,  false  otherwise. First we’ll shuffle the files and split them into a training and a validation set using   Random \n idxs   =   shuffle ( 1 : length ( files ) ) \n cut   =   round ( Int ,   0.8   *   length ( idxs ) ) \n trainidxs ,   valididxs   =   idxs [ 1 : cut ] ,   idxs [ cut + 1 : end ] \n trainfiles ,   validfiles   =   files [ trainidxs ] ,   files [ valididxs ] \n summary . ( ( trainfiles ,   validfiles ) ) Let’s create a custom data container that returns pairs of indices and a Boolean indicating whether the label is the same. Half of the pairs will have the same label, and half will not. Additionally, during training the other image will be chosen randomly while for the validation the pairs will always be the same. While you can get far with basic data containers like  FileDataset  and transformations like  mapobs  and  filterobs , sometimes it’s simplest to create a custom data container. You just need to implement  getobs  and  nobs  for your type, similar to how you would implement  __getindex__  and  __len__  for a PyTorch  Dataset . import   LearnBase \n \n  struct   SiamesePairs \n     labels \n     same \n     other \n     valid \n end \n \n function   SiamesePairs ( labels ;   valid   =   false ) \n     ulabels   =   unique ( labels ) \n     same   =   Dict ( \n         label   =>   [ i   for   ( i ,   l )   in   enumerate ( labels )   if   l   ==   label ] \n         for   label   in   ulabels ) \n     other   =   Dict ( \n         label   =>   [ i   for   ( i ,   l )   in   enumerate ( labels )   if   l   !=   label ] \n         for   label   in   ulabels ) \n \n     return   SiamesePairs ( labels ,   same ,   other ,   valid ) \n end \n \n function   LearnBase . getobs ( si :: SiamesePairs ,   idx :: Int ) \n     rng   =   si . valid   ?   MersenneTwister ( idx )   :   Random . GLOBAL_RNG  \n     if   rand ( rng )   >   0.5 \n         return   ( ( idx ,   rand ( rng ,   si . same [ si . labels [ idx ] ] ) ) ,   true ) \n     else \n         return   ( ( idx ,   rand ( rng ,   si . other [ si . labels [ idx ] ] ) ) ,   false ) \n     end \n end \n \n LearnBase . nobs ( si :: SiamesePairs )   =   length ( si . labels ) We can combine this data container that gives us pairs of indices with the files and map the loading and preprocessing functions over it to get a data container that is ready to be passed to a  DataLoader . function   siamesedata ( files ;   valid   =   false ,   transformfn   =   identity ) \n     labels   =   map ( label_func ,   files ) \n     si   =   SiamesePairs ( labels ;   valid   =   valid ) \n     return   mapobs ( si )   do   obs \n         ( i ,   j ) ,   same   =   obs \n         image1   =   transformfn ( loadfile ( files [ i ] ) ) \n         image2   =   transformfn ( loadfile ( files [ j ] ) ) \n         return   ( ( image1 ,   image2 ) ,   same ) \n     end \n end traindata   =   siamesedata ( trainfiles ;   transformfn   =   transform_image ) \n validdata   =   siamesedata ( validfiles ;   transformfn   =   transform_image ,   valid   =   true ) ; We can see that each observation consists of two images and a Boolean: summary . ( getobs ( traindata ,   1 ) ) To use the above data containers in training, we can simply pass them to a  DataLoader . traindl   =   DataLoader ( shuffleobs ( traindata ) ,   16 ) \n \n validdl   =   DataLoader ( validdata ,   32 ) Next let’s look at how we can extend this example to make use FastAI.jl’s data augmentation, visualize our data and more. Using the Data Block API This is where FastAI.jl’s API diverges a bit from fast.ai’s. Note how above, we made sure to separate the data container creation and loading from disk from the preprocessing that is applied to every observation. In FastAI.jl, the preprocessing or “encoding” is implemented through  a learning task . Learning tasks contain any configuration and, beside data processing, have extensible functions for visualizations and model building. One advantage of this separation between loading and encoding is that the data container can easily be swapped out as long as it has observations suitable for the learning task (in this case a tuple of two images and a Boolean). It also makes it easy to  export  models and all the necessary configuration. The easiest way to create learning tasks is using the data block API which should suit the very most of all use cases. It is also possible to directly  implement the lower - level  LearningTask  interface . The best way to understand it is to use it, so let’s build a learning task for Siamese image similarity. We specify the kinds of input and target data as  blocks . Here, we have two 2D images as input ( (Image{2}(), Image{2}()) ) and a binary label ( Label([true, false]) ) as output. We also pass in a tuple of encodings that describe how the data is transformed before being fed to a model. Here  ProjectiveTransforms  resizes the images to the same size,  ImagePreprocessing  converts the images to the right format and  OneHot  one-hot encodes the labels. task   =   BlockTask ( \n     ( ( Image { 2 } ( ) ,   Image { 2 } ( ) ) ,   Label ( [ true ,   false ] ) ) , \n     ( \n         ProjectiveTransforms ( ( 128 ,   128 ) ,   buffered = false ,   sharestate = false ) , \n         ImagePreprocessing ( buffered = false ) , \n         OneHot ( ) , \n     ) \n ) We can get a better understanding of the representations the data goes through using  describetask : describetask ( task ) We can reuse all the code above for creating the data container, we just omit the preprocessing function.  taskdataloaders  constructs training and validation data loaders from data containers by mapping the task encoding over the data containers: traindata   =   siamesedata ( trainfiles ,   valid   =   true ) \n validdata   =   siamesedata ( validfiles ;   valid   =   true ) ; \n traindl ,   valdl   =   taskdataloaders ( traindata ,   validdata ,   task ,   32 ) The above is equivalent to lazily mapping the encoding over the data containers and creating a  DataLoader  from them. traindl   =   DataLoader ( \n \n     mapobs ( \n \n         sample   ->   encodesample ( task ,   Training ( ) ,   sample ) , \n \n         shuffleobs ( traindata ) ) , \n \n     4 ) \n \n validdl   =   DataLoader ( \n \n     mapobs ( \n \n         sample   ->   encodesample ( task ,   Validation ( ) ,   sample ) , \n \n         validdata ) , \n \n     8 ) By separating the data container preparation from the task-specific data preprocessing and augmentation, we were able to completely reuse the data container creation and quickly add FastAI.jl’s data augmentation. With the data iterators ready for training, we still need a model to train. For the Siamese similarity setting, a common model architecture is a encoder/feature extractor that is applied to both images and a head that transforms the concatenated image features, resulting in a similar/not similar categorical output. Flux.jl makes it easy to create models in a similar fashion to PyTorch modules. The forward pass is implemented by overloading the call operator, and the backward pass is automatically generated. using   FastAI . Flux \n \n  struct   SiameseModel { E ,   H } \n     encoder :: E \n     head :: H \n end \n \n # tells Flux that this struct contains submodules in its fields \n Flux . @ functor   SiameseModel \n \n # forward pass \n function   ( m :: SiameseModel ) ( ( xs1 ,   xs2 ) ) \n     return   m . head ( cat ( m . encoder ( xs1 ) ,   m . encoder ( xs2 ) ;   dims = 3 ) )  \n end We’ll use a XResNet model for the encoder and the same head that is used for classification models. Using  Flux.outputsize  we can check how many channels the encoder outputs without having to evaluate the model: using   Metalhead \n encoder   =   Models . xresnet18 ( ) \n encoder   =   Metalhead . ResNet50 ( pretrain = true ) . layers [ 1 ] [ 1 : end - 1 ] \n h ,   w ,   ch ,   b   =   Flux . outputsize ( encoder ,   ( 128 ,   128 ,   3 ,   1 ) ) We need to double this feature count since the head gets the concatenated features from two images: head   =   Models . visionhead ( 2  ch ,   2 ) model   =   SiameseModel ( encoder ,   head ) ; Let’s test the model works on a batch of training data: xs ,   ys   =   first ( traindl ) \n ŷs   =   gpu ( model ) ( gpu ( xs ) ) We’ll use categorical crossentropy (for logits) as a loss function and ADAM as optimizer: lossfn   =   Flux . Losses . logitcrossentropy \n optimizer   =   Flux . ADAM ( ) Now we can create a  Learner  and start training following the usual process. callbacks   =   [ \n     ToGPU ( ) , \n     Metrics ( accuracy ) \n ] \n learner   =   Learner ( model ,   ( traindl ,   valdl ) ,   optimizer ,   lossfn ,   callbacks ... ) plot ( lrfind ( learner ) ) encoder   =   Metalhead . ResNet50 ( pretrain = true ) . layers [ 1 ] [ 1 : end - 1 ] \n h ,   w ,   ch ,   b   =   Flux . outputsize ( encoder ,   ( 128 ,   128 ,   3 ,   1 ) ) \n head   =   Models . visionhead ( 2  ch ,   2 ) \n model   =   SiameseModel ( encoder ,   head ) ; \n learner   =   Learner ( model ,   ( traindl ,   valdl ) ,   ADAM ( 0.01 ) ,   lossfn ,   callbacks ... ) fitonecycle! ( learner ,   10 ,   4e-4 )"},{"doctype":"documentation","id":"references/Flux.Recur","title":"Recur","text":" Recur(cell)\n Recur  takes a recurrent cell and makes it stateful, managing the hidden state in the background.  cell  should be a model of the form: h, y = cell(h, x...)\n For example, here’s a recurrent network that keeps a running total of its inputs: accum ( h ,   x )   =   ( h   +   x ,   x ) \n rnn   =   Flux . Recur ( accum ,   0 ) \n rnn ( 2 )        # 2 \n rnn ( 3 )        # 3 \n rnn . state     # 5 \n rnn . ( 1 : 10 )    # apply to a sequence \n rnn . state     # 60 Folding over a 3d Array of dimensions  (features, batch, time)  is also supported: accum ( h ,   x )   =   ( h   .+   x ,   x ) \n rnn   =   Flux . Recur ( accum ,   zeros ( Int ,   1 ,   1 ) ) \n rnn ( [ 2 ] )                      # 2 \n rnn ( [ 3 ] )                      # 3 \n rnn . state                     # 5 \n rnn ( reshape ( 1 : 10 ,   1 ,   1 ,   : ) )   # apply to a sequence of (features, batch, time) \n rnn . state                     # 60"},{"doctype":"documentation","id":"references/Flux.dropout_mask","title":"dropout_mask","text":""},{"doctype":"documentation","id":"references/FluxTraining.getindexperm","title":"getindexperm","text":""},{"doctype":"documentation","id":"references/Flux._layer_show","title":"_layer_show","text":""},{"doctype":"documentation","id":"references/FluxTraining.Callback","title":"Callback","text":" abstract type Callback\n Supertype of all callbacks. Callbacks add custom functionality to the training loop by hooking into different  Events.Event s Any  Callback  can be used by passing it to  Learner . See subtypes(FluxTraining.Callback)  for implementations. Extending See  Custom callbacks  for a less succinct tutorial format. Create a  struct MyCallback  that subtypes  FluxTraining.Callback . Add event handlers by implementing methods for on (event, phase, callback, learner) . Methods should always dispatch on your callback, and may dispatch on specific  Phases.Phase s and  Events.Event s. For example, to implement an event handler that runs at the end of every step during training:  on(::StepEnd, ::AbstractTrainingPhase, ::MyCallback, learner) . Define what state the callback accesses and/or modifies by implementing stateaccess (::MyCallback) . While  learner  is always passed as an argument to  on  event handlers, by default a callback can not read or write to its fields. See  stateaccess  for more detail. If a callback needs to write some state that other callbacks should be able to access, it can store it in  learner.cbstate  if you add a permission in stateaccess . If the callback needs some one-time initialization, you can implement  init! which will be run at least once before any step is run."},{"doctype":"documentation","id":"references/FastAI.Datasets.runtests","title":"runtests","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.Models.ResBlock","title":"ResBlock","text":""},{"doctype":"documentation","id":"references/FastAI.showpredictions","title":"showpredictions","text":" showpredictions([backend], task, preds)\nshowpredictions([backend], task, samples, preds)\n Show predictions  pred . If  samples  are also given, show them next to the prediction."},{"doctype":"document","id":"documents/notebooks/tabularclassification.ipynb","title":"Tabular Classification","text":" Tabular Classification Tabular Classification involves having a categorical column as the target. Here, we’ll use the adult sample dataset from fastai and try to predict whether the salary is above 50K or not, making this a binary classification task. using   Flux \n using   FastAI \n using   Tables \n using   Statistics \n using   FluxTraining \n import   DataAugmentation We can quickly download and get the path of any dataset from fastai by using  datasetpath . Once we have the path, we’ll load the data in a  TableDataset . By default, if we pass in just the path to  TableDataset , the data is loaded in a  DataFrame , but we can use any package for accessing our data, and pass an object satisfying the  Tables . jl  interface to it. data   =   TableDataset ( joinpath ( datasetpath ( \" adult_sample \" ) ,   \" adult.csv \" ) ) In case our data was present in a different format for eg. parquet, it could be loaded into a data container as follows: using   Parquet \n \n TableDataset ( read_parquet ( parquet_path ) ) ; mapobs  is used here to split our target column from the rest of the row in a lazy manner, so that each observation consists of a row of inputs and a target variable. splitdata   =   mapobs ( row   ->   ( row ,   row [ : salary ] ) ,   data ) ; To create a learning task for tabular classification task, we need an input block, an output block, and the encodings to be performed on the data. The input block here is a  TableRow  which contains information about the nature of the columns (ie. categorical or continuous) along with an indexable collection mapping categorical column names to a collection with distinct classes in that column. We can get this mapping by using the  gettransformationdict  task with  DataAugmentation.Categorify . The outblock block used is  Label  for single column classification and the unique classes have to passed to it. This is followed by the encodings which needs to be applied on our input and output blocks. For the input block, we have used the  gettransforms  function here to get a standard bunch of transformations to apply, but this can be easily customized by passing in any tabular transformation from DataAugmentation.jl or a composition of those, to  TabularPreprocessing . In addition to this, we have just one-hot encoded the outblock. cat ,   cont   =   FastAI . getcoltypes ( data ) \n target   =   : salary \n cat   =   filter ( ! isequal ( target ) ,   cat ) \n catdict   =   FastAI . gettransformdict ( data ,   DataAugmentation . Categorify ,   cat ) ; inputblock   =   TableRow ( cat ,   cont ,   catdict ) \n targetblock   =   Label ( unique ( data . table [ : ,   target ] ) ) \n \n task   =   BlockTask ( \n     ( inputblock ,   targetblock ) , \n     ( \n         setup ( TabularPreprocessing ,   inputblock ,   data ) , \n         FastAI . OneHot ( ) \n     ) \n ) In case our initial problem wasn’t a classification task, and we had a continuous target column, we would need to perform tabular regression. To create a learning task suitable for regression, we use a  Continuous  block for representing our target column. This can be done even with multiple continuous target columns by just passing the number of columns in  Continuous . For example, the task here could be used for 3 targets. task2   =   BlockTask ( \n \n     ( \n \n         TableRow ( cat ,   cont ,   catdict ) ,  \n \n         Continuous ( 3 ) \n \n     ) , \n \n     ( ( FastAI . TabularPreprocessing ( data ) , ) ) , \n \n     outputblock   =   Continuous ( 3 ) \n \n ) To get an overview of the learning task created, and as a sanity test, we can use  describetask . This shows us what encodings will be applied to which blocks, and how the predicted ŷ values are decoded. describetask ( task ) getobs  gets us a row of data from the  TableDataset , which we encode here. This gives us a tuple with the input and target. The input here is again a tuple, containing the categorical values (which have been label encoded or “categorified”) and the continuous values (which have been normalized and any missing values have been filled). getobs ( splitdata ,   1000 ) x   =   encodesample ( task ,   Training ( ) ,   getobs ( splitdata ,   1000 ) ) To get a model suitable for our learning task, we can use  taskmodel  which constructs a suitable model based on the target block. model   =   taskmodel ( task ) Of course you can also create a custom backbone using the functions present in  FastAI.Models . cardinalities   =   collect ( map ( col   ->   length ( catdict [ col ] ) ,   cat ) ) \n \n ovdict   =   Dict ( : workclass   =>   10 ,   : education   =>   12 ,   Symbol ( \" native-country \" )   =>   16 ) \n overrides   =   collect ( map ( col   ->   col   in   keys ( ovdict )   ?   ovdict [ col ]   :   nothing ,   cat ) ) \n \n embedszs   =   FastAI . Models . get_emb_sz ( cardinalities ,   overrides ) \n catback   =   FastAI . Models . tabular_embedding_backbone ( embedszs ,   0.2 ) ; We can then pass a named tuple  (categorical = ..., continuous = ...)  to  taskmodel  to replace the default backbone. backbone   =   ( categorical   =   catback ,   continuous   =    BatchNorm ( length ( cont ) ) ) \n model   =   taskmodel ( task ,   backbone ) To directly get a  Learner  suitable for our task and data, we can use the  tasklearner  function. This creates both batched data loaders and a model for us. learner   =   tasklearner ( task ,   splitdata ; \n     backbone = backbone ,   callbacks = [ Metrics ( accuracy ) ] , \n     batchsize = 128 ,   buffered = false ) Once we have a  Learner , we can call  fitonecycle!  on it to train it for the desired number of epochs: fitonecycle! ( learner ,   3 ,   0.2 )"},{"doctype":"documentation","id":"references/Flux._bool_tie_check","title":"_bool_tie_check","text":""},{"doctype":"documentation","id":"references/FastAI.decay_optim","title":"decay_optim","text":" decay_optim(optim, wd)\n Add  WeightDecay  with value  wd  to optimizer  optim ."},{"doctype":"document","id":"documents/docs/fastai_api_comparison.md","title":"fastai API comparison","text":" fastai API comparison FastAI.jl is in many ways similar to the original Python  fastai , but also has its differences. This reference goes through all the sections in the  fastai: A Layered API for Deep Learning  paper and comments what the interfaces for the same functionality in FastAI.jl are, and where they differ or functionality is still missing. Applications FastAI.jl’s own data block API makes it possible to derive every part of a high-level interface with a unified API across tasks. Instead it suffices to create a learning task and based on the blocks and encodings specified the proper model builder, loss function, and visualizations are implemented (see below). For a high-level API, a complete  Learner  can be constructed using  tasklearner  without much boilerplate. There are some helper functions for  creating these learning tasks, for example  ImageClassificationSingle  and  ImageSegmentation . FastAI.jl additionally has a unified API for registering and discovering functionality across applications also based on the data block abstraction.   finddatasets  and  loaddataset  let you quickly load common datasets matching some data modality and  findlearningtask  lets you find learning task helpers for common tasks. See  the discovery tutorial  for more info. Vision Computer vision is the most developed part of FastAI.jl with good support for different tasks and optimized data pipelines with N-dimensional images, masks and keypoints. See the tutorial section for many examples. Tabular Support for tabular data is merged into master but is lacking documentation which will come with the next release (0.2.0). Deployment Through FastAI.jl’s  LearningTask  interface , the data processing logic is decoupled from the dataset creation and training and can be easily serialized and loaded to make predictions. See the tutorial on  saving and loading models . There is no integration (yet!) for text and collaborative filtering applications. High - level API High - level API foundations FastAI.jl also has a data block API but it differs from fastai’s in a number of ways. In the Julia package it only handles the data encoding and decoding part, and doesn’t concern itself with creating datasets. For dataset loading, see the  data container API . As mentioned above, the high-level application-specific logic is also derived from the data block API. To use it you need to specify a tuple of input and target blocks as well as a tuple of encodings that are applied to the data. The encodings  are invertible data-specific data processing steps which correspond to  fastai.Transform s. As in fastai, dispatch is used to transform applicable data and pass other data through unchanged. Unlike in fastai, there are no default steps associated with a block, allowing greater flexibility. We can create a  BlockTask  (similar to  fastai.DataBlock ) and get information about the representations the data goes through. using   FastAI \n import   FastAI :   Image \n \n task   =   BlockTask ( \n     ( Image { 2 } ( ) ,   Mask { 2 } ( [ \" foreground \" ,   \" background \" ] ) ) , \n     ( \n         ProjectiveTransforms ( ( 128 ,   128 ) ) , \n         ImagePreprocessing ( ) , \n         OneHot ( ) , \n     ) \n ) \n describetask ( task ) From this short definition, many things can be derived: data encoding model output decoding how to create a model from a backbone the loss function to use how to visualize samples and predictions Together with a  data container   data , we can quickly create a  Learner  using  tasklearner  which, like in fastai, handles the training for us. There are no application-specific  Learner  constructors like  cnn_learner  or  unet_learner  in FastAI.jl. learner   =   tasklearner ( task ,   data ) High-level training protocols like the  one - cycle learning rate schedule ,  fine - tuning  and the  learning rate finder  are then available to us: fit! ( learner ,   10 )                    # Basic training for 10 epochs \n finetune! ( learner ,   5 ,   1e-3 )          # Finetuning regimen for 1+5 epochs with lr=1e-3 \n fitonecycle! ( learner ,   10 )            # One-cycle learning rate regimen \n res   =   lrfind ( learner ) ;   plot ( res )     # Run learning rate finder and plot suggestions Incrementally adapting PyTorch code Since it is a Julia package, FastAI.jl is not written on top of PyTorch, but a Julia library for deep learning:  Flux . jl . In any case, the point of this section is to note that the abstractions in fastai are decoupled and existing projects can easily be reused. This is also the case for FastAI.jl as it is built on top of several decoupled libraries. Many of these were built specifically for FastAI.jl, but they are unaware of each other and useful in their own right: Flux . jl  provides models, optimizers, and loss functions, fulfilling a similar role to PyTorch MLDataPattern . jl  gives you tools for building and transforming data containers DataLoaders . jl  takes care of efficient, parallelized iteration of data containers DataAugmentation . jl  takes care of the lower levels of high-performance, composable data augmentations. FluxTraining . jl  contributes a highly extensible training loop with 2-way callbacks If that seems like a lot: don’t worry! If you’ve installed FastAI.jl, the functionality of most of these packages is reexported and you don’t have to install any of them explicitly. Consistency across domains While computer vision is the only domain with mature support for now, the abstractions underlying FastAI.jl are carefully crafted to ensure that learning tasks for different domains can be created using the same set of interfaces. This shows in that there’s no need for application-specific functionality above the data block API. Mid - level APIs Learner The  Learner  is very similar to fastai’s. It takes a model: any parameterized, differentiable function like a neural network or even  a trebuchet simulator training and validation data iterators: these can be  DataLoader s which paralellize data loading but any iterator over batches can be used optimizer loss function Two - way callbacks The training loop also supports two-way callbacks. See the  FluxTraining . jl docs  for a list of all available callbacks. While supporting all the functionality of fastai’s callbacks and training loop, it also provides  an extensible training loop API  that makes it straightforward to integrate custom training steps with the available callbacks. As a result, different training steps for problems other than standard supervised training can make use of existing callbacks  without the need to handle control flow through callbacks. Additionally, callbacks have an additional level of safety by being required to declare what state they access and modify. With a little more effort up-front, this guarantees correct ordering of callback execution through  a dependency graph . In the future, this will also make it possible to automatically run callbacks in parallel and asynchronously to reduce overhead by long-running callbacks like costly metric calculations and logging over the network. Encodings and blocks In the paper, this subsection is in the low-level section (named Transforms and Pipelines), but I’m putting it here since it is the core of FastAI.jl’s data block API. FastAI.jl provides  Encoding s and  Block s which correspond to fastai’s  Transform s and  Block s. Encodings implement an  encode  (and optionally  decode ) function that describes how data corresponding to some blocks is transformed and how that transformation can be inverted. There is also support for stateful encodings like  ProjectiveTransforms  which need to use the same random state to augment every data point. Additionally, encodings describe what kind of block data is returned from encoding, allowing inspection of the whole data pipeline. The  Block s are used to dispatch in the  encode  function to implement block-specific transformations. If no  encode  task is implemented for a pair of encoding and block, the default is to pass the data through unchanged like in fastai. The  Block s also allow implementing task-specific functionality: blocklossfn  takes a prediction and encoded target block to determine a good loss function to use. For example, for image classification we want to compare two one-hot encoded labels and hence define  blocklossfn(::OneHotTensor{0}, ::OneHotTensor{0}) = logitcrossentropy . blockmodel  constructs a model from a backbone that maps an input block to an output block. For example, for image segmentation we have  ImageTensor{N}()  as the input block and  OneHotTensor{N}  (one-hot encoded N-dimensional masks) as output, so  blockmodel  turns the backbone into a U-Net. showblock!  defines how to visualize a block of data. Generic optimizer FastAI.jl uses the optimizers from Flux.jl, which provides a similarly  composable API for optimzers . Generalized metric API Metrics are handled by the  Metrics  callback which takes in reducing metric functions or  FluxTraining.AbstractMetric s which have a similar API to fastai’s. fastai . data . external FastAI.jl makes all the same datasets available in  fastai.data.external  available. See  FastAI.Datasets.DATASETS  for a list of all datasets and use  datasetpath (name)  to download and extract a dataset. funcs _ kwargs and DataLoader, fastai . data . core In FastAI.jl, you are not restricted to a specific type of data iterator and can pass any iterator over batches to  Learner . In cases where performance is important  DataLoader  can speed up data iteration by loading and batching samples in parallel on background threads. All transformations of data happen through the data container interface which requires a type to implement  LearnBase.getobs  and  LearnBase.nobs , similar to PyTorch’s  torch.utils.data.Dataset . Data containers are then transformed into other data containers. Some examples: mapobs (f, data)  lazily maps a function  f  of over  data  such that  getobs(mapobs(f, data), idx) == f(getobs(data, idx)) . For example  mapobs(loadfile, files)  turns a vector of image files into a data container of images. DataLoader(data, batchsize)  is a wrapper around  batchviewcollated  which turns a data container of samples into one of collated batches and  eachobsparallel  which creates a parallel, buffered iterator over the observations (here batches) in the resulting container. groupobs (f, data)  splits a container into groups using a grouping function  f . For example,  groupobs(grandparentname, files)  creates training splits for files where the grandparent folder indicates the split. datasubset (data, idxs)  lazily takes a subset of the observations in  data . For more information, see the  data container tutorial  and the  MLDataPattern . jl docs . At a higher level, there are also convenience functions like  FileDataset  to create data containers. Layers and architectures Flux.jl already does a better job at functionally creating model architectures than PyTorch, so FastAI.jl makes use of its layers. For example  Flux.SkipConnection   corresponds to fastai’s  MergeLayer . The  FastAI.Models  submodule currently provides some high-level architectures like  xresnet18  and a U-Net builder  UNetDynamic  that can create U-Nets from  any  convolutional feature extractor. The  optional dependency   Metalhead . jl  also provides common pretrained vision models. Low - level APIs Due to the nature of the Julia language and its design around multiple dispatch, packages tend to compose really well, so it was not necessary to reimplement or provide a unified API for low-level operations. We’ll comment on the libraries that we were able to use. PyTorch foundations Unlike Python, Julia has native support for N-dimensional regular arrays. As such, there is a standard interface for arrays and libraries don’t need to implement their own. Consider that every deep learning framework in Python implements their own CPU and GPU arrays, which is part of the reason they are  frameworks , not  libraries  (with the latter being vastly preferable). Julia’s standard libraries implements the standard CPU  Array  type. GPU arrays are implemented through  CUDA . jl   CuArray  type (with unified support for GPU vendors other than nvidia in the works). As a result, Flux.jl, the deep learning library of choice for FastAI.jl, does not need to reimplement their own CPU and GPU array versions. This kind of composability in general largely benefits what can be accomplished in Julia. Some other libraries which are used under the hood: for image processing, the  Images . jl  ecosystem of packages is used; for reading and processing tabular data  DataFrames . jl  and  Tables . jl ; for plotting  Makie . jl . Type dispatch Multiple dispatch already is a core feature of the Julia language, hence the extensible interfaces in FastAI.jl are built around it and are natural fit for the language. Object - oriented semantic tensors As mentioned above, Julia has great support for arrays with extra functionality available to packages that provide wrapper arrays like  NamedDims . jl  which should generally  just work  with every part of the library. Hence there is no need for an addtional API that unifies separate packages, which in turn makes FastAI.jl more composable with other packages. In encodings, the array types are used for dispatch only where an especially performant implementation is possible, and the block information is used for dispatching the semantics of the encoding. GPU - accelerated augmentation FastAI.jl does not support GPU-accelerated augmentation (yet). Please open an issue if you run into a situation where data processing  becomes the bottleneck  and we’ll prioritize this. The affine transformations implemented in DataAugmentation.jl and used in FastAI.jl are properly composed to ensure high quality results. They are also optimized for speed and memory usage (with complete support for inplace transformations). Convenience functionality Much of the convenience provided by fastai is not required in Julia: @delegates : Due to the absence of deep class hierarchies, keyword arguments are seldom passed around (the only instance where this happens in FastAI.jl is  tasklearner ). @patch : since Julia is built around multiple dispatch, not classes, you just implement the task for a type, no patching needed L : due to first-class array support such a wrapper list container isn’t needed nbdev There is no  nbdev -equivalent in Julia at the moment. That said, this documentation is generated by a document creation package  Pollen . jl  that could be extended to support such a workflow. It already has support for different source and output formats like Jupyter notebooks, code execution and is built for interactive work with incremental rebuilds. Hopefully this page has given you some context for how FastAI.jl relates to fastai and how to map concepts between the two. You are encouraged to go through the tutorials to see the design decisions made in practice."},{"doctype":"documentation","id":"references/Flux.ones","title":"ones","text":""},{"doctype":"documentation","id":"references/Flux.Losses.huber_loss","title":"huber_loss","text":" huber_loss(ŷ, y; δ = 1, agg = mean)\n Return the mean of the  Huber loss given the prediction  ŷ  and true values  y .              | 0.5 * |ŷ - y|^2,            for |ŷ - y| <= δ\nHuber loss = |\n             |  δ * (|ŷ - y| - 0.5 * δ), otherwise\n"},{"doctype":"documentation","id":"references/Flux.LSTMCell","title":"LSTMCell","text":""},{"doctype":"documentation","id":"references/FluxTraining.protect","title":"protect","text":""},{"doctype":"documentation","id":"references/Flux.onehot","title":"onehot","text":" onehot(x, labels, [default])\n Return a  OneHotVector  which is roughly a sparse representation of  x .== labels . Instead of storing say  Vector{Bool} , it stores the index of the first occurrence of  x  in  labels . If  x  is not found in labels, then it either returns  onehot(default, labels) , or gives an error if no default is given. See also  onehotbatch  to apply this to many  x s, and  onecold  to reverse either of these, as well as to generalise  argmax . Examples julia> β = Flux.onehot(:b, (:a, :b, :c))\n3-element OneHotVector(::UInt32) with eltype Bool:\n ⋅\n 1\n ⋅\n\njulia> αβγ = (Flux.onehot(0, 0:2), β, Flux.onehot(:z, [:a, :b, :c], :c))  # uses default\n(Bool[1, 0, 0], Bool[0, 1, 0], Bool[0, 0, 1])\n\njulia> hcat(αβγ...)  # preserves sparsity\n3×3 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  ⋅  ⋅\n ⋅  1  ⋅\n ⋅  ⋅  1\n"},{"doctype":"documentation","id":"references/Flux.Losses.label_smoothing","title":"label_smoothing","text":" label_smoothing(y::Union{Number, AbstractArray}, α; dims::Int=1)\n Returns smoothed labels, meaning the confidence on label values are relaxed. When  y  is given as one-hot vector or batch of one-hot, its calculated as y .* (1 - α) .+ α / size(y, dims)\n when  y  is given as a number or batch of numbers for binary classification, its calculated as y .* (1 - α) .+ α / 2\n in which case the labels are squeezed towards  0.5 . α is a number in interval (0, 1) called the smoothing factor. Higher the value of α larger the smoothing of  y . dims  denotes the one-hot dimension, unless  dims=0  which denotes the application of label smoothing to binary distributions encoded in a single number. Example julia> y = Flux.onehotbatch([1, 1, 1, 0, 1, 0], 0:1)\n2×6 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n ⋅  ⋅  ⋅  1  ⋅  1\n 1  1  1  ⋅  1  ⋅\n\njulia> y_smoothed = Flux.label_smoothing(y, 0.2f0)\n2×6 Matrix{Float32}:\n 0.1  0.1  0.1  0.9  0.1  0.9\n 0.9  0.9  0.9  0.1  0.9  0.1\n\njulia> y_sim = softmax(y .* log(2f0))\n2×6 Matrix{Float32}:\n 0.333333  0.333333  0.333333  0.666667  0.333333  0.666667\n 0.666667  0.666667  0.666667  0.333333  0.666667  0.333333\n\njulia> y_dis = vcat(y_sim[2,:]', y_sim[1,:]')\n2×6 Matrix{Float32}:\n 0.666667  0.666667  0.666667  0.333333  0.666667  0.333333\n 0.333333  0.333333  0.333333  0.666667  0.333333  0.666667\n\njulia> Flux.crossentropy(y_sim, y) < Flux.crossentropy(y_sim, y_smoothed)\ntrue\n\njulia> Flux.crossentropy(y_dis, y) > Flux.crossentropy(y_dis, y_smoothed)\ntrue\n"},{"doctype":"documentation","id":"references/Flux.Dropout","title":"Dropout","text":" Dropout(p; dims=:, rng = rng_from_array())\n Dropout layer. In the forward pass, apply the  Flux.dropout  function on the input. To apply dropout along certain dimension(s), specify the  dims  keyword. e.g.  Dropout(p; dims = 3)  will randomly zero out entire channels on WHCN input (also called 2D dropout). Specify  rng  to use a custom RNG instead of the default. Custom RNGs are only supported on the CPU. Does nothing to the input once  Flux.testmode!  is  true ."},{"doctype":"documentation","id":"references/Flux.MeanPool","title":"MeanPool","text":" MeanPool(window::NTuple; pad=0, stride=window)\n Mean pooling layer, averaging all pixels in a block of size  window . Expects as input an array with  ndims(x) == N+2 , i.e. channel and batch dimensions, after the  N  feature dimensions, where  N = length(window) . By default the window size is also the stride in each dimension. The keyword  pad  accepts the same options as for the  Conv  layer, including  SamePad() . See also  Conv ,  MaxPool ,  AdaptiveMeanPool . Examples julia> xs = rand(Float32, 100, 100, 3, 50);\n\njulia> m = Chain(Conv((5,5), 3 => 7), MeanPool((5,5), pad=SamePad()))\nChain(\n  Conv((5, 5), 3 => 7),                 # 532 parameters\n  MeanPool((5, 5), pad=2),\n)\n\njulia> m[1](xs) |> size\n(96, 96, 7, 50)\n\njulia> m(xs) |> size\n(20, 20, 7, 50)\n"},{"doctype":"documentation","id":"references/DataAugmentation.AdjustBrightness","title":"AdjustBrightness","text":" AdjustBrightness(δ = 0.2)\nAdjustBrightness(distribution)\n Adjust the brightness of an image by a factor chosen uniformly from  f ∈ [1-δ, 1+δ]  by multiplying each color channel by  f . You can also pass any  Distributions.Sampleable  from which the factor is selected. Example using   DataAugmentation ,   TestImages \n \n item   =   Image ( testimage ( \" lighthouse \" ) ) \n tfm   =   AdjustBrightness ( 0.2 ) \n titems   =   [ apply ( tfm ,   item )   for   _   in   1 : 8 ] \n showgrid ( titems ;   ncol   =   4 ,   npad   =   16 )"},{"doctype":"documentation","id":"references/FastAI.getgroup","title":"getgroup","text":""},{"doctype":"documentation","id":"references/DataAugmentation.ItemWrapper","title":"ItemWrapper","text":" abstract type ItemWrapper{Item}\n"},{"doctype":"documentation","id":"references/FastAI.methoddataset","title":"methoddataset","text":""},{"doctype":"documentation","id":"references/Flux.Losses.tversky_loss","title":"tversky_loss","text":" tversky_loss(ŷ, y; β = 0.7)\n Return the  Tversky loss . Used with imbalanced data to give more weight to false negatives. Larger β weigh recall more than precision (by placing more emphasis on false negatives) Calculated as: 1 - sum(|y .* ŷ| + 1) / (sum(y .* ŷ + β*(1 .- y) .* ŷ + (1 - β) y  .  (1 .- ŷ)) + 1)"},{"doctype":"documentation","id":"references/FastAI.Tabular._emb_sz_rule","title":"_emb_sz_rule","text":" _emb_sz_rule(n_cat)\n Compute an embedding size corresponding to the number of classes for a categorical variable using the rule of thumb present in python fastai. (see https://github.com/fastai/fastai/blob/2742fe844573d06e700f869839fb9ec5f3a9bca9/fastai/tabular/model.py#L12)"},{"doctype":"documentation","id":"references/Flux.convfilter","title":"convfilter","text":" convfilter(filter::Tuple, in => out[; init = glorot_uniform])\n Constructs a standard convolutional weight matrix with given  filter  and channels from  in  to  out . Accepts the keyword  init  (default:  glorot_uniform ) to control the sampling distribution."},{"doctype":"documentation","id":"references/Flux.conv_reshape_bias","title":"conv_reshape_bias","text":""},{"doctype":"documentation","id":"references/FluxTraining.@unpack_History","title":"@unpack_History","text":""},{"doctype":"documentation","id":"references/Flux._track_stats!","title":"_track_stats!","text":""},{"doctype":"documentation","id":"references/FluxTraining.FrequencyThrottle","title":"FrequencyThrottle","text":""},{"doctype":"document","id":"documents/docs/glossary.md","title":"Glossary","text":" Glossary Terms commonly used in  FastAI . jl . Type abbreviations In many docstrings, generic types are abbreviated with the following symbols. Many of these refer to a learning task; the context should make clear which task is meant. DC{T} : A  data container  of type T, meaning a type that implements the data container interface  getobs  and  nobs  where  getobs : (DC{T}, Int) -> Int , that is, each observation is of type  T . I : Type of the unprocessed input in the context of a task. T : Type of the target variable. X : Type of the processed input. This is fed into a  model , though it may be batched beforehand.  Xs  represents a batch of processed inputs. Y : Type of the model output.  Ys  represents a batch of model outputs. model / M : A learnable mapping  M : (X,) -> Y  or  M : (Xs,) -> Ys . It predicts an encoded target from an encoded input. The learnable part of a learning task. Some examples of these in use: LearningTask  is a concrete approach to learning to predict  T  from  I  by using the encoded representations  X  and  Y . encodeinput : (task, context, I) -> X  encodes an input so that a prediction can be made by a model. A task dataset is a  DC{(I, T)} , i.e. a data container where each observation is a 2-tuple of an input and a target. Definitions Data container A data structure that is used to load a number of data observations separately and lazily. It defines how many observations it holds with  nobs  and how to load a single observation with  getobs . Learning task An instance of  DLPipelines.LearningTask . A concrete approach to solving a learning task. Encapsulates the logic and configuration for processing data to train a model and make predictions. See the DLPipelines.jl documentation for more information. Task data container / dataset DC{(I, T)} . A data container containing pairs of inputs and targets. Used in  taskdataset ,  taskdataloaders  and  evaluate ."},{"doctype":"documentation","id":"references/FluxTraining.SanityCheck","title":"SanityCheck","text":" SanityCheck([checks; usedefault = true])\n Callback that runs sanity  Check s when the  Learner  is initialized. If  usedefault  is  true , it will run all checks in FluxTraining.CHECKS in addition to the ones you pass in."},{"doctype":"documentation","id":"references/FastAI.checkblock","title":"checkblock","text":" checkblock(block, obs)\ncheckblock(blocks, obss)\n Check whether  obs  is compatible with  block , returning a  Bool . Examples checkblock ( Image { 2 } ( ) ,   rand ( RGB ,   16 ,   16 ) )   ==   true checkblock ( \n     ( Image { 2 } ( ) ,          Label ( [ \" cat \" ,   \" dog \" ] ) ) , \n     ( rand ( RGB ,   16 ,   16 ) ,   \" cat \"                  ) , \n )   ==   true Extending An implementation of  checkblock  should be as specific as possible. The default method returns  false , so you only need to implement methods for valid types and return  true ."},{"doctype":"documentation","id":"references/DataAugmentation.FromCenter","title":"FromCenter","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Bounds","title":"Bounds","text":""},{"doctype":"documentation","id":"references/FastAI.encodestate","title":"encodestate","text":""},{"doctype":"documentation","id":"references/FluxTraining.ToGPU","title":"ToGPU","text":" ToGPU()\n Callback that moves model and batch data to the GPU during training. Convenience for  ToDevice (Flux.gpu) ."},{"doctype":"documentation","id":"references/FastAI.loadmethodmodel","title":"loadmethodmodel","text":""},{"doctype":"documentation","id":"references/FastAI.defaultgrouper","title":"defaultgrouper","text":""},{"doctype":"documentation","id":"references/FluxTraining.on","title":"on","text":" on(event::Event, phase::Phase, callback::AbstractCallback, learner)\n Handle  event  with  Callback   callback . By default, this event handler does nothing for a callback. To see events which an  AbstractCallback  handles, use methods(Training.on, (Any, Any, MyCallbackType, Any) Extending You can add event handlers to  Callback s by implementing a method for  on . See also  Callback  and  custom callbacks . A method of  on  should  always  dispatch on the callback type, i.e. on(event, phase, cb::MyCallback, learner) . It may also dispatch on specific Event s and  Phase . It should not dispatch on a specific type for learner ."},{"doctype":"documentation","id":"references/DataAugmentation.showitem!","title":"showitem!","text":" showitem!(item)\n Visualize  item . Should return an image."},{"doctype":"documentation","id":"references/DataAugmentation.centered","title":"centered","text":" centered(P, bounds)\n Transform  P  so that is applied around the center of  bounds instead of the origin"},{"doctype":"documentation","id":"references/FastAI.defaultdataregistry","title":"defaultdataregistry","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.initdatadeps","title":"initdatadeps","text":""},{"doctype":"documentation","id":"references/FastAI.showsample","title":"showsample","text":" showsample([backend], task, sample)\n Show an unprocessed  sample  for  LearningTask   task  to backend:: ShowBackend . Examples data ,   blocks   =   loaddataset ( \" imagenette2-160 \" ,   ( Image ,   Label ) ) \n task   =   ImageClassificationSingle ( data ) \n sample   =   getobs ( data ,   1 ) \n showsample ( task ,   sample )    # select backend automatically \n showsample ( ShowText ( ) ,   task ,   sample )"},{"doctype":"documentation","id":"references/DataAugmentation.Crop","title":"Crop","text":""},{"doctype":"documentation","id":"references/Flux.Losses.binarycrossentropy","title":"binarycrossentropy","text":" binarycrossentropy(ŷ, y; agg = mean, ϵ = eps(ŷ))\n Return the binary cross-entropy loss, computed as agg(@.(-y * log(ŷ + ϵ) - (1 - y) * log(1 - ŷ + ϵ)))\n Where typically, the prediction  ŷ  is given by the output of a  sigmoid  activation. The  ϵ  term is included to avoid infinity. Using  logitbinarycrossentropy  is recomended over  binarycrossentropy  for numerical stability. Use  label_smoothing  to smooth the  y  value as preprocessing before computing the loss. See also:  crossentropy ,  logitcrossentropy . Examples julia> y_bin = Bool[1,0,1]\n3-element Vector{Bool}:\n 1\n 0\n 1\n\njulia> y_prob = softmax(reshape(vcat(1:3, 3:5), 2, 3) .* 1f0)\n2×3 Matrix{Float32}:\n 0.268941  0.5  0.268941\n 0.731059  0.5  0.731059\n\njulia> Flux.binarycrossentropy(y_prob[2,:], y_bin)\n0.43989f0\n\njulia> all(p -> 0 < p < 1, y_prob[2,:])  # else DomainError\ntrue\n\njulia> y_hot = Flux.onehotbatch(y_bin, 0:1)\n2×3 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n ⋅  1  ⋅\n 1  ⋅  1\n\njulia> Flux.crossentropy(y_prob, y_hot)\n0.43989f0\n"},{"doctype":"documentation","id":"references/DataAugmentation.denormalize","title":"denormalize","text":""},{"doctype":"documentation","id":"references/Flux.NilNumber.nil","title":"nil","text":""},{"doctype":"documentation","id":"references/FluxTraining.testbatch","title":"testbatch","text":""},{"doctype":"documentation","id":"references/Flux.multigate","title":"multigate","text":""},{"doctype":"documentation","id":"references/DataAugmentation.OneOf","title":"OneOf","text":" OneOf(tfms)\nOneOf(tfms, ps)\n Apply one of  tfms  selected randomly with probability  ps  each or uniformly chosen if no  ps  is given."},{"doctype":"documentation","id":"references/FastAI.Datasets.matches","title":"matches","text":""},{"doctype":"documentation","id":"references/FastAI.mockmodel","title":"mockmodel","text":" mockmodel(xblock, ŷblock)\nmockmodel(task::AbstractBlockTask)\n Create a fake model that maps batches of block  xblock  to batches of block ŷblock . Useful for testing. mockmodel(task)\n Generate a  model  compatible with  task  for testing."},{"doctype":"documentation","id":"references/Flux.NilNumber","title":"NilNumber","text":""},{"doctype":"documentation","id":"references/DataAugmentation.TabularItem","title":"TabularItem","text":""},{"doctype":"documentation","id":"references/FastAI.decodestate","title":"decodestate","text":""},{"doctype":"documentation","id":"references/FastAI.setschedules!","title":"setschedules!","text":" setschedules!(learner, phase, schedules...)\n Set  schedules  on  learner ’s  Scheduler  callback so that training resumes from there. If  learner  does not have a  Scheduler  callback yet, adds it. fit! ( learner ,   1 ) \n setschedules! ( learner ,   onecycle ( 1 ,   0.01 ) ) \n fit! ( learner ,   1 )"},{"doctype":"documentation","id":"references/FastAI.Vision.ImageSegmentationFolders","title":"ImageSegmentationFolders","text":" ImageSegmentationFolders(; imagefolder=\"images\", maskfolder=\"labels\", labelfile=\"codes.txt\")\n Dataset recipe for loading 2D image segmentation datasets from a common format where images and masks are stored as images in two different subfolders <root>/<imagefolder>  and  <root>/<maskfolder> The class labels should be in a newline-delimited file  <root>/<labelfile> ."},{"doctype":"document","id":"documents/README.md","title":"FastAI","text":" FastAI Docs:  Dev FastAI.jl is inspired by  fastai , and is a repository of best practices for deep learning in Julia. Its goal is to easily enable creating state-of-the-art models. FastAI enables the design, training, and delivery of deep learning models that compete with the best in class, using few lines of code. Install with using   Pkg \n Pkg . add ( \" FastAI \" ) or try it out with this  Google Colab template . As an example, here is how to train an image classification model: using   FastAI \n data ,   blocks   =   loaddataset ( \" imagenette2-160 \" ,   ( Image ,   Label ) ) \n task   =   ImageClassificationSingle ( blocks ) \n learner   =   tasklearner ( task ,   data ,   callbacks = [ ToGPU ( ) ] ) \n fitonecycle! ( learner ,   10 ) \n showoutputs ( task ,   learner ) Please read  the documentation  for more information and see the  setup instructions ."},{"doctype":"documentation","id":"references/Flux.Losses.∇ctc_loss","title":"∇ctc_loss","text":""},{"doctype":"documentation","id":"references/Flux.Upsample","title":"Upsample","text":" Upsample(mode = :nearest; [scale, size]) \nUpsample(scale, mode = :nearest)  \n An upsampling layer. One of two keywords must be given: If  scale  is a number, this applies to all but the last two dimensions (channel and batch) of the input. It may also be a tuple, to control dimensions individually. Alternatively, keyword size  accepts a tuple, to directly specify the leading dimensions of the output. Currently supported upsampling  mode s and corresponding NNlib’s methods are: :nearest  ->  NNlib.upsample_nearest :bilinear  ->  NNlib.upsample_bilinear :trilinear  ->  NNlib.upsample_trilinear Examples julia> m = Upsample(scale = (2, 3))\nUpsample(:nearest, scale = (2, 3))\n\njulia> m(ones(2, 2, 1, 1)) |> size\n(4, 6, 1, 1)\n\njulia> m = Upsample(:bilinear, size = (4, 5))\nUpsample(:bilinear, size = (4, 5))\n\njulia> m(ones(2, 2, 1, 1)) |> size\n(4, 5, 1, 1)\n"},{"doctype":"document","id":"documents/docs/background/datapipelines.md","title":"Performant data pipelines","text":" Performant data pipelines Bottlenecks in data pipelines and how to measure and fix them When training large deep learning models on a GPU we clearly want wait as short as possible for the training to complete. The hardware bottleneck is usually the GPU power you have available to you. This means that data pipelines need to be fast enough to keep the GPU at 100% utilization, that is, keep it from “starving”. Reducing the time the GPU has to wait for the next batch of data directly lowers the training time until the GPU is fully utilized. There are other ways to reduce training time like using hyperparameter schedules and different optimizers for faster convergence, but we’ll only talk about improving GPU utilization here. Reasons for low GPU utilization The main cause of low GPU utilization is that the next batch of data is not available after a training step and the GPU has to wait. This means that in order to get full GPU utilization, loading a batch must not take longer than a training step; and the data must be loaded in the background, so that it is ready the moment the GPU needs it. These issues can be addressed by using worker threads to load multiple batches in parallel keeping the primary thread free; and reducing the time it takes to load a single batch FastAI.jl by default uses  DataLoader  from the  DataLoaders . jl  package which addresses points 1. and 2. For those familiar with PyTorch, it closely resembles  torch.utils.data.DataLoader . It also efficiently collates the data by reusing a buffer where supported. We can measure the large performance difference by comparing a naive sequential data iterator with  eachobsparallel , the data iterator that  DataLoader  uses. using   DataLoaders :   batchviewcollated \n using   FastAI \n using   FastAI . Datasets \n \n data   =   loadtaskdata ( datasetpath ( \" imagenette2-320 \" ) ,   ImageClassification ) \n task   =   ImageClassification ( Datasets . getclassesclassification ( \" imagenette2-320 \" ) ,   ( 224 ,   224 ) ) \n \n # maps data processing over `data` \n taskdata   =   taskdataset ( data ,   task ,   Training ( ) ) \n \n # creates a data container of collated batches \n batchdata   =   batchviewcollated ( taskdata ,   16 ) \n \n NBATCHES   =   200 \n \n # sequential data iterator \n @ time   for   ( i ,   batch )   in   enumerate ( getobs ( batchdata ,   i )   for   i   in   1 : nobs ( batchdata ) ) \n     i   !=   NBATCHES   ||   break \n end \n \n # parallel data iterator \n @ time   for   ( i ,   batch )   in   enumerate ( eachobsparallel ( batchdata ) ) \n     i   !=   NBATCHES   ||   break \n end Running each timer twice to forego compilation time, the sequential iterator takes 20 seconds while the parallel iterator using 11 background threads only takes 2.5 seconds. This certainly isn’t a proper benchmark, but it shows the performance can be improved by an order of magnitude with no effort. Beside increasing the amount of compute available with worker threads as above, the data loading performance can also be improved by reducing the time it takes to load a single batch. Since a batch is made up of some number of observations, this usually boils down to reducing the loading time of a single observation. If you’re using the  LearningTask  API, this can be further broken down into the loading and encoding part. Measuring performance So how do you know if your GPU is underutilized? If it isn’t, then improving data pipeline performance won’t help you at all! One way to check this is to start training and run  > watch -n 0.1 nvidia-smi  in a terminal which displays and refreshs GPU stats every 1/10th of a second. If  GPU-Util  stays between 90% and 99%, you’re good! If that’s not the case, you might see it frantically jumping up and down. We can get a better estimate of how much training time can be sped up by running the following experiment: Load one batch and run  n  optimization steps on this batch. The time this takes corresponds to the training time when the GPU does not have to wait for data to be available. Next take your data iterator and time iterating over the first  n  batches  without  an optimization step. The speed of the complete training loop (data loading and optimization) will be around the maximum of either measurement. Roughly speaking, if 1. takes 100 seconds and 2. takes 200 seconds, you know that you can speed up training by about a factor of 2 if you reduce data loading time by half, after which the GPU will become the bottleneck. using   FastAI \n using   FastAI . Datasets \n using   FluxTraining :   step! \n \n data   =   loaddataset ( \" imagenette2-320 \" ,   ( Image ,   Label ) ) \n task   =   ImageClassificationSingle ( blocks ) \n learner   =   tasklearner ( task ,   data ) \n \n NBATCHES   =   100 \n \n # Measure GPU time \n batch   =   gpu ( first ( learner . data . training ) ) \n learner . model   =   gpu ( model ) \n @ time   for   i   in   1 : NBATCHES \n     step! ( learner ,   batch ,   TrainingPhase ( ) ) \n end \n \n # Measure data loading time \n @ time   for   ( i ,   batch )   in   zip ( learner . data . training ,   1 : NBATCHES ) \n end Again, make sure to run each measurement twice so you don’t include the compilation time. To find performance bottlenecks in the loading of each observation, you’ll want to compare the time it takes to load an observation of the task data container and the time it takes to encode that observation. using   BenchmarkTools \n using   FastAI \n using   FastAI . Datasets \n \n # Since loading times can vary per observation, we'll average the measurements over multiple observations \n N   =   10 \n data   =   datasubset ( data ,   1 : N ) \n \n # Time it takes to load an `(image, class)` observation \n @ btime   for   i   in   1 : N \n     getobs ( data ,   i ) \n end \n \n \n # Time it takes to encode an `(image, class)` observation into `(x, y)` \n obss   =   [ getobs ( data ,   i )   for   i   in   1 : N ] \n @ btime   for   i   in   1 : N \n     encodesample ( task ,   Training ( ) ,   obss [ i ] ) \n end This will give you a pretty good idea of where the performance bottleneck is. Note that the encoding performance is often dependent of the task configuration. If we used  ImageClassification  with input size  (64, 64)  it would be much faster. Improving performance So, you’ve identified the data pipeline as a performance bottleneck. What now? Before anything else, make sure you’re doing the following: Use  DataLoaders.DataLoader  as a data iterator. If you’re using  taskdataloaders  or  tasklearner , this is already the case. Start Julia with multiple threads by specifying the  -t n / -t auto  flag when starting Julia. If it is successful,  Threads.nthreads()  should be larger than  1 . If the data loading is still slowing down training, you’ll probably have to speed up the loading of each observation. As mentioned above, this can be broken down into observation loading and encoding. The exact strategy will depend on your use case, but here are some examples. Reduce loading time of image datasets by presizing For many computer vision tasks, you will resize and crop images to a specific size during training for GPU performance reasons. If the images themselves are large, loading them from disk itself can take some time. If your dataset consists of 1920x1080 resolution images but you’re resizing them to 256x256 during training, you’re wasting a lot of time loading the large images.  Presizing  means saving resized versions of each image to disk once, and then loading these smaller versions during training. We can see the performance difference using ImageNette since it comes in 3 sizes: original, 360px and 180px. data_orig ,   _   =   loaddataset ( \" imagenette2 \" ,   ( Image ,   Label ) ) \n @ time   eachobsparallel ( data_orig ,   buffered   =   false ) \n \n data_320px ,   _   =   loaddataset ( \" imagenette2-320 \" ,   ( Image ,   Label ) ) \n @ time   eachobsparallel ( data_320px ,   buffered   =   false ) \n \n data_160px ,   _   =   loaddataset ( \" imagenette2-160 \" ,   ( Image ,   Label ) ) \n @ time   eachobsparallel ( data_160px ,   buffered   =   false ) Reducing allocations with inplace operations When implementing the  LearningTask  interface, you have the option to implement  encode!(buf, task, context, sample) , an inplace version of  encode  that reuses a buffer to avoid allocations. Reducing allocations often speeds up the encoding step and can also reduce the frequency of garbage collector pauses during training which can reduce GPU utilization. Using efficient data augmentation Many kinds of augmentation can be composed efficiently. A prime example of this are image transformations like resizing, scaling and cropping which are powered by  DataAugmentation . jl . See  its documentation  to find out how to implement efficient, composable data transformations."},{"doctype":"documentation","id":"references/FastAI.Tabular._get_emb_sz","title":"_get_emb_sz","text":" _get_emb_sz(cardinalities::AbstractVector, [size_overrides::AbstractVector])\n Given a vector of  cardinalities  of each categorical column (i.e. each element of  cardinalities  is the number of classes in that categorical column), compute the output embedding size according to  _emb_sz_rule . Return a vector of tuples where each element is  (in_size, out_size)  for an embedding layer. Keyword arguments size_overrides : A collection of integers (or  nothing  to skip override) where the value present at any index will be used to as the output embedding size for that column."},{"doctype":"documentation","id":"references/DataAugmentation.AdjustContrast","title":"AdjustContrast","text":" AdjustContrast(factor = 0.2)\nAdjustContrast(distribution)\n Adjust the contrast of an image by a factor chosen uniformly from  f ∈ [1-δ, 1+δ] . Pixels  c  are transformed  c + μ*(1-f)  where  μ  is the mean color of the image. You can also pass any  Distributions.Sampleable  from which the factor is selected. Example using   DataAugmentation ,   TestImages \n \n item   =   Image ( testimage ( \" lighthouse \" ) ) \n tfm   =   AdjustContrast ( 0.2 ) \n titems   =   [ apply ( tfm ,   item )   for   _   in   1 : 8 ] \n showgrid ( titems ;   ncol   =   4 ,   npad   =   16 )"},{"doctype":"documentation","id":"references/FastAI.OneHot","title":"OneHot","text":" OneHot()\nOneHot(T, threshold)\n Encoding  that turns categorical labels into one-hot encoded arrays of type  T . Encodes       `Mask{N, U}` -> `OneHotTensor{N, T}`\n`LabelMulti{N, U}` -> `OneHotTensorMulti{N, T}`\n        `Label{U}` -> `OneHotTensor{N, T}`\n"},{"doctype":"documentation","id":"references/FluxTraining.defaultcallbacks","title":"defaultcallbacks","text":""},{"doctype":"documentation","id":"references/FluxTraining.NoConflict","title":"NoConflict","text":" abstract type NoConflict <: ConflictResolution\n Return from  resolveconflict  to indicate that, while the callbacks modify the same state, they can be used together without any problems."},{"doctype":"documentation","id":"references/FastAI.decodeypred!","title":"decodeypred!","text":""},{"doctype":"documentation","id":"references/FastAI.Vision._segmentationloss","title":"_segmentationloss","text":""},{"doctype":"documentation","id":"references/FluxTraining.Learner","title":"Learner","text":" Learner(model, data, optimizer, lossfn, [callbacks...; kwargs...])\n Holds and coordinates all state of the training.  model  is trained by optimizing  lossfn  with  optimizer  on  data . Arguments model : A Flux.jl model or a  NamedTuple  of models. data : Data iterators. A 2-tuple will be treated as  (trainingdataiter, validdataiter) . You can also pass in an empty tuple  ()  and use the  epoch!  method with a dataiter  as third argument. A data iterator is an iterable over batches. For regular supervised training, each batch should be a tuple  (xs, ys) . lossfn : Function with signature  lossfn(model(x), y) -> Number optimizer callbacks... : Any other unnamed arguments are callbacks Keyword arguments usedefaultcallbacks = true : Whether to add some basic callbacks. Included are  Metrics ,  Recorder ,  ProgressPrinter , StopOnNaNLoss , and  MetricsPrinter . cbrunner = LinearRunner() : Callback runner to use. Fields (Use this as a reference when implementing callbacks) model ,  optimizer , and  lossfn  are stored as passed in data  is a  PropDict  of data iterators, usually  :training  and  :validation . params : An instance of  model ’s parameters of type  Flux.Params . If  model  is a  NamedTuple , then  params  is a  NamedTuple  as well. step:: PropDict : State of the last step. Contents depend on the last run Phase . cbstate:: PropDict : Special state container that callbacks can save state to for other callbacks. Its keys depend on what callbacks are being used. See the  custom callbacks guide for more info."},{"doctype":"document","id":"documents/docs/data_containers.md","title":"Data containers","text":" Data containers This tutorial explains what data containers are, how they are used in FastAI . jl and how to create your own .  You are encouraged to follow along in a REPL or a Jupyter notebook and explore the code .  You will find small exercises at the end of some sections to deepen your understanding . Introduction In the  quickstart  section, you have already come in contact with data containers. The following code was used to load a data container for image classification: ENV [ \" DATADEPS_ALWAYS_ACCEPT \" ]   =   \" true \" using   FastAI \n import   FastAI :   Image \n data ,   _   =   loaddataset ( \" imagenette2-160 \" ,   ( Image ,   Label ) ) A data container is any type that holds observations of data and allows us to load them with  getobs  and query the number of observations with  nobs . In this case, each observation is a tuple of an image and the corresponding class; after all, we want to use it for image classification. image ,   class   =   obs   =   getobs ( data ,   1 ) \n @ show   class \n image nobs ( data ) loaddataset  makes it easy to a load a data container that is compatible with some block types, but to get a better feel for what it does, let’s look under the hood by creating the same data container using some mid-level APIs. Creating data containers from files Before we recreate the data container,  datasetpath  downloads a dataset and returns the path to the extracted files. dir   =   datasetpath ( \" imagenette2-160 \" ) Now we’ll start with  FileDataset  which creates a data container (here a  Vector ) of files given a path. We’ll use the path of the downloaded dataset: files   =   FileDataset ( dir ) files  is a data container where each observation is a path to a file. We’ll confirm that using  getobs : p   =   getobs ( files ,   100 ) Next we need to load an image and the corresponding class from the path. If you have a look at the folder structure of  dir  you can see that the parent folder of each file gives the name of class. So we can use the following function to load the  (image, class)  pair from a path: function   loadimageclass ( p ) \n     return   ( \n         loadfile ( p ) , \n         pathname ( pathparent ( p ) ) , \n     ) \n end \n \n image ,   class   =   loadimageclass ( p ) \n @ show   class \n image Finally, we use  mapobs  to lazily transform each observation and have a data container ready to be used for training an image classifier. data   =   mapobs ( loadimageclass ,   files ) ; Exercises Using  mapobs  and  loadfile , create a data container where every observation is only an image. Change the above code to run on a different dataset from the list in  Datasets.DATASETS_IMAGECLASSIFICATION . Splitting a data container into subsets Until now, we’ve only created a single data container containing all observations in a dataset. In practice, though, you’ll want to have at least a training and validation split. The easiest way to get these is to randomly split your data container into two parts. Here we split  data  into 80% training and 20% validation data. Note the use of  shuffleobs  to make sure each split has approximately the same class distribution. traindata ,   valdata   =   splitobs ( shuffleobs ( data ) ,   at   =   0.8 ) ; This is great for experimenting, but where possible you will want to use the official training/validation split for a dataset. Consider the image classification dataset folder structure: - $dir\n    - train\n        - class1\n            - image1.jpg\n            - image2.jpg\n            - ...\n        - class2\n        - ...\n    - valid\n        - class1\n        - class2\n        - ...\n As you can see, the grandparent folder of each image indicates which split it is a part of.  groupobs  allows us to partition a data container using a function. Let’s use it to split  filedata  based on the name of the grandparent directory. (We can’t reuse  data  for this since it no longer carries the file information.) datagroups   =   groupobs ( files )   do   p \n     pathname ( pathparent ( pathparent ( p ) ) )    # equivalent to `grandparentname(p)` \n end \n trainfiles ,   validfiles   =   datagroups [ \" train \" ] ,   datagroups [ \" val \" ] Using this official split, it will be easier to compare the performance of your results with those of others’. Dataset recipes We saw above how different image classification datasets can be loaded with the same logic as long as they are in a common format. To encapsulate the logic for loading common dataset formats, FastAI.jl has  DatasetRecipe s. When we used  finddatasets  in the  discovery tutorial , it returned pairs of a dataset name and a  DatasetRecipe . For example,  \"imagenette2-160\"  has an associated  ImageFolders  recipe and we can load it using [ loadrecipe ] and the path to the downloaded dataset: name ,   recipe   =   finddatasets ( blocks = ( Image ,   Label ) ,   name = \" imagenette2-160 \" ) [ 1 ] \n data ,   blocks   =   loadrecipe ( recipe ,   datasetpath ( name ) ) These recipes also take care of loading the data block information for the dataset. Read the  discovery tutorial  to find out more about that."},{"doctype":"documentation","id":"references/Flux.OneHotLike","title":"OneHotLike","text":""},{"doctype":"documentation","id":"references/FastAI.PropagateAlways","title":"PropagateAlways","text":""},{"doctype":"documentation","id":"references/FluxTraining.runchecks","title":"runchecks","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.Models.runtests","title":"runtests","text":""},{"doctype":"documentation","id":"references/FluxTraining.TimeThrottle","title":"TimeThrottle","text":""},{"doctype":"documentation","id":"references/FluxTraining.Phases.TrainingPhase","title":"TrainingPhase","text":" TrainingPhase() <: AbstractTrainingPhase\n A regular training phase for supervised learning. It iterates over batches in  learner.data.training  and updates the model parameters using  learner.optim  after calculating the gradients. Throws the following events in this order: EpochBegin  when an epoch starts, StepBegin  when a step starts, LossBegin  after the forward pass but before loss calculation, BackwardBegin  after loss calculation but before backward pass, BackwardEnd  after the bacward pass but before the optimization step, StepEnd  when a step ends; and EpochEnd  when an epoch ends It writes the following step state to  learner.state , grouped by the event from which on it is available. StepBegin : xs  and  ys : encoded input and target (batch) LossBegin : ŷs : model output BackwardBegin : loss : loss BackwardEnd : grads : calculated gradients"},{"doctype":"documentation","id":"references/FastAI.fillblock","title":"fillblock","text":" fillblock(inblocks, outblocks)\n Replaces all  nothing s in outblocks with the corresponding block in  inblocks . outblocks  may be obtained by"},{"doctype":"documentation","id":"references/Flux","title":"Flux","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.InvDecay","title":"InvDecay","text":" InvDecay(γ = 0.001)\n Apply inverse time decay to an optimiser, so that the effective step size at iteration  n  is  eta / (1 + γ * n)  where  eta  is the initial step size. The wrapped optimiser’s step size is not modified. See also the  Scheduling Optimisers  section of the docs for more general scheduling techniques. Examples InvDecay  is typically composed  with other optimizers as the last transformation of the gradient: # Inverse decay of the learning rate \n # with starting value 0.001 and decay coefficient 0.01. \n opt   =   Optimiser ( Adam ( 1f-3 ) ,   InvDecay ( 1f-2 ) )"},{"doctype":"documentation","id":"references/DataAugmentation.ScaleFixed","title":"ScaleFixed","text":" ScaleFixed(sizes)\n Projective transformation that scales sides to  sizes , disregarding aspect ratio. See also  ScaleKeepAspect ."},{"doctype":"documentation","id":"references/DataAugmentation.Zoom","title":"Zoom","text":" Zoom(scales = (1, 1.2)) <: ProjectiveTransform\nZoom(distribution)\n Zoom into an item by a factor chosen from the interval  scales or  distribution ."},{"doctype":"documentation","id":"references/Flux.AdaptiveMaxPool","title":"AdaptiveMaxPool","text":" AdaptiveMaxPool(out::NTuple)\n Adaptive max pooling layer. Calculates the necessary window size such that its output has  size(y)[1:N] == out . Expects as input an array with  ndims(x) == N+2 , i.e. channel and batch dimensions, after the  N  feature dimensions, where  N = length(out) . See also  MaxPool ,  AdaptiveMeanPool . Examples julia> xs = rand(Float32, 100, 100, 3, 50);  # batch of 50 RGB images\n\njulia> AdaptiveMaxPool((25, 25))(xs) |> size\n(25, 25, 3, 50)\n\njulia> MaxPool((4,4))(xs) ≈ AdaptiveMaxPool((25, 25))(xs)\ntrue\n"},{"doctype":"documentation","id":"references/Flux.Optimise.ExpDecay","title":"ExpDecay","text":" ExpDecay(η = 0.001, decay = 0.1, decay_step = 1000, clip = 1e-4, start = 1)\n Discount the learning rate  η  by the factor  decay  every  decay_step  steps till a minimum of  clip . Parameters Learning rate ( η ): Amount by which gradients are discounted before updating the weights. decay : Factor by which the learning rate is discounted. decay_step : Schedule decay operations by setting the number of steps between two decay operations. clip : Minimum value of learning rate. ‘start’: Step at which the decay starts. See also the  Scheduling Optimisers  section of the docs for more general scheduling techniques. Examples ExpDecay  is typically composed  with other optimizers as the last transformation of the gradient: opt   =   Optimiser ( ADAM ( ) ,   ExpDecay ( ) )"},{"doctype":"documentation","id":"references/FastAI.Vision._maskimage","title":"_maskimage","text":""},{"doctype":"documentation","id":"references/Flux._isactive","title":"_isactive","text":""},{"doctype":"documentation","id":"references/DataAugmentation.getprojection","title":"getprojection","text":" getprojection(tfm, bounds; randstate)\n Create a projection for an item with spatial bounds  bounds . The projection should be a  CoordinateTransformations.Transformation . See  CoordinateTransformations . jl"},{"doctype":"documentation","id":"references/DataAugmentation.adjustcontrast!","title":"adjustcontrast!","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.Descent","title":"Descent","text":" Descent(η = 0.1)\n Classic gradient descent optimiser with learning rate  η . For each parameter  p  and its gradient  δp , this runs  p -= η*δp Parameters Learning rate ( η ): Amount by which gradients are discounted before updating the weights. Examples opt   =   Descent ( ) \n \n opt   =   Descent ( 0.3 ) \n \n ps   =   Flux . params ( model ) \n \n gs   =   gradient ( ps )   do \n     loss ( x ,   y ) \n end \n \n Flux . Optimise . update! ( opt ,   ps ,   gs )"},{"doctype":"documentation","id":"references/FastAI.blocklossfn","title":"blocklossfn","text":" blocklossfn(predblock, yblock)\n Construct a loss function that compares a batch of model outputs ŷs  and encoded targets  ys  and returns a scalar loss. For example for  block = OneHotLabel(classes)  (i.e. an encoded Label(classes) ), we have blocklossfn(block, block) == Flux.Losses.logitcrossentropy ."},{"doctype":"documentation","id":"references/Flux._channels_out","title":"_channels_out","text":""},{"doctype":"documentation","id":"references/FluxTraining.Events.EpochEnd","title":"EpochEnd","text":" EpochEnd()\n Event  called at the end of an epoch."},{"doctype":"documentation","id":"references/FastAI.getencodings","title":"getencodings","text":""},{"doctype":"documentation","id":"references/Flux.early_stopping","title":"early_stopping","text":" early_stopping(f, delay; distance = -, init_score = 0, min_dist = 0)\n Return a function that internally counts by one when distance(best_score, f(...)) <= min_dist , where best_score  is the last seen best value of  f(...) . If the count is greater than or equal to  delay , the function returns  true , otherwise it returns  false . The count is reset when  distance(best_score, f(...)) > min_dist . Examples julia> loss = let l = 0\n         () -> l += 1\n       end; # pseudo loss function that returns increasing values\n\njulia> es = Flux.early_stopping(loss, 3);\n\n\njulia> Flux.@epochs 10 begin\n         es() && break\n       end\n[ Info: Epoch 1\n[ Info: Epoch 2\n[ Info: Epoch 3\n"},{"doctype":"documentation","id":"references/FastAI.Datasets.mapobs","title":"mapobs","text":" mapobs(f, data)\n Lazily map  f  over the observations in a data container  data . data   =   1 : 10 \n getobs ( data ,   8 )   ==   8 \n mdata   =   mapobs ( - ,   data ) \n getobs ( mdata ,   8 )   ==   -8 mapobs(fs, data)\n Lazily map each function in tuple  fs  over the observations in data container  data . Returns a tuple of transformed data containers. mapobs(namedfs::NamedTuple, data)\n Map a  NamedTuple  of functions over  data , turning it into a data container of  NamedTuple s. Field syntax can be used to select a column of the resulting data container. data   =   1 : 10 \n nameddata   =   mapobs ( ( x   =   sqrt ,   y   =   log ) ,   data ) \n getobs ( nameddata ,   10 )   ==   ( x   =   sqrt ( 10 ) ,   y   =   log ( 10 ) ) \n getobs ( nameddata . x ,   10 )   ==   sqrt ( 10 )"},{"doctype":"documentation","id":"references/FluxTraining.Phases","title":"Phases","text":""},{"doctype":"documentation","id":"references/Flux.Losses.xlogy","title":"xlogy","text":" xlogy(x, y)\n Return  x * log(y)  for  y > 0 , and zero when  x == 0 ."},{"doctype":"documentation","id":"references/DataAugmentation.CroppedProjectiveTransform","title":"CroppedProjectiveTransform","text":""},{"doctype":"documentation","id":"references/DataAugmentation.apply!","title":"apply!","text":" apply!(buffer::I, tfm, item::I)\n Applies  tfm  to  item , mutating the preallocated  buffer . buffer  can be obtained with  buffer = makebuffer(tfm, item) apply!(buffer, tfm::Transform, item::I; randstate) = apply(tfm, item; randstate)\n Default to  apply(tfm, item)  (non-mutating version)."},{"doctype":"documentation","id":"references/FastAI.Datasets.parentname","title":"parentname","text":""},{"doctype":"documentation","id":"references/DataAugmentation.itemfield","title":"itemfield","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.EPS","title":"EPS","text":""},{"doctype":"documentation","id":"references/FluxTraining.SmoothLoss","title":"SmoothLoss","text":""},{"doctype":"documentation","id":"references/Flux.dropout","title":"dropout","text":" dropout([rng = rng_from_array(x)], x, p; dims=:, active=true)\n The dropout function. If  active  is  true , for each input, either sets that input to  0  (with probability p ) or scales it by  1 / (1 - p) .  dims  specifies the unbroadcasted dimensions, e.g.  dims=1  applies dropout along columns and  dims=2  along rows. This is used as a regularisation, i.e. it reduces overfitting during training. If  active  is  false , it just returns the input  x . Specify  rng  for custom RNGs instead of the default RNG. Note that custom RNGs are only supported on the CPU. Warning: when using this function, you have to manually manage the activation state. Usually in fact, dropout is used while training but is deactivated in the inference phase. This can be automatically managed using the  Dropout  layer instead of the dropout  function. The  Dropout  layer is what you should use in most scenarios."},{"doctype":"documentation","id":"references/FluxTraining.Phases.Phase","title":"Phase","text":" abstract type Phase\n Abstract supertype for all phases. See  subtypes(FluxTraining.Phase) . A  Phase  is used in dispatch for training loop functions  step! and  epoch!  as well as in  Callback  handler methods  on ."},{"doctype":"documentation","id":"references/FastAI.Vision.imagedatasetstats","title":"imagedatasetstats","text":" imagedatasetstats(data, C[; parallel, progress])\n Given a data container of images  data , compute the color channel-wise means and standard deviations across all observations. Images are converted to color type C  (e.g.  RGB{N0f8} ,  Gray{N0f8} ) before statistics are calculated. If  progress = true , show a progress bar."},{"doctype":"documentation","id":"references/FluxTraining","title":"FluxTraining","text":""},{"doctype":"documentation","id":"references/FastAI.showoutputbatch","title":"showoutputbatch","text":" showoutputbatch([backend], task, outputbatch)\nshowoutputbatch([backend], task, batch, outputbatch)\n Show collated batch of outputs to  backend . If a collated batch of encoded samples batch  is also given, show them next to the outputs. See  showoutputs  if you have vectors of outputs and not collated batches."},{"doctype":"documentation","id":"references/Flux.params","title":"params","text":" params(model)\nparams(layers...)\n Given a model or specific layers from a model, create a  Params  object pointing to its trainable parameters. This can be used with the  gradient  function, see  Taking Gradients , or as input to the [ Flux.train! ](  Flux.train!) function. The behaviour of  params  on custom types can be customized using  Functor.@functor  or  Flux.trainable . Examples julia> using Flux: params\n\njulia> params(Chain(Dense(ones(2,3)), softmax))  # unpacks Flux models\nParams([[1.0 1.0 1.0; 1.0 1.0 1.0], [0.0, 0.0]])\n\njulia> bn = BatchNorm(2, relu)\nBatchNorm(2, relu)  # 4 parameters, plus 4 non-trainable\n\njulia> params(bn)  # only the trainable parameters\nParams([Float32[0.0, 0.0], Float32[1.0, 1.0]])\n\njulia> params([1, 2, 3], [4])  # one or more arrays of numbers\nParams([[1, 2, 3], [4]])\n\njulia> params([[1, 2, 3], [4]])  # unpacks array of arrays\nParams([[1, 2, 3], [4]])\n\njulia> params(1, [2 2], (alpha=[3,3,3], beta=Ref(4), gamma=sin))  # ignores scalars, unpacks NamedTuples\nParams([[2 2], [3, 3, 3]])\n"},{"doctype":"documentation","id":"references/FastAI.Datasets.loaddataset","title":"loaddataset","text":" loaddataset(name[, blocks = Any]) -> (data, blocks)\n Load dataset  name  with a recipe that matches block types blocks . The first matching recipe is selected and loaded. Examples Load a data container suitable for single-label image classification: data ,   blocks   =   loaddataset ( \" imagenette2-160 \" ,   ( Image ,   Label ) ) Load dataset with any recipe: data ,   blocks   =   loaddataset ( name )"},{"doctype":"documentation","id":"references/Flux.Optimise.AMSGrad","title":"AMSGrad","text":" AMSGrad(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = \n"},{"doctype":"documentation","id":"references/Flux.Optimise.RADAM","title":"RADAM","text":" RADAM(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = \n"},{"doctype":"documentation","id":"references/Flux.Losses.MAX_THREADS","title":"MAX_THREADS","text":""},{"doctype":"documentation","id":"references/Flux.CrossCor","title":"CrossCor","text":" CrossCor(weight::AbstractArray, [bias, activation; stride, pad, dilation])\n Constructs a layer with the given weight and bias arrays. Accepts the same keywords as the  CrossCor((4,4), 3 => 7, relu)  method. CrossCor(filter, in => out, σ=identity; stride=1, pad=0, dilation=1, [bias, init])\n Standard cross convolutional layer.  filter  is a tuple of integers specifying the size of the convolutional kernel; in  and  out  specify the number of input and output channels. Parameters are controlled by additional keywords, with defaults init=glorot_uniform  and  bias=true . See also  Conv  for more detailed description of keywords. Examples julia> xs = rand(Float32, 100, 100, 3, 50);  # a batch of 50 RGB images\n\njulia> lay = CrossCor((5,5), 3 => 6, relu; bias=false)\nCrossCor((5, 5), 3 => 6, relu, bias=false)  # 450 parameters\n\njulia> lay(xs) |> size\n(96, 96, 6, 50)\n\njulia> CrossCor((5,5), 3 => 7, stride=3, pad=(2,0))(xs) |> size\n(34, 32, 7, 50)\n"},{"doctype":"documentation","id":"references/FastAI.showsamples","title":"showsamples","text":" showsample([backend], task, sample)\n Show a vector of unprocessed  samples  for  LearningTask   task  to backend:: ShowBackend . Examples data ,   blocks   =   loaddataset ( \" imagenette2-160 \" ,   ( Image ,   Label ) ) \n task   =   ImageClassificationSingle ( data ) \n samples   =   [ getobs ( data ,   i )   for   i   in   1 : 4 ] \n showsamples ( task ,   samples )    # select backend automatically \n showsamples ( ShowText ( ) ,   task ,   samples )"},{"doctype":"documentation","id":"references/FastAI.Inference","title":"Inference","text":""},{"doctype":"documentation","id":"references/DataAugmentation.showimage!","title":"showimage!","text":""},{"doctype":"documentation","id":"references/FastAI.LearningTask","title":"LearningTask","text":" abstract type LearningTask\n Represents a concrete approach for solving a learning task. A  LearningTask  defines how data is processed encoded and decoded before and after going through a model. Extending It is recommended to use  AbstractBlockTask s like  BlockTask and  SupervisedTask  to construct tasks, but you may subtype LearningTask  for lower-level control. There is a core interface that will allow you to train models and perform inference (for supervised tasks). It consists of encodesample encodeinput decodeypred You can optionally implement additional interfaces to get support for higher-level features of the library. Training interface:  tasklossfn ,  taskmodel Testing interface:  mocksample ,  mockinput ,  mocktarget , mockmodel Batching:  shouldbatch"},{"doctype":"documentation","id":"references/Flux.ofeltype","title":"ofeltype","text":""},{"doctype":"documentation","id":"references/FluxTraining.CancelEpochException","title":"CancelEpochException","text":" CancelEpochException(message)\n Throw during fitting to cancel the currently running epoch. This prematurely ends the current epoch without throwing an error. Must be thrown inside the context of  runepoch . Examples runepoch ( learner ,   phase )   do   _ \n     for   batch   in   batches \n         step! ( learner ,   phase ,   batch ) \n         if   learner . step . loss   <   1. \n             throw ( CancelEpochException ( \" Reached target loss \" ) ) \n         end \n     end \n end"},{"doctype":"documentation","id":"references/FastAI.Tabular.TableClassificationRecipe","title":"TableClassificationRecipe","text":""},{"doctype":"documentation","id":"references/Flux.nil_input","title":"nil_input","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.DATASETS","title":"DATASETS","text":""},{"doctype":"documentation","id":"references/FastAI.Vision.augs_lighting","title":"augs_lighting","text":" augs_lighting([; intensity = 0.2, p = 0.75])\n Helper to create a set of lighting transformations for image data. With probability  p , applies  AdjustBrightness (intensity)  and AdjustContrast (intensity) ."},{"doctype":"documentation","id":"references/FastAI.makebatch","title":"makebatch","text":" makebatch(task, data, [idxs; context]) -> (xs, ys)\n Create a batch of encoded data by loading  idxs  from data container  data . Useful for inspection and as input to  showbatch . Samples are encoded in  context  which defaults to  Training ."},{"doctype":"documentation","id":"references/FastAI.decode","title":"decode","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.isimagefile","title":"isimagefile","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.maskfromimage","title":"maskfromimage","text":""},{"doctype":"documentation","id":"references/FastAI.isshowable","title":"isshowable","text":""},{"doctype":"documentation","id":"references/FastAI.encodedblockfilled","title":"encodedblockfilled","text":""},{"doctype":"documentation","id":"references/Flux._any","title":"_any","text":""},{"doctype":"documentation","id":"references/FastAI.getbatch","title":"getbatch","text":" getbatch(learner[; validation = false, n = nothing])\n Get a batch of data from  learner . Take a batch of training data by default or validation data if  validation = true . If  n  take only the first n  samples from the batch."},{"doctype":"documentation","id":"references/FastAI.AbstractBlock","title":"AbstractBlock","text":" abstract type AbstractBlock\n Abstract supertype of all blocks. You should not subtype form this, but instead from  Block  or  WrapperBlock ."},{"doctype":"documentation","id":"references/Flux.Losses.log_plus_f","title":"log_plus_f","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.ADAM","title":"ADAM","text":" ADAM(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = \n"},{"doctype":"documentation","id":"references/FastAI.loadtaskmodel","title":"loadtaskmodel","text":" loadtaskmodel(path) -> (task, model)\n Load a trained  model  along with a  task  from  path  that were saved using  savetaskmodel . JLD2 . jl  is used for serialization."},{"doctype":"documentation","id":"references/FastAI.Tabular.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.__init__","title":"__init__","text":""},{"doctype":"documentation","id":"references/Flux._onehotbatch","title":"_onehotbatch","text":""},{"doctype":"documentation","id":"references/FastAI.decodeypred","title":"decodeypred","text":""},{"doctype":"documentation","id":"references/DataAugmentation.compose","title":"compose","text":" compose(transforms...)\n Compose tranformations. Use  |>  as an alias. Defaults to creating a  Sequence  of transformations, but smarter behavior can be implemented. For example,  MapElem(f) |> MapElem(g) == MapElem(g ∘ f) ."},{"doctype":"documentation","id":"references/DataAugmentation.showkeypoint!","title":"showkeypoint!","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets._iterobs","title":"_iterobs","text":""},{"doctype":"documentation","id":"references/Flux.FluxCUDAAdaptor","title":"FluxCUDAAdaptor","text":""},{"doctype":"documentation","id":"references/Flux.Losses.compute_alpha_kernel","title":"compute_alpha_kernel","text":""},{"doctype":"documentation","id":"references/FastAI.tasklearner","title":"tasklearner","text":" tasklearner(task, traindata, validdata[; callbacks=[], kwargs...]) -> Learner\ntasklearner(task, data; pctgval = 0.2, kwargs...)\n Create a  Learner  to train a model for learning task  task  using data . Keyword arguments callbacks = [] :  Callback s to use during training. batchsize = 16 : Batch size used for the training data loader. backbone = nothing : Backbone model to construct task-specific model from using taskmodel (task, backbone) . model = nothing : Complete model to use. If given, the  backbone  argument is ignored. optimizer = ADAM() : Optimizer passed to  Learner . lossfn =  tasklossfn (task) : Loss function passed to  Learner . Any other keyword arguments will be passed to  taskdataloaders . Examples Full example: data ,   blocks   =   loaddataset ( \" imagenette2-160 \" ,   ( Image ,   Label ) ) \n task   =   ImageClassificationSingle ( blocks ) \n learner   =   tasklearner ( task ,   data ) \n fitonecycle! ( learner ,   10 ) Custom training and validation split: learner   =   tasklearner ( task ,   traindata ,   validdata ) Using callbacks: learner   =   tasklearner ( task ,   data ;   callbacks = [ \n     ToGPU ( ) ,   Checkpointer ( ) ,   LogMetrics ( TensorboardBackend ( ) ) \n ] )"},{"doctype":"documentation","id":"references/DataAugmentation.getrandstate","title":"getrandstate","text":" getrandstate(transform)\n Generates random state for stochastic transformations. Calling  apply(tfm, item)  is equivalent to apply(tfm, item; randstate = getrandstate(tfm)) . It defaults to  nothing , so you it only needs to be implemented for stochastic  Transform s."},{"doctype":"documentation","id":"references/FastAI.Datasets.finddatasets","title":"finddatasets","text":" finddatasets([registry]; name=nothing, blocks=Any)\n Find preconfigured dataset recipes for datasets that match block types  blocks  in all data sources (if  name == nothing ) or dataset source with  name .  blocks  can be given as a type or a nested tuple of block types. Return a vector of  Pair s  datasetname => recipe Examples Loading a result datasetname ,   recipe   =   finddatasets ( blocks = ( Image ,   Label ) ) [ 1 ] \n data ,   blocks   =   loadrecipe ( recipe ,   datasetpath ( datasetname ) ) Example searches # Single-label image classification\nfinddatasets(blocks=(Image, Label))\n\n# Single-label classification from any data\nfinddatasets(blocks=(Any, Label))\n\n# Datasets with images as input data\nfinddatasets(blocks=(Image, Any))\n\n# All ways to load `pascal2007`\nfinddatasets(name=\"pascal2007\")\n"},{"doctype":"documentation","id":"references/FastAI.Tabular.runtests","title":"runtests","text":""},{"doctype":"documentation","id":"references/Flux.Losses.logitbinarycrossentropy","title":"logitbinarycrossentropy","text":" logitbinarycrossentropy(ŷ, y; agg = mean)\n Mathematically equivalent to binarycrossentropy(σ(ŷ), y)  but is more numerically stable. See also:  crossentropy ,  logitcrossentropy . Examples julia> y_bin = Bool[1,0,1];\n\njulia> y_model = Float32[2, -1, pi]\n3-element Vector{Float32}:\n  2.0\n -1.0\n  3.1415927\n\njulia> Flux.logitbinarycrossentropy(y_model, y_bin)\n0.160832f0\n\njulia> Flux.binarycrossentropy(sigmoid.(y_model), y_bin)\n0.16083185f0\n"},{"doctype":"documentation","id":"references/Flux.loadmodel!","title":"loadmodel!","text":" loadmodel!(dst, src)\n Copy all the parameters (trainable and non-trainable) from  src  into  dst . Recursively walks  dst  and  src  together using  Functors.children , and calling  copyto!  on parameter arrays or throwing an error when there is a mismatch. Non-array elements (such as activation functions) are not copied and need not match. Zero bias vectors and  bias=false  are considered equivalent (see extended help for more details). Examples julia >   dst   =   Chain ( Dense ( Flux . ones32 ( 2 ,   5 ,   tanh ) ) ,   Dense ( 2   =>   1 ;   bias   =   [ 1f0 ] ) ) \n Chain ( \n   Dense ( 5   =>   2 ,   tanh ) ,                    # 12 parameters \n   Dense ( 2   =>   1 ) ,                          # 3 parameters \n )                     # Total: 4 arrays, 15 parameters, 316 bytes. \n \n julia >   dst [ 1 ] . weight   ≈   ones ( 2 ,   5 )    # by construction \n true \n \n julia >   src   =   Chain ( Dense ( 5   =>   2 ,   relu ) ,   Dense ( 2   =>   1 ,   bias = false ) ) ; \n \n julia >   Flux . loadmodel! ( dst ,   src ) ; \n \n julia >   dst [ 1 ] . weight   ≈   ones ( 2 ,   5 )    # values changed \n false \n \n julia >   iszero ( dst [ 2 ] . bias ) \n true Extended help Throws an error when: dst  and  src  do not share the same fields (at any level) the sizes of leaf nodes are mismatched between  dst  and  src copying non-array values to/from an array parameter (except inactive parameters described below) dst  is a “tied” parameter (i.e. refers to another parameter) and loaded into multiple times with mismatched source values Inactive parameters can be encoded by using the boolean value  false  instead of an array. If  dst == false  and  src  is an all-zero array, no error will be raised (and no values copied); however, attempting to copy a non-zero array to an inactive parameter will throw an error. Likewise, copying a  src  value of  false  to any  dst  array is valid, but copying a  src  value of  true  will error."},{"doctype":"documentation","id":"references/FastAI.Datasets.FileDataset","title":"FileDataset","text":""},{"doctype":"documentation","id":"references/FluxTraining.Events.EpochBegin","title":"EpochBegin","text":" EpochBegin()\n Event  called at the beginning of an epoch."},{"doctype":"documentation","id":"references/Flux.crosscor","title":"crosscor","text":""},{"doctype":"documentation","id":"references/FastAI.decodeŷ","title":"decodeŷ","text":""},{"doctype":"documentation","id":"references/FluxTraining.onecycle","title":"onecycle","text":" onecycle(nsteps, max_val, [start_val, end_val; pct_start])\n Creates a one-cycle  Schedule  over  nsteps  steps from  start_val over  max_val  to  end_val . Examples epochlength = length(traindataiter)\ncb = Scheduler(LearningRate => onecycle(10epochlength, 0.01))\nlearner = Learner(<args>..., cb)"},{"doctype":"documentation","id":"references/FastAI.DiscriminativeLRs","title":"DiscriminativeLRs","text":" DiscriminativeLRs(paramgroups, factors)\n Use different learning rates based on  paramgroups .  factors  maps each group to a factor that the learning rate is multiplied by, so for a parameter  x  the factor is get(factors, getgroup(paramgroups, x), 1) . See  ParamGroups . Examples Combining with regular gradient descent, but only training a part of the model. using   Flux . Optimise :   Descent ,   Optimiser \n \n model   =   Chain ( Dense ( 3 ,   5 ) ,   Dense ( 5 ,   3 ) ) \n paramgroups   =   ParamGroups ( IndexGrouper ( [ 1 ,   2 ] ) ,   model ) \n \n dlro   =   DiscriminativeLRs ( paramgroups ,   Dict ( 1   =>   0. ,   2   =>   1. ) ) \n o   =   Optimiser ( dlro ,   Descent ( 0.1 ) )"},{"doctype":"documentation","id":"references/FastAI.Datasets.listdatasources","title":"listdatasources","text":" listdatasources([registry])\n List the dataset sources registered in  registry  (defaults to FastAI.defaultdataregistry() )."},{"doctype":"documentation","id":"references/FastAI.blockcolumn","title":"blockcolumn","text":""},{"doctype":"documentation","id":"references/Flux.identity_init","title":"identity_init","text":" identity_init(size...; gain=1, shift=0) -> Array\nidentity_init(; kw...) -> Function\n Return an  Array{Float32}  of the given  size  which yields an identity mapping when used as parameters in most Flux layers. Use  gain  to scale the identity by a constant. Often useful in the context of transfer learning, i.e when one wants to add more capacity to a model but start from the same mapping. Has the following behaviour 1D: A  Vector  of  zeros  (useful for an identity bias) 2D: An identity matrix (useful for an identity matrix multiplication) More than 2D: A dense block array of center tap spatial filters (useful for an identity convolution) Some caveats: Not all layers will be identity mapping when used with this init. Exceptions include recurrent layers and normalization layers. Layers must have  input_size == output_size  for identity mapping to be possible. When this is not the case, extra dimensions of the array are padded with zeros. For convolutional layers, in addition to the above, the kernel sizes must also be odd and padding must be applied so that output feature maps have the same size as input feature maps, e.g by using  SamePad . Use keyword  shift  (integer or tuple) to apply circular shift to the output, equivalent to  Base.circshift(identity_init(size...), shift) . For consistency with other initialisers, it accepts  rng::AbstractRNG  as an optional first argument. But this is ignored, since the result is not random. Examples julia> Flux.identity_init(3,5)\n3×5 Matrix{Float32}:\n 1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0\n\njulia> Dense(5 => 3, relu, init=Flux.identity_init)([1,-2,3,-4,5])\n3-element Vector{Float32}:\n 1.0\n 0.0\n 3.0\n\njulia> Flux.identity_init(3,3,2; gain=100)\n3×3×2 Array{Float32, 3}:\n[:, :, 1] =\n   0.0  0.0  0.0\n 100.0  0.0  0.0\n   0.0  0.0  0.0\n\n[:, :, 2] =\n 0.0    0.0  0.0\n 0.0  100.0  0.0\n 0.0    0.0  0.0\n\njulia> x4 = cat([1 2 3; 4 5 6; 7 8 9]; dims=4);\n\njulia> Conv((2,2), 1 => 1, init=Flux.identity_init(gain=10), pad=SamePad())(x4)\n3×3×1×1 Array{Float32, 4}:\n[:, :, 1, 1] =\n 10.0  20.0  30.0\n 40.0  50.0  60.0\n 70.0  80.0  90.0\n"},{"doctype":"documentation","id":"references/FastAI.Datasets.blocksmatch","title":"blocksmatch","text":""},{"doctype":"documentation","id":"references/FastAI.savetaskmodel","title":"savetaskmodel","text":" savetaskmodel(path, task, model[; force = false])\n Save a trained  model  along with a  task  to  path  for later inference. Use  loadtaskmodel  for loading both back into a session. If  path already exists, only write to it if  force = true . If  model  weights are on a GPU, they will be moved to the CPU before saving so they can be loaded in a non-GPU environment. JLD2 . jl  is used for serialization."},{"doctype":"documentation","id":"references/FastAI.Continuous","title":"Continuous","text":" Continuous(size) <: Block\n Block  for collections of numbers.  obs  is a valid observation if it’s length is  size  and contains  Number s."},{"doctype":"documentation","id":"references/FluxTraining.ProgressPrinter","title":"ProgressPrinter","text":" ProgressPrinter()\n Prints a progress bar of the currently running epoch."},{"doctype":"documentation","id":"references/FastAI.testencoding","title":"testencoding","text":" testencoding(encoding, block[, obs])\n Performs some tests that the encoding interface is set up properly for encoding  and  block . Tests that obs  is a valid instance  block encode  returns a valid  encodedblock(encoding, block) decode  returns a valid  decodedblock(encoding, encodedblock(encoding, block)) and that the block is identical to  block"},{"doctype":"documentation","id":"references/FluxTraining.handle","title":"handle","text":""},{"doctype":"documentation","id":"references/Flux.Losses.count_repeats","title":"count_repeats","text":""},{"doctype":"documentation","id":"references/DataAugmentation.OneHot","title":"OneHot","text":" OneHot([T = Float32])\n One-hot encodes a  MaskMulti  with  n  classes and size  sz  into an array item of size  (sz..., n)  with element type  T . Supports  apply! . item   =   MaskMulti ( rand ( 1 : 4 ,   100 ,   100 ) ,   1 : 4 ) \n apply ( OneHot ( ) ,   item )"},{"doctype":"documentation","id":"references/DataAugmentation.adjustbrightness!","title":"adjustbrightness!","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Item","title":"Item","text":" abstract type Item\n Abstract supertype of concrete items. Subtype if you want to create a new item. If you want to wrap an existing item, see  ItemWrapper ."},{"doctype":"documentation","id":"references/FastAI.Datasets.ROOT_URL","title":"ROOT_URL","text":""},{"doctype":"documentation","id":"references/FastAI.typify","title":"typify","text":""},{"doctype":"documentation","id":"references/DataAugmentation.reflectionmatrix","title":"reflectionmatrix","text":""},{"doctype":"documentation","id":"references/Flux.Maxout","title":"Maxout","text":" Maxout(layers...)\nMaxout(f, n_alts)\n This contains a number of internal layers, each of which receives the same input. Its output is the elementwise maximum of the the internal layers’ outputs. Instead of defining layers individually, you can provide a zero-argument function which constructs them, and the number to construct. Maxout over linear dense layers satisfies the univeral approximation theorem. See Goodfellow, Warde-Farley, Mirza, Courville & Bengio “Maxout Networks” https://arxiv . org/abs/1302 . 4389 . See also  Parallel  to reduce with other operators. Examples julia> m = Maxout(x -> abs2.(x), x -> x .* 3);\n\njulia> m([-2 -1 0 1 2])\n1×5 Matrix{Int64}:\n 4  1  0  3  6\n\njulia> m3 = Maxout(() -> Dense(5 => 7, tanh), 3)\nMaxout(\n  Dense(5 => 7, tanh),                  # 42 parameters\n  Dense(5 => 7, tanh),                  # 42 parameters\n  Dense(5 => 7, tanh),                  # 42 parameters\n)                   # Total: 6 arrays, 126 parameters, 888 bytes.\n\njulia> Flux.outputsize(m3, (5, 11))\n(7, 11)\n"},{"doctype":"documentation","id":"references/FluxTraining.errorwriteconflict","title":"errorwriteconflict","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.DATASETCONFIGS","title":"DATASETCONFIGS","text":""},{"doctype":"documentation","id":"references/FastAI.getblocks","title":"getblocks","text":""},{"doctype":"documentation","id":"references/Flux.f32","title":"f32","text":" f32(m)\n Converts the  eltype  of model’s parameters to  Float32  (which is Flux’s default). Recurses into structs marked with  @functor ."},{"doctype":"documentation","id":"references/Flux.Optimise","title":"Optimise","text":""},{"doctype":"documentation","id":"references/DataAugmentation.ScaleKeepAspect","title":"ScaleKeepAspect","text":" ScaleKeepAspect(minlengths) <: ProjectiveTransform\n Scales the shortest side of  item  to  minlengths , keeping the original aspect ratio. Examples using   DataAugmentation ,   TestImages \n image   =   testimage ( \" lighthouse \" ) \n tfm   =   ScaleKeepAspect ( ( 200 ,   200 ) ) \n apply ( tfm ,   Image ( image ) )   |>   showitems"},{"doctype":"documentation","id":"references/FastAI.Vision.ImageClassificationMulti","title":"ImageClassificationMulti","text":" ImageClassificationMulti(size, classes; kwargs...)\n Learning task for multi-label image classification. Images are resized to  size  and classified into multiple of  classes . Use  ImageClassificationSingle  for the single-class setting. Keyword arguments computestats = false : Whether to compute image statistics on dataset  data  or use default ImageNet stats. aug_projections =  DataAugmentation.Identity : augmentation to apply during ProjectiveTransforms  (resizing and cropping) aug_image =  DataAugmentation.Identity : pixel-level augmentation to apply during ImagePreprocessing C = RGB{N0f8} : Color type images are converted to before further processing. Use  Gray{N0f8} for grayscale images."},{"doctype":"documentation","id":"references/DataAugmentation.adjustbrightness","title":"adjustbrightness","text":""},{"doctype":"documentation","id":"references/FastAI.LRFinderResult","title":"LRFinderResult","text":" LRFinderResult(lrs, losses)\n Result of the learning rate finder  lrfind . Use  plot to visualize."},{"doctype":"documentation","id":"references/FluxTraining.LoggerBackend","title":"LoggerBackend","text":" abstract type LoggerBackend\n Backend for logging callbacks like. To add support for logging  Loggables.Loggable   L  to backend  B , implement log_to (backend::B, loggable::L, names, i) See also  LogMetrics ,  LogHyperParams ,  log_to"},{"doctype":"documentation","id":"references/Flux.glorot_uniform","title":"glorot_uniform","text":" glorot_uniform([rng=GLOBAL_RNG], size...; gain = 1) -> Array\nglorot_uniform([rng]; kw...) -> Function\n Return an  Array{Float32}  of the given  size  containing random numbers drawn from a uniform distribution on the interval  [-x, x] , where  x = gain * sqrt(6 / (fan_in + fan_out)) . This method is described in [1] and also known as Xavier initialization. Examples julia> Flux.glorot_uniform(3, 4) |> summary\n\"3×4 Matrix{Float32}\"\n\njulia> round.(extrema(Flux.glorot_uniform(10, 100)), digits=3)\n(-0.232f0, 0.234f0)\n\njulia> round.(extrema(Flux.glorot_uniform(100, 10)), digits=3)\n(-0.233f0, 0.233f0)\n\njulia> round.(extrema(Flux.glorot_uniform(100, 100)), digits=3)\n(-0.173f0, 0.173f0)\n\njulia> Dense(3 => 2, tanh; init = Flux.glorot_uniform(MersenneTwister(1)))\nDense(3 => 2, tanh)  # 8 parameters\n\njulia> ans.bias\n2-element Vector{Float32}:\n 0.0\n 0.0\n References [1] Glorot, Xavier, and Yoshua Bengio. “Understanding the difficulty of training deep feedforward neural networks.”  Proceedings of the thirteenth international conference on artificial intelligence and statistics . 2010."}]