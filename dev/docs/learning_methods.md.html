<HTML><head><title>Learning methods</title><link href=../template/hugobook.css rel=stylesheet ></link><link href=../template/ansi.css rel=stylesheet ></link><meta content=Type=text/html; charset=utf-8 http-equiv=Content-Type ></meta></head><body><input onclick=toggleMenu() id=menu-control class=hidden toggle type=checkbox ></input><input id=toc-control type=checkbox class=hidden toggle ></input><main class=container flex ><aside id=menu-container class=book-menu ><nav class=book-menu-content ><h2 id=title >FastAI.jl</h2><div id=sidebar ><div class=doctree ><body><ul><li><p><a href=../README.md.html title= >README</a></p></li><li><p><a href=setup.md.html title= >Setup</a></p></li><li><p><a href=quickstart.md.html title= >Quickstart</a></p></li><li><p>Tutorials</p><ul><li><p><a href=introduction.md.html title= >Introduction</a></p></li><li><p><a href=data_containers.md.html title= >Data containers</a></p></li><li><p><a href=learning_methods.md.html title= >Learning methods</a></p></li><li><p><a href=notebooks/serialization.ipynb.html title= >Saving and loading models</a></p></li><li><p><a href=tutorials/presizing.ipynb.html title= >Presizing vision datasets</a></p></li></ul></li><li><p>Learning tasks</p><ul><li><p><a href=methods/imageclassification.md.html title= >Image classification</a></p></li><li><p><a href=notebooks/imagesegmentation.ipynb.html title= >Image segmentation</a></p></li><li><p><a href=notebooks/keypointregression.ipynb.html title= >Keypoint regression</a></p></li></ul></li><li><p>How To</p><ul><li><p><a href=notebooks/fitonecycle.ipynb.html title= >Train a model from scratch</a></p></li><li><p><a href=notebooks/finetune.ipynb.html title= >Finetune a pretrained model</a></p></li><li><p><a href=notebooks/lrfind.ipynb.html title= >Find a good learning rate</a></p></li><li><p><a href=howto/augmentvision.md.html title= >Augment vision data</a></p></li><li><p><a href=notebooks/how_to_visualize.ipynb.html title= >Visualize data</a></p></li><li><p><a href=howto/logtensorboard.md.html title= >Log to TensorBoard</a></p></li></ul></li><li><p>Reference</p><ul><li><p><a href=../REFERENCE.html title= >Docstrings</a></p></li><li><p><a href=interfaces.md.html title= >Interfaces</a></p></li><li><p><a href=api.md.html title= >API</a></p></li><li><p><a href=glossary.md.html title= >Glossary</a></p></li></ul></li><li><p>Background</p><ul><li><p><a href=background/datapipelines.md.html title= >Performant data pipelines</a></p></li></ul></li></ul></body></div></div></nav></aside><div class=book-page ><header class=book-header ></header><article><h1 id=learning-methods >Learning methods</h1><p><em>This tutorial explains what learning tasks and methods are and how to create your own.</em></p><p>In the <a href=./quickstart.md.html title= >quickstart</a> section, you’ve already seen a learning method in action: <a href=../REFERENCE/FastAI.ImageClassification.html ><code>ImageClassification</code></a>. The learning method abstraction powers FastAI.jl’s high-level interface allowing you to make training models for a task simple. In this tutorial we’ll implement our own version of the image classification learning method. You’re encouraged to follow along in a REPL or notebook. This tutorial can also serve as a template for implementing a custom learning method for your own project.</p><p>A learning method describes how we need to process data so we can train a model for some task. In our case, the task we want to solve is to classify an image. The task defines what kind of data we need, here pairs of images and class labels. That alone, however, isn’t enough to train a model since we can’t just throw an image in any format into a model and get a class out. Almost always the input data needs to be processed in some way before it is input to a model (we call this <strong>encoding</strong>) and the same goes for the model outputs (we call this <strong>decoding</strong>).</p><p>So let’s say we have an image and a trained model. How do we make a prediction? First we encode the image, run it through the model, and then decode the output. Similarly, how we can use a pair of image and class to train a model? We encode both, run the encoded input through the model and then compare the output with the encoded class using a <strong>loss function</strong>. The result tells us how we’ll need to update the weights of the model to improve its performance.</p><p>In essence, the learning method interface allows us to implement these steps and derive useful functionality from it, like training and evaluating models. Later we’ll also cover some optional interfaces that allow us to define other parts of a deep learning project.</p><h2 id=setup >Setup</h2><p>Next to FastAI.jl, you’ll need to install</p><pre lang=juliarepl ><code>] add DataAugmentation DLPipelines Colors
</code></pre><h2 id=datasets >Datasets</h2><p>Before we get started, let’s load up a <a href=./data_containers.md.html title= >data container</a> that we can test our code on as we go. It’s always a good idea to interactively test your code! Since we’ll be implementing a method for image classification, the observations in our data container will of course have to be pairs of images and classes. We’ll use one of the many image classification datasets available from the fastai dataset repository. I’ll use ImageNette, but you can use any of the datasets listed in <code>FastAI.Datasets.DATASETS_IMAGECLASSIFICATION</code>. The way the interface is built allows you to easily swap out the dataset you’re using.</p><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>using FastAI
using FastAI.Datasets
DATASET = &quot;imagenette2-160&quot;
data = Datasets.loadtaskdata(Datasets.datasetpath(DATASET), ImageClasssification)
image, class = getobs(data, 1)
image
</code></pre><pre class=codeoutput ><code>
7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU E5-2673 v4 @ 2.30GHz (406F1),ASM,AES-NI)


Extracting archive: 
--
Path = 
Type = tar
Code Page = UTF-8

Everything is Ok

Folders: 23
Files: 13397
Size:       107794109
Compressed: 6872064
</code></pre><pre class=coderesult ><code>LoadError("string", 4, UndefVarError(:ImageClasssification))</code></pre></div><p>We’ll also collect the unique class names:</p><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>classes = unique([getobs(data.target, i) for i in 1:nobs(data.target)])
</code></pre><pre class=coderesult ><code>LoadError("string", 1, UndefVarError(:data))</code></pre></div><h2 id=implementation >Implementation</h2><h3 id=learning-method-struct >Learning method struct</h3><p>Now let’s get to it! The first thing we need to do is to create a <a href=../REFERENCE/DLPipelines.LearningMethod.html ><code>LearningMethod</code></a> struct. The <code>LearningMethod</code> <code>struct</code> should contain all the configuration needed for encoding and decoding the data. We’ll keep it simple here and include a list of the classes and the image dimensions input to the model. The reference implementation <a href=../REFERENCE/FastAI.ImageClassification.html ><code>ImageClassification</code></a> of course has many more parameters that can be configured.</p><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>using FastAI: DLPipelines

struct MyImageClassification &lt;: DLPipelines.LearningMethod
    classes
    size
end
</code></pre></div><p>Now we can create an instance of it, though of course it can’t do anything (yet!).</p><div result=false class=cellcontainer cell=main lang=julia ><pre result=false class=codecell cell=main lang=julia ><code>method = MyImageClassification(classes, (128, 128))
</code></pre></div><h3 id=encoding-and-decoding >Encoding and decoding</h3><p>There are 3 methods we need to define before we can use our learning method to train models and make predictions:</p><ul><li><p><code>DLPipelines.encodeinput</code> will encode an image so it can be input to a model;</p></li><li><p><code>DLPipelines.encodetarget</code> encodes a class so we can compare it with a model output; and</p></li><li><p><code>DLPipelines.decodeŷ</code> (write <code>\hat&lt;TAB&gt;</code> for  <code> ̂</code>) decodes a model output into a class label</p></li></ul><p>Note: These functions always operate on <em>single</em> images and classes, even if we want to pass batches to the model later on.</p><p>While it’s not the focus of this tutorial, let’s give a quick recap of how the data is encoded and decoded for image classification.</p><ul><li><p>Images are cropped to a common size so they can be batched, converted to a 3D array with dimensions (height, width, color channels) and normalized</p></li><li><p>Classes are encoded as one-hot vectors, teaching the model to predict a confidence distribution over all classes. To decode a predicted one-hot vector, we can simply find the index with the highest value and look up the class label.</p></li></ul><p>Each of the methods also takes a <code>context::</code><a href=../REFERENCE/DLPipelines.Context.html ><code>Context</code></a> argument which allows it to behave differently during training, validation and inference. We’ll make use of that to choose a different image crop for each situation. During training we’ll use a random crop for augmentation, while during validation a center crop will ensure that any metrics we track are the same every epoch. During inference, we won’t crop the image so we don’t lose any information.</p><h4 id=inputs >Inputs</h4><p>We implement <a href=../REFERENCE/DLPipelines.encodeinput.html ><code>encodeinput</code></a> using <a href=https://github.com/lorenzoh/DataAugmentation.jl title= >DataAugmentation.jl</a>. Feel free to look at <a href=https://lorenzoh.github.io/DataAugmentation.jl/dev/docs/literate/intro.html title= >its documentation</a>, we won’t focus on it here.</p><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>using DataAugmentation
using Colors: RGB
using FastAI: IMAGENET_MEANS, IMAGENET_STDS  # color statistics for normalization

# Helper for crop based on context
getresizecrop(context::Training, sz) = DataAugmentation.RandomResizeCrop(sz)
getresizecrop(context::Validation, sz) = CenterResizeCrop(sz)
getresizecrop(context::Inference, sz) = ResizePadDivisible(sz, 32)

function DLPipelines.encodeinput(
        method::MyImageClassification,
        context::Context,
        image)
    tfm = DataAugmentation.compose(
        getresizecrop(context, method.size),
        ToEltype(RGB{Float32}),
        ImageToTensor(),
        Normalize(IMAGENET_MEANS, IMAGENET_STDS);
    )
    return apply(tfm, Image(image)) |&gt; itemdata
end
</code></pre></div><p>If we test this out on an image, it should give us a 3D array of size <code>(128, 128, 3)</code>, and indeed it does:</p><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>x = encodeinput(method, Training(), image)
summary(x)
</code></pre><pre class=coderesult ><code>LoadError("string", 1, UndefVarError(:method))</code></pre></div><h4 id=outputs >Outputs</h4><p><code>encodetarget</code> is much simpler:</p><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>function DLPipelines.encodetarget(
        method::MyImageClassification,
        ::Context,
        class)
    idx = findfirst(isequal(class), method.classes)
    v = zeros(Float32, length(method.classes))
    v[idx] = 1.
    return v
end
</code></pre></div><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>y = encodetarget(method, Training(), class)
</code></pre><pre class=coderesult ><code>LoadError("string", 1, UndefVarError(:method))</code></pre></div><p>The same goes for the decoding step:</p><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>function DLPipelines.decodeŷ(method::MyImageClassification, ::Context, ŷ)
    return method.classes[argmax(ŷ)]
end
</code></pre></div><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>decodeŷ(method, Training(), y) == class
</code></pre><pre class=coderesult ><code>LoadError("string", 1, UndefVarError(:method))</code></pre></div><h2 id=training >Training</h2><p>And that’s all we need to start training models! There are some optional interfaces that make that even easier, but let’s use what we have for now.</p><p>With our <code>LearningMethod</code> defined, we can use <a href=../REFERENCE/DLPipelines.methoddataloaders.html ><code>DLPipelines.methoddataloaders</code></a> to turn a dataset into a set of training and validation data loaders that can be thrown into a training loop.</p><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>traindl, valdl = methoddataloaders(data, method)
</code></pre><pre class=coderesult ><code>LoadError("string", 1, UndefVarError(:data))</code></pre></div><p>Now, with a makeshift model, an optimizer and a loss function we can create a <a href=../REFERENCE/FluxTraining.Learner.html ><code>Learner</code></a>.</p><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>using FastAI: Flux

model = Chain(
    Models.xresnet18(),
    Chain(
            AdaptiveMeanPool((1,1)),
            flatten,
            Dense(512, length(method.classes)),
    )
)
opt = ADAM()
lossfn = Flux.Losses.logitcrossentropy

learner = Learner(model, (traindl, valdl), opt, lossfn)
</code></pre><pre class=coderesult ><code>LoadError("string", 3, UndefVarError(:method))</code></pre></div><p>From here, you’re free to start training using  <a href=../REFERENCE/FluxTraining.fit!.html ><code>fit!</code></a> or <a href=../REFERENCE/FastAI.fitonecycle!.html ><code>fitonecycle!</code></a>.</p></article><footer class=book-footer ></footer></div><aside class=book-toc ><nav id=toc class=book-toc-content ><ul><li><a href=#learning-methods >Learning methods</a><ul><li><a href=#setup >Setup</a><ul></ul></li><li><a href=#datasets >Datasets</a><ul></ul></li><li><a href=#implementation >Implementation</a><ul><li><a href=#learning-method-struct >Learning method struct</a><ul></ul></li><li><a href=#encoding-and-decoding >Encoding and decoding</a><ul><li><a href=#inputs >Inputs</a></li><li><a href=#outputs >Outputs</a></li></ul></li></ul></li><li><a href=#training >Training</a><ul></ul></li></ul></li></ul></nav></aside></main></body></HTML>