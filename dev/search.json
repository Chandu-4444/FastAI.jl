[{"body":"public   methodlearner   —   function Create a  Learner  to train a model for learning method  method  using data .  See also  Learner .","id":"docstrings/FastAI.methodlearner.html"},{"body":"public   joinobs   —   function Concatenate data containers  datas .","id":"docstrings/FastAI.Datasets.joinobs.html"},{"body":"Datasets","id":"docs/interfaces.html#datasets"},{"body":"private   setschedules!   —   function Set  schedules  on  learner ’ s  Scheduler  callback so that training resumes from there . If  learner  does not have a  Scheduler  callback yet, adds it .","id":"docstrings/FastAI.setschedules!.html"},{"body":"High - level Quickly get started training and finetuning models using already implemented learning methods and callbacks . methodlearner fit! fitonecycle! finetune! evaluate learning methods ImageClassification ImageSegmentation callbacks","id":"docs/interfaces.html#high-level"},{"body":"private   plotxy   —   function","id":"docstrings/FastAI.plotxy.html"},{"body":"Low - level Full control over data containers . getobs nobs","id":"docs/interfaces.html#low-level-1"},{"body":"Exercises Using  mapobs  and  loadfile , create a data container where every observation is only an image .","id":"docs/data_containers.html#exercises-1"},{"body":"Data loaders Next we turn the data container into training and validation data loaders .  These take care of efficiently loading batches of data (by default in parallel) .  The observations are already preprocessed using the information in  method  and then batched together .  Let ’ s look at a single batch: xs  is a batch of cropped and normalized images with dimensions  (height, width, color channels, batch size)  and  ys  a batch of one - hot encoded classes with dimensions  (classes, batch size) .","id":"docs/introduction.html#data-loaders"},{"body":"private   makeitem   —   function Tries to assign a  DataAugmentation.Item  from  data  based on its type . args  are passed to the chosen   Item  constructor . AbstractMatrix{<:Colorant}   - >  Image Vector{<:Union{Nothing, SVector}}   - >  Keypoints","id":"docstrings/FastAI.makeitem.html"},{"body":"public   ProjectiveTransforms   —   struct Pipeline step that resizes images and keypoints to  size . In context  Training , applies  augmentations .","id":"docstrings/FastAI.ProjectiveTransforms.html"},{"body":"public   ImageClassification   —   struct A  Method  for multi - class image classification using softmax probabilities . classes  is a vector of the category labels .  Alternatively, you can pass an integer . Images are resized to  sz . During training, a random crop is used and  augmentations , a  DataAugmentation.Transform are applied .","id":"docstrings/FastAI.ImageClassification.html"},{"body":"private   invert   —   function Applies the inverse of the operation  step  to  data","id":"docstrings/FastAI.invert.html"},{"body":"public   groupobs   —   function Split data container data  data  into different data containers, grouping observations by  f(obs) .","id":"docstrings/FastAI.Datasets.groupobs.html"},{"body":"private   PixelShuffle   —   struct Pixel shuffle layer that upscales height and width of  x  by  scale .  Has reduced checkerboard artifacts compared to  ConvTranspose Introduced in  Real - Time Single Image and Video Super - Resolution Using an EfficientSub - Pixel Convolutional Neural Network .","id":"docstrings/FastAI.Models.PixelShuffle.html"},{"body":"Name Module Visibility Category  datasetpath   FastAI.Datasets   public   function   filterobs   FastAI.Datasets   public   function   getclassesclassification   FastAI.Datasets   private   function   getclassessegmentation   FastAI.Datasets   private   function   groupobs   FastAI.Datasets   public   function   joinobs   FastAI.Datasets   public   function   loadfile   FastAI.Datasets   public   function   loadtaskdata   FastAI.Datasets   public   function   mapobs   FastAI.Datasets   public   function   Datasets   FastAI.Datasets   public   module   ImageClassification   FastAI   public   struct   ImagePreprocessing   FastAI   public   struct   PixelShuffle   FastAI.Models   private   struct   ProjectiveTransforms   FastAI   public   struct   finetune!   FastAI   public   function   fitonecycle!   FastAI   public   function   freeze   FastAI   private   function   invert!   FastAI   private   function   invert   FastAI   private   function   makeitem   FastAI   private   function   methodlearner   FastAI   public   function   plotsample!   FastAI   private   function   plotsample   FastAI   private   function   plotxy!   FastAI   private   function   plotxy   FastAI   private   function   run!   FastAI   private   function   run   FastAI   private   function   setschedules!   FastAI   private   function   withcallbacks   FastAI   private   function   withfields   FastAI   private   function ","id":"docstrings.html#docstring-index"},{"body":"Definitions","id":"docs/glossary.html#definitions"},{"body":"Learning task An abstract subtype of  DLPipelines.LearningTask  that represents the problem of learning a mapping from some input type  I  to a target type  T .  For example,  ImageClassificationTask  represents the task of learning to map an image to a class .  See  learning method","id":"docs/glossary.html#learning-task"},{"body":"public   loadtaskdata   —   function Load a task data container for  LearningTask   Task  stored in  dir in a canonical format . Load a data container for  ImageClassificationTask  with observations (input = image, target = class) . If  split  is  true , returns a tuple of the data containers split by the name of the grandparent folder . dir  should contain the data in the following canonical format: dir split (e . g .   “ train ” ,  “ valid ” … ) class (e . g .   “ cat ” ,  “ dog ” … ) image32434 . { jpg/png/ … } … … … Load a data container for  ImageSegmentationTask  with observations (input = image, target = mask) . If  split  is  true , returns a tuple of the data containers split by the name of the grandparent folder .","id":"docstrings/FastAI.Datasets.loadtaskdata.html"},{"body":"public   Datasets   —   module Commonly used datasets and utilities for creating data containers . ToDos: add localization/segmentation datasets add labels for classification datasets","id":"docstrings/FastAI.Datasets.html"},{"body":"Learner Finally we bring the model and data loaders together with an optimizer and loss function in a  Learner .  The  Learner  stores all state for training the model .  It also features a powerful, extensible  callback system  enabling checkpointing, hyperparameter scheduling, TensorBoard logging, and many other features .  Here we use the  ToGPU()  callback so that model and batch data will be transferred to an available GPU and  Metrics(accuracy)  to track the classification accuracy during training . With that setup, training  learner  is dead simple:","id":"docs/introduction.html#learner"},{"body":"Measuring performance So how do you know if your GPU is underutilized? If it isn ’ t, then improving data pipeline performance won ’ t help you at all !  One way to check this is to start training and run  > watch -n 0.1 nvidia-smi  in a terminal which displays and refreshs GPU stats every 1/10th of a second .  If  GPU-Util  stays between 90% and 99%, you ’ re good ! If that ’ s not the case, you might see it frantically jumping up and down .  We can get a better estimate of how much training time can be sped up by running the following experiment: Load one batch and run  n  optimization steps on this batch .  The time this takes corresponds to the training time when the GPU does not have to wait for data to be available . Next take your data iterator and time iterating over the first  n  batches  without  an optimization step . The speed of the complete training loop (data loading and optimization) will be around the maximum of either measurement .  Roughly speaking, if 1 .  takes 100 seconds and 2 .  takes 200 seconds, you know that you can speed up training by about a factor of 2 if you reduce data loading time by half, after which the GPU will become the bottleneck . Again, make sure to run each measurement twice so you don ’ t include the compilation time . To find performance bottlenecks in the loading of each observation, you ’ ll want to compare the time it takes to load an observation of the task data container and the time it takes to encode that observation . This will give you a pretty good idea of where the performance bottleneck is .  Note that the encoding performance is often dependent of the method configuration .  If we used  ImageClassification  with input size  (64, 64)  it would be much faster .","id":"docs/background/datapipelines.html#measuring-performance"},{"body":"private   invert!   —   function Applies the inverse of the operation  step  to  buf  inplace .   buf  is mutated,","id":"docstrings/FastAI.invert!.html"},{"body":"public   fitonecycle!   —   function Fit  learner  for  nepochs  using a one - cycle learning rate schedule .","id":"docstrings/FastAI.fitonecycle!.html"},{"body":"Image classification See  ImageClassification  for now .","id":"docs/methods/imageclassification.html#image-classification"},{"body":"Model input size:  (sz..., ch, batch)  where  ch  depends on color type  C . output size:  (nclasses, batch)","id":"docstrings/FastAI.ImageClassification.html#model"},{"body":"private   plotsample   —   function","id":"docstrings/FastAI.plotsample.html"},{"body":"public   ImagePreprocessing   —   struct Converts an image to a color  C , then to a 3D - array of type  T  and finally normalizes the values using  means  and  stds . If no  means  or  stds  are given, uses ImageNet statistics .","id":"docstrings/FastAI.ImagePreprocessing.html"},{"body":"private   freeze   —   function Freeze all parameters in  model , except those in  model[indices] .","id":"docstrings/FastAI.freeze.html"},{"body":"private   run!   —   function Applies the operation  step  inplace to  buf .   buf  is mutated .","id":"docstrings/FastAI.run!.html"},{"body":"public   filterobs   —   function Return a subset of data container  data  including all indices  i  for which  f(getobs(data, i)) === true .","id":"docstrings/FastAI.Datasets.filterobs.html"},{"body":"Quickstart FastAI . jl makes it easy to train models for common tasks .  For example, we can train an image classification model in just 6 lines .","id":"docs/quickstart.html#quickstart"},{"body":"private   withfields   —   function Replace fields on  x  with given keyword arguments, run  f  and then restore the fields . Every keyword argument is a mapping  (field, value)  or  (field, (setfn!, value)) . setfn!(x, val)  will be used to set the field; if as in the first case none is given  setfield!  is used .","id":"docstrings/FastAI.withfields.html"},{"body":"FastAI . jl interfaces","id":"docs/interfaces.html#fastaijl-interfaces"},{"body":"Data containers This tutorial explains what data containers are, how they are used in FastAI . jl and how to create your own .  You are encouraged to follow along in a REPL or a Jupyter notebook and explore the code .  You will find small exercises at the end of some sections to deepen your understanding .","id":"docs/data_containers.html#data-containers"},{"body":"Model Now we create a Flux . jl model .   methodmodel  is a part of the learning method interface that knows how to smartly construct an image classification model from different backbone architectures .  Here a classficiation head with the appropriate number of classes is stacked on a slightly modified version of the ResNet architecture .","id":"docs/introduction.html#model"},{"body":"Type abbreviations In many docstrings, generic types are abbreviated with the following symbols .  Many of these refer to a learning method; the context should make clear which method is meant . DC{T} : A  data container  of type T, meaning a type that implements the data container interface  getobs  and  nobs  where  getobs : (DC{T}, Int) -> Int , that is, each observation is of type  T . I : Type of the unprocessed input in the context of a method . T : Type of the target variable . X : Type of the processed input .  This is fed into a  model , though it may be batched beforehand .   Xs  represents a batch of processed inputs . Y : Type of the model output .   Ys  represents a batch of model outputs . model / M : A learnable mapping  M : (X,) -> Y  or  M : (Xs,) -> Ys .  It predicts an encoded target from an encoded input .  The learnable part of a learning method . Some examples of these in use: LearningTask  represents the task of learning to predict  T  from  I . LearningMethod  is a concrete approach to learning to predict  T  from  I  by using the encoded representations  X  and  Y . encodeinput : (method, context, I) -> X  encodes an input so that a prediction can be made by a model . A task dataset is a  DC{(I, T)} , i . e .  a data container where each observation is a 2 - tuple of an input and a target .","id":"docs/glossary.html#type-abbreviations"},{"body":"Interface DC  is a data container with observations of type D (i . e .   typeof(getobs(::DC<D>, i)::D) ) Transformations: mapobs(f::(D -> E), ::DC<D>)::DC<E> Map a function (or a tuple of functions) over a data container . Tuple(DC<D1>, ..., DC<DN>)::DC<(D1,...,DN)> Combine multiple data containers into a single data container that returns tuples of the each ’ s observations . filterobs(f, DC<D>)::DC<D> Keep only observations for which  f(obs) === true . groupobs(f, DC<D>)::(DC1<D>, ..., DCN<D>)  with  N  the unique return values of  f(::D) joinobs(f, DC<D1>, ..., DC<DN>)::DC<D> Combines N datasets into a single one,  “ concatenating ”  them . Primitive datasets: FileDataset(dir; filterfn) Each file in  dir  is one observation .  Currently implemented in DLDatasets . jl with FileTrees . jl and observation type  FileTrees.File . TableDataset(table) Every row in the table is an observation .  Could use Tables . jl interface to be compatible with tons of packages .","id":"docstrings/FastAI.Datasets.html#interface"},{"body":"public   finetune!   —   function Behaves as  fastai.Learner.fine_tune","id":"docstrings/FastAI.finetune!.html"},{"body":"Setup FastAI . jl is not registered yet since it depends on some unregistered packages (Flux . jl v0 . 12 . 0), but you can try it out using the included  Manifest.toml .  You will need Julia 1 . 6 for this ( ! ) as the Manifest is not backwards compatible .  You should be able to install FastAI . jl using the REPL as follows:","id":"docs/setup.html#setup"},{"body":"Data container A data structure that is used to load a number of data observations separately and lazily .  It defines how many observations it holds with  nobs  and how to load a single observation with  getobs .","id":"docs/glossary.html#data-container"},{"body":"Learning methods This tutorial explains what learning tasks and methods are, how they are used in FastAI . jl and how to create your own . TODO .  For now, refer to the  DLPipelines . jl docs .","id":"docs/learning_methods.html#learning-methods"},{"body":"Mid - level Learner methodmodel adaptmodel methodlossfn Callback","id":"docs/interfaces.html#mid-level"},{"body":"Training","id":"docs/interfaces.html#training"},{"body":"private   plotxy!   —   function","id":"docstrings/FastAI.plotxy!.html"},{"body":"public   mapobs   —   function Lazily map  f  over the observations in a data container  data . Lazily map each function in tuple  fs  over the observations in data container  data . Returns a tuple of transformed data containers . Map a  NamedTuple  of functions over  data , turning it into a data container of  NamedTuple s .  Field syntax can be used to select a column of the resulting data container .","id":"docstrings/FastAI.Datasets.mapobs.html"},{"body":"private   run   —   function Applies the operation  step  to  data","id":"docstrings/FastAI.run.html"},{"body":"Introduction This tutorial explains the qickstart examples and some core abstractions FastAI . jl is built on . On the  quickstart page , we showed how to train models on common tasks in a few lines of code: Let ’ s unpack each line .","id":"docs/introduction.html#introduction"},{"body":"Performant data pipelines Explainer on how data pipelines in FastAI . jl are made fast and how to make yours fast . When training large deep learning models on a GPU we clearly want wait as short as possible for the training to complete .  The hardware bottleneck is usually the GPU power you have available to you .  This means that data pipelines need to be fast enough to keep the GPU at 100% utilization, that is, keep it from  “ starving ” .  Reducing the time the GPU has to wait for the next batch of data directly lowers the training time until the GPU is fully utilized .  There are other ways to reduce training time like using hyperparameter schedules and different optimizers for faster convergence, but we ’ ll only talk about improving GPU utilization here .","id":"docs/background/datapipelines.html#performant-data-pipelines"},{"body":"FastAI Stable Dev Build Status FastAI . jl is inspired by  fastai , and is a repository of best practices for deep learning in Julia .  Its goal is to easily enable creating state - of - the - art models .  FastAI enables the design, training, and delivery of deep learning models that compete with the best in class, using few lines of code . As an example, training an image classification model from scratch is as simple as Please read  the documentation  for more information and see the  setup instructions","id":"README.html#fastai"},{"body":"Exercises Have a look at the other image classification datasets in  Datasets.DATASETS_IMAGECLASSIFICATION  and change the above code to load a different dataset .","id":"docs/data_containers.html#exercises"},{"body":"Creating data containers from files loadtaskdata  makes it easy to get started when your dataset already comes in the correct format, but alas, datasets come in all different shapes and sizes .  Let ’ s create the same data container, but now using more general functions FastAI . jl provides to get a look behind the scenes .  If each observation in your dataset is a file in a folder,  FileDataset  conveniently creates a data container given a path .  We ’ ll use the path of the downloaded dataset: filedata  is a data container where each observation is a path to a file .  We ’ ll confirm that using  getobs : Next we need to load an image and the corresponding class from the path .  If you have a look at the folder structure of  dir  you can see that the parent folder of each file gives the name of class .  So we can use the following function to load the  (image, class)  pair from a path: Finally, we use  mapobs  to lazily transform each observation and have a data container ready to be used for training an image classifier .","id":"docs/data_containers.html#creating-data-containers-from-files"},{"body":"Task data container / dataset DC{(I, T)} .  A data container containing pairs of inputs and targets .  Used in  methoddataset ,  methoddataloaders  and  evaluate .","id":"docs/glossary.html#task-data-container--dataset"},{"body":"private   withcallbacks   —   function Run  f  with  callbacks  on  learner .  Existing callbacks on  learner  of the same type as in  callbacks  are swapped during the execution of  f .","id":"docstrings/FastAI.withcallbacks.html"},{"body":"Glossary Terms commonly used in  FastAI . jl .","id":"docs/glossary.html#glossary"},{"body":"private   getclassessegmentation   —   function Get the list of classes for classification dataset  name .","id":"docstrings/FastAI.Datasets.getclassessegmentation.html"},{"body":"Mid - level Load and transform data containers . Datasets.datasetpath Datasets.FileDataset Datasets.TableDataset mapobs groupobs joinobs groupobs","id":"docs/interfaces.html#mid-level-1"},{"body":"Improving performance So, you ’ ve identified the data pipeline as a performance bottleneck .  What now? Before anything else, make sure you ’ re doing the following: Use  DataLoaders.DataLoader  as a data iterator .  If you ’ re using  methoddataloaders  or  methodlearner , this is already the case . Start Julia with multiple threads by specifying the  -t n / -t auto  flag when starting Julia .  If it is successful,  Threads.nthreads()  should be larger than  1 . If the data loading is still slowing down training, you ’ ll probably have to speed up the loading of each observation .  As mentioned above, this can be broken down into observation loading and encoding .  The exact strategy will depend on your use case, but here are some examples . Reduce loading time of image datasets by presizing For many computer vision tasks, you will resize and crop images to a specific size during training for GPU performance reasons .  If the images themselves are large, loading them from disk itself can take some time .  If your dataset consists of 1920x1080 resolution images but you ’ re resizing them to 256x256 during training, you ’ re wasting a lot of time loading the large images .   Presizing  means saving resized versions of each image to disk once, and then loading these smaller versions during training .  We can see the performance difference using ImageNette since it comes in 3 sizes: original, 360px and 180px . Reducing allocations with inplace operations When implementing the  LearningMethod  interface, you have the option to implement  encode!(buf, method, context, sample) , an inplace version of  encode  that reuses a buffer to avoid allocations .  Reducing allocations often speeds up the encoding step and can also reduce the frequency of garbage collector pauses during training which can reduce GPU utilization . Using efficient data augmentation Many kinds of augmentation can be composed efficiently .  A prime example of this are image transformations like resizing, scaling and cropping which are powered by  DataAugmentation . jl .  See  its documentation  to find out how to implement efficient, composable data transformations .","id":"docs/background/datapipelines.html#improving-performance"},{"body":"Introduction In the  quickstart  section, you have already come in contact with data containers .  The following code was used to load a data container for image classification: A data container is any type that holds observations of data and allows us to load them with  getobs  and query the number of observations with  nobs : In this case, each observation is a tuple of an image and the corresponding class; after all, we want to use it for image classification . As you saw above, the  Datasets  submodule provides functions for loading and creating data containers .  We used  Datasets.datasetpath  to download a dataset if it wasn ’ t yet and get the folder it was downloaded to .  Then,  Datasets.loadtaskdata  took the folder and loaded a data container suitable for image classification .  FastAI . jl makes it easy to download the datasets from fastai ’ s collection on AWS Open Datasets .  For the full list, see  Datasets.DATASETS","id":"docs/data_containers.html#introduction"},{"body":"Data containers This line downloads and loads the  ImageNette  image classification dataset, a small subset of ImageNet with 10 different classes .   dataset  is a  data container  that can be used to load individual observations, here of images and the corresponding labels .  We can use  getobs(dataset, i)  to load the  i - th observation and  nobs  to find out how many observations there are . To train on a different dataset, you could replace  dataset  with other data containers made up of pairs of images and classes .","id":"docs/introduction.html#data-containers"},{"body":"Low - level LearningMethod LearningTask encode encodeinput decodey","id":"docs/interfaces.html#low-level"},{"body":"Examples Loading and splitting an image classification dataset stored in the same file structure as ImageNette, i . e . : train class1 obs1 … obs2 class1 obs1 … obs2 valid … Turning a container of (input, target) into a container of (x, y) and then an iterator of batches (xs, ys) .  This is pretty much all  methoddataset  and  methoddataloaders  do . Loading an image dataset without labels for inference .","id":"docstrings/FastAI.Datasets.html#examples"},{"body":"Learning method An instance of  DLPipelines.LearningMethod .  A concrete approach to solving a learning task .  Encapsulates the logic and configuration for processing data to train a model and make predictions . See the DLPipelines . jl documentation for more information .","id":"docs/glossary.html#learning-method"},{"body":"public   loadfile   —   function Load a file from disk into the appropriate format .","id":"docstrings/FastAI.Datasets.loadfile.html"},{"body":"Method Here we define  ImageClassification , which defines how data is processed before being fed to the model and how model outputs are turned into predictions .   classes  is a vector of strings naming each class, and  (224, 224)  the size of the images that are input to the model . ImageClassification  is a  LearningMethod , an abstraction that encapsulates the logic and configuration for training models on a specific learning task .  See  learning methods  to find out more about how they can be used and how to create custom learning methods .","id":"docs/introduction.html#method"},{"body":"Types input::AbstractMatrix{2, <:Colorant} : an image target  the class label that the image belongs to x::AbstractArray{Float32, 3} : a normalized 3D - array with dimensions  height, width, channels y::AbstractVector{Float32} : one - hot encoding of category","id":"docstrings/FastAI.ImageClassification.html#types"},{"body":"High - level Quickly download and load task data containers from the fastai dataset library . Datasets.loadtaskdata Datasets.DATASETS","id":"docs/interfaces.html#high-level-1"},{"body":"Reasons for low GPU utilization The main cause of low GPU utilization is that the next batch of data is not available after a training step and the GPU has to wait .  This means that in order to get full GPU utilization, loading a batch must not take longer than a training step; and the data must be loaded in the background, so that it is ready the moment the GPU needs it . These issues can be addressed by using worker threads to load multiple batches in parallel keeping the primary thread free; and reducing the time it takes to load a single batch FastAI . jl by default uses  DataLoader  from the  DataLoaders . jl  package which addresses points 1 .  and 2 .  For those familiar with PyTorch, it closely resembles  torch.utils.data.DataLoader .  It also efficiently collates the data by reusing a buffer where supported . We can measure the large performance difference by comparing a naive sequential data iterator with  eachobsparallel , the data iterator that  DataLoader  uses . Running each timer twice to forego compilation time, the sequential iterator takes 20 seconds while the parallel iterator using 11 background threads only takes 2 . 5 seconds .  This certainly isn ’ t a proper benchmark, but it shows the performance can be improved by an order of magnitude with no effort . Beside increasing the amount of compute available with worker threads as above, the data loading performance can also be improved by reducing the time it takes to load a single batch .  Since a batch is made up of some number of observations, this usually boils down to reducing the loading time of a single observation .  If you ’ re using the  LearningMethod  API, this can be further broken down into the loading and encoding part .","id":"docs/background/datapipelines.html#reasons-for-low-gpu-utilization"},{"body":"public   datasetpath   —   function Return the folder that dataset  name  is stored . If it hasn ’ t been downloaded yet, you will be asked if you want to download it .  See  Datasets.DATASETS  for a list of available datasets .","id":"docstrings/FastAI.Datasets.datasetpath.html"},{"body":"private   plotsample!   —   function","id":"docstrings/FastAI.plotsample!.html"},{"body":"Splitting a data container into subsets Until now, we ’ ve only created a single data container containing all observations in a dataset .  In practice, though, you ’ ll want to have at least a training and validation split .  The easiest way to get these is to randomly split your data container into two parts .  Here we split  data  into 80% training and 20% validation data .  Note the use of  shuffleobs  to make sure each split has approximately the same class distribution . This is great for experimenting, but where possible you will want to use the official training/validation split for a dataset .  Consider the image classification dataset folder structure: As you can see, the grandparent folder of each image indicates which split it is a part of .   groupobs  allows us to partition a data container using a function .  Let ’ s use it to split  filedata  based on the name of the grandparent directory .  (We can ’ t reuse  data  for this since it no longer carries the file information . ) Using this official split, it will be easier to compare the performance of your results with those of others ’ .","id":"docs/data_containers.html#splitting-a-data-container-into-subsets"},{"body":"private   getclassesclassification   —   function Get the list of classes for classification dataset  name .","id":"docstrings/FastAI.Datasets.getclassesclassification.html"},{"body":"Image classification Train an image classifier from scratch: Or finetune a pretrained model:","id":"docs/quickstart.html#image-classification"}]